{
  "doc-99d2908f4f99a9467cfed62940c1a834": {
    "content": "The Transformer has been on a lot of people’s minds over the last year five years. This post presents an annotated version of the paper in the form of a line-by-line implementation. It reorders and deletes some sections from the original paper and adds comments throughout. This document itself is a working notebook, and should be a completely usable implementation. Code is available here.\n# !pip install -r requirements.txt\n# # Uncomment for colab\n# #\n# !pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil\n# !python -m spacy download de_core_news_sm\n# !python -m spacy download en_core_web_sm\nimport os\nfrom os.path import exists\nimport torch\nimport torch.nn as nn\nfrom torch.nn.functional import log_softmax, pad\nimport math\nimport copy\nimport time\nfrom torch.optim.lr_scheduler import LambdaLR\nimport pandas as pd\nimport altair as alt\nfrom torchtext.data.functional import to_map_style_dataset\nfrom torch.utils.data import DataLoader\nfrom torchtext.vocab import build_vocab_from_iterator\nimport torchtext.datasets as datasets\nimport spacy\nimport GPUtil\nimport warnings\nfrom torch.utils.data.distributed import DistributedSampler\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n# Set to False to skip notebook execution (e.g. for debugging)\nwarnings.filterwarnings(\"ignore\")\nRUN_EXAMPLES = True\n# Some convenience helper functions used throughout the notebook\ndef is_interactive_notebook():\nreturn __name__ == \"__main__\"\ndef show_example(fn, args=[]):\nif __name__ == \"__main__\" and RUN_EXAMPLES:\nreturn fn(*args)\ndef execute_example(fn, args=[]):\nif __name__ == \"__main__\" and RUN_EXAMPLES:\nfn(*args)\nclass DummyOptimizer(torch.optim.Optimizer):\ndef __init__(self):\nself.param_groups = [{\"lr\": 0}]\nNone\ndef step(self):\nNone\ndef zero_grad(self, set_to_none=False):\nNone\nclass DummyScheduler:\ndef step(self):\nNone\nMy comments are blockquoted. The main text is all from the paper itself.\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet and ConvS2S, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations. End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks.\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution.\nMost competitive neural sequence transduction models have an encoder-decoder structure (cite). Here, the encoder maps an input sequence of symbol representations (x_1, ..., x_n) to a sequence of continuous representations \\mathbf{z} = (z_1, ..., z_n). Given \\mathbf{z}, the decoder then generates an output sequence (y_1,...,y_m) of symbols one element at a time. At each step the model is auto-regressive (cite), consuming the previously generated symbols as additional input when generating the next.\nclass EncoderDecoder(nn.Module):\n\"\"\"\nA standard Encoder-Decoder architecture. Base for this and many\nother models.\n\"\"\"\ndef __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\nsuper(EncoderDecoder, self).__init__()\nself.encoder = encoder\nself.decoder = decoder\nself.src_embed = src_embed\nself.tgt_embed = tgt_embed\nself.generator = generator\ndef forward(self, src, tgt, src_mask, tgt_mask):\n\"Take in and process masked src and target sequences.\"\nreturn self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\ndef encode(self, src, src_mask):\nreturn self.encoder(self.src_embed(src), src_mask)\ndef decode(self, memory, src_mask, tgt, tgt_mask):\nreturn self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\nclass Generator(nn.Module):\n\"Define standard linear + softmax generation step.\"\ndef __init__(self, d_model, vocab):\nsuper(Generator, self).__init__()\nself.proj = nn.Linear(d_model, vocab)\ndef forward(self, x):\nreturn log_softmax(self.proj(x), dim=-1)\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\nThe encoder is composed of a stack of N=6 identical layers.\ndef clones(module, N):\n\"Produce N identical layers.\"\nreturn nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\nclass Encoder(nn.Module):\n\"Core encoder is a stack of N layers\"\ndef __init__(self, layer, N):\nsuper(Encoder, self).__init__()\nself.layers = clones(layer, N)\nself.norm = LayerNorm(layer.size)\ndef forward(self, x, mask):\n\"Pass the input (and mask) through each layer in turn.\"\nfor layer in self.layers:\nx = layer(x, mask)\nreturn self.norm(x)\nWe employ a residual connection (cite) around each of the two sub-layers, followed by layer normalization (cite).\nclass LayerNorm(nn.Module):\n\"Construct a layernorm module (See citation for details).\"\ndef __init__(self, features, eps=1e-6):\nsuper(LayerNorm, self).__init__()\nself.a_2 = nn.Parameter(torch.ones(features))\nself.b_2 = nn.Parameter(torch.zeros(features))\nself.eps = eps\ndef forward(self, x):\nmean = x.mean(-1, keepdim=True)\nstd = x.std(-1, keepdim=True)\nreturn self.a_2 * (x - mean) / (std + self.eps) + self.b_2\nThat is, the output of each sub-layer is \\mathrm{LayerNorm}(x + \\mathrm{Sublayer}(x)), where \\mathrm{Sublayer}(x) is the function implemented by the sub-layer itself. We apply dropout (cite) to the output of each sub-layer, before it is added to the sub-layer input and normalized.\nTo facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d_{\\text{model}}=512.\nclass SublayerConnection(nn.Module):\n\"\"\"\nA residual connection followed by a layer norm.\nNote for code simplicity the norm is first as opposed to last.\n\"\"\"\ndef __init__(self, size, dropout):\nsuper(SublayerConnection, self).__init__()\nself.norm = LayerNorm(size)\nself.dropout = nn.Dropout(dropout)\ndef forward(self, x, sublayer):\n\"Apply residual connection to any sublayer with the same size.\"\nreturn x + self.dropout(sublayer(self.norm(x)))\nEach layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.\nclass EncoderLayer(nn.Module):\n\"Encoder is made up of self-attn and feed forward (defined below)\"\ndef __init__(self, size, self_attn, feed_forward, dropout):\nsuper(EncoderLayer, self).__init__()\nself.self_attn = self_attn\nself.feed_forward = feed_forward\nself.sublayer = clones(SublayerConnection(size, dropout), 2)\nself.size = size\ndef forward(self, x, mask):\n\"Follow Figure 1 (left) for connections.\"\nx = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\nreturn self.sublayer[1](x, self.feed_forward)\nThe decoder is also composed of a stack of N=6 identical layers.\nclass Decoder(nn.Module):\n\"Generic N layer decoder with masking.\"\ndef __init__(self, layer, N):\nsuper(Decoder, self).__init__()\nself.layers = clones(layer, N)\nself.norm = LayerNorm(layer.size)\ndef forward(self, x, memory, src_mask, tgt_mask):\nfor layer in self.layers:\nx = layer(x, memory, src_mask, tgt_mask)\nreturn self.norm(x)\nIn addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.\nclass DecoderLayer(nn.Module):\n\"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\ndef __init__(self, size, self_attn, src_attn, feed_forward, dropout):\nsuper(DecoderLayer, self).__init__()\nself.size = size\nself.self_attn = self_attn\nself.src_attn = src_attn\nself.feed_forward = feed_forward\nself.sublayer = clones(SublayerConnection(size, dropout), 3)\ndef forward(self, x, memory, src_mask, tgt_mask):\n\"Follow Figure 1 (right) for connections.\"\nm = memory\nx = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\nx = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\nreturn self.sublayer[2](x, self.feed_forward)\nWe also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\ndef subsequent_mask(size):\n\"Mask out subsequent positions.\"\nattn_shape = (1, size, size)\nsubsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\ntorch.uint8\n)\nreturn subsequent_mask == 0\nBelow the attention mask shows the position each tgt word (row) is allowed to look at (column). Words are blocked for attending to future words during training.\ndef example_mask():\nLS_data = pd.concat(\n[\npd.DataFrame(\n{\n\"Subsequent Mask\": subsequent_mask(20)[0][x, y].flatten(),\n\"Window\": y,\n\"Masking\": x,\n}\n)\nfor y in range(20)\nfor x in range(20)\n]\n)\nreturn (\nalt.Chart(LS_data)\n.mark_rect()\n.properties(height=250, width=250)\n.encode(\nalt.X(\"Window:O\"),\nalt.Y(\"Masking:O\"),\nalt.Color(\"Subsequent Mask:Q\", scale=alt.Scale(scheme=\"viridis\")),\n)\n.interactive()\n)\nshow_example(example_mask)\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\nWe call our particular attention “Scaled Dot-Product Attention”. The input consists of queries and keys of dimension d_k, and values of dimension d_v. We compute the dot products of the query with all keys, divide each by \\sqrt{d_k}, and apply a softmax function to obtain the weights on the values.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. We compute the matrix of outputs as:\n\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\ndef attention(query, key, value, mask=None, dropout=None):\n\"Compute 'Scaled Dot Product Attention'\"\nd_k = query.size(-1)\nscores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\nif mask is not None:\nscores = scores.masked_fill(mask == 0, -1e9)\np_attn = scores.softmax(dim=-1)\nif dropout is not None:\np_attn = dropout(p_attn)\nreturn torch.matmul(p_attn, value), p_attn\nThe two most commonly used attention functions are additive attention (cite), and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of \\frac{1}{\\sqrt{d_k}}. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\nWhile for small values of d_k the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d_k (cite). We suspect that for large values of d_k, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients (To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q \\cdot k = \\sum_{i=1}^{d_k} q_ik_i, has mean 0 and variance d_k.). To counteract this effect, we scale the dot products by \\frac{1}{\\sqrt{d_k}}.\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O \\\\ \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)\nWhere the projections are parameter matrices W^Q_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}, W^K_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}, W^V_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v} and W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}.\nIn this work we employ h=8 parallel attention layers, or heads. For each of these we use d_k=d_v=d_{\\text{model}}/h=64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\nclass MultiHeadedAttention(nn.Module):\ndef __init__(self, h, d_model, dropout=0.1):\n\"Take in model size and number of heads.\"\nsuper(MultiHeadedAttention, self).__init__()\nassert d_model % h == 0\n# We assume d_v always equals d_k\nself.d_k = d_model // h\nself.h = h\nself.linears = clones(nn.Linear(d_model, d_model), 4)\nself.attn = None\nself.dropout = nn.Dropout(p=dropout)\ndef forward(self, query, key, value, mask=None):\n\"Implements Figure 2\"\nif mask is not None:\n# Same mask applied to all h heads.\nmask = mask.unsqueeze(1)\nnbatches = query.size(0)\n# 1) Do all the linear projections in batch from d_model => h x d_k\nquery, key, value = [\nlin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\nfor lin, x in zip(self.linears, (query, key, value))\n]\n# 2) Apply attention on all the projected vectors in batch.\nx, self.attn = attention(\nquery, key, value, mask=mask, dropout=self.dropout\n)\n# 3) \"Concat\" using a view and apply a final linear.\nx = (\nx.transpose(1, 2)\n.contiguous()\n.view(nbatches, -1, self.h * self.d_k)\n)\ndel query\ndel key\ndel value\nreturn self.linears[-1](x)\nThe Transformer uses multi-head attention in three different ways: 1) In “encoder-decoder attention” layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as (cite).\nThe encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\nSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to -\\infty) all values in the input of the softmax which correspond to illegal connections.\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is d_{\\text{model}}=512, and the inner-layer has dimensionality d_{ff}=2048.\nclass PositionwiseFeedForward(nn.Module):\n\"Implements FFN equation.\"\ndef __init__(self, d_model, d_ff, dropout=0.1):\nsuper(PositionwiseFeedForward, self).__init__()\nself.w_1 = nn.Linear(d_model, d_ff)\nself.w_2 = nn.Linear(d_ff, d_model)\nself.dropout = nn.Dropout(dropout)\ndef forward(self, x):\nreturn self.w_2(self.dropout(self.w_1(x).relu()))\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d_{\\text{model}}. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to (cite). In the embedding layers, we multiply those weights by \\sqrt{d_{\\text{model}}}.\nclass Embeddings(nn.Module):\ndef __init__(self, d_model, vocab):\nsuper(Embeddings, self).__init__()\nself.lut = nn.Embedding(vocab, d_model)\nself.d_model = d_model\ndef forward(self, x):\nreturn self.lut(x) * math.sqrt(self.d_model)\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d_{\\text{model}} as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed (cite).\nIn this work, we use sine and cosine functions of different frequencies:\nPE_{(pos,2i)} = \\sin(pos / 10000^{2i/d_{\\text{model}}})\nPE_{(pos,2i+1)} = \\cos(pos / 10000^{2i/d_{\\text{model}}})\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2\\pi to 10000 \\cdot 2\\pi. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PE_{pos+k} can be represented as a linear function of PE_{pos}.\nIn addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P_{drop}=0.1.\nclass PositionalEncoding(nn.Module):\n\"Implement the PE function.\"\ndef __init__(self, d_model, dropout, max_len=5000):\nsuper(PositionalEncoding, self).__init__()\nself.dropout = nn.Dropout(p=dropout)\n# Compute the positional encodings once in log space.\npe = torch.zeros(max_len, d_model)\nposition = torch.arange(0, max_len).unsqueeze(1)\ndiv_term = torch.exp(\ntorch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n)\npe[:, 0::2] = torch.sin(position * div_term)\npe[:, 1::2] = torch.cos(position * div_term)\npe = pe.unsqueeze(0)\nself.register_buffer(\"pe\", pe)\ndef forward(self, x):\nx = x + self.pe[:, : x.size(1)].requires_grad_(False)\nreturn self.dropout(x)\nBelow the positional encoding will add in a sine wave based on position. The frequency and offset of the wave is different for each dimension.\ndef example_positional():\npe = PositionalEncoding(20, 0)\ny = pe.forward(torch.zeros(1, 100, 20))\ndata = pd.concat(\n[\npd.DataFrame(\n{\n\"embedding\": y[0, :, dim],\n\"dimension\": dim,\n\"position\": list(range(100)),\n}\n)\nfor dim in [4, 5, 6, 7]\n]\n)\nreturn (\nalt.Chart(data)\n.mark_line()\n.properties(width=800)\n.encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n.interactive()\n)\nshow_example(example_positional)\nWe also experimented with using learned positional embeddings (cite) instead, and found that the two versions produced nearly identical results. We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\nHere we define a function from hyperparameters to a full model.\ndef make_model(\nsrc_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n):\n\"Helper: Construct a model from hyperparameters.\"\nc = copy.deepcopy\nattn = MultiHeadedAttention(h, d_model)\nff = PositionwiseFeedForward(d_model, d_ff, dropout)\nposition = PositionalEncoding(d_model, dropout)\nmodel = EncoderDecoder(\nEncoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\nDecoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\nnn.Sequential(Embeddings(d_model, src_vocab), c(position)),\nnn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\nGenerator(d_model, tgt_vocab),\n)\n# This was important from their code.\n# Initialize parameters with Glorot / fan_avg.\nfor p in model.parameters():\nif p.dim() > 1:\nnn.init.xavier_uniform_(p)\nreturn model\nHere we make a forward step to generate a prediction of the model. We try to use our transformer to memorize the input. As you will see the output is randomly generated due to the fact that the model is not trained yet. In the next tutorial we will build the training function and try to train our model to memorize the numbers from 1 to 10.\ndef inference_test():\ntest_model = make_model(11, 11, 2)\ntest_model.eval()\nsrc = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\nsrc_mask = torch.ones(1, 1, 10)\nmemory = test_model.encode(src, src_mask)\nys = torch.zeros(1, 1).type_as(src)\nfor i in range(9):\nout = test_model.decode(\nmemory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n)\nprob = test_model.generator(out[:, -1])\n_, next_word = torch.max(prob, dim=1)\nnext_word = next_word.data[0]\nys = torch.cat(\n[ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1\n)\nprint(\"Example Untrained Model Prediction:\", ys)\ndef run_tests():\nfor _ in range(10):\ninference_test()\nshow_example(run_tests)\nExample Untrained Model Prediction: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\nExample Untrained Model Prediction: tensor([[0, 3, 4, 4, 4, 4, 4, 4, 4, 4]])\nExample Untrained Model Prediction: tensor([[ 0, 10, 10, 10, 3, 2, 5, 7, 9, 6]])\nExample Untrained Model Prediction: tensor([[ 0, 4, 3, 6, 10, 10, 2, 6, 2, 2]])\nExample Untrained Model Prediction: tensor([[ 0, 9, 0, 1, 5, 10, 1, 5, 10, 6]])\nExample Untrained Model Prediction: tensor([[ 0, 1, 5, 1, 10, 1, 10, 10, 10, 10]])\nExample Untrained Model Prediction: tensor([[ 0, 1, 10, 9, 9, 9, 9, 9, 1, 5]])\nExample Untrained Model Prediction: tensor([[ 0, 3, 1, 5, 10, 10, 10, 10, 10, 10]])\nExample Untrained Model Prediction: tensor([[ 0, 3, 5, 10, 5, 10, 4, 2, 4, 2]])\nExample Untrained Model Prediction: tensor([[0, 5, 6, 2, 5, 6, 2, 6, 2, 2]])\nThis section describes the training regime for our models.\nWe stop for a quick interlude to introduce some of the tools needed to train a standard encoder decoder model. First we define a batch object that holds the src and target sentences for training, as well as constructing the masks.\nclass Batch:\n\"\"\"Object for holding a batch of data with mask during training.\"\"\"\ndef __init__(self, src, tgt=None, pad=2): # 2 = <blank>\nself.src = src\nself.src_mask = (src != pad).unsqueeze(-2)\nif tgt is not None:\nself.tgt = tgt[:, :-1]\nself.tgt_y = tgt[:, 1:]\nself.tgt_mask = self.make_std_mask(self.tgt, pad)\nself.ntokens = (self.tgt_y != pad).data.sum()\n@staticmethod\ndef make_std_mask(tgt, pad):\n\"Create a mask to hide padding and future words.\"\ntgt_mask = (tgt != pad).unsqueeze(-2)\ntgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(\ntgt_mask.data\n)\nreturn tgt_mask\nNext we create a generic training and scoring function to keep track of loss. We pass in a generic loss compute function that also handles parameter updates.\nclass TrainState:\n\"\"\"Track number of steps, examples, and tokens processed\"\"\"\nstep: int = 0 # Steps in the current epoch\naccum_step: int = 0 # Number of gradient accumulation steps\nsamples: int = 0 # total # of examples used\ntokens: int = 0 # total # of tokens processed\ndef run_epoch(\ndata_iter,\nmodel,\nloss_compute,\noptimizer,\nscheduler,\nmode=\"train\",\naccum_iter=1,\ntrain_state=TrainState(),\n):\n\"\"\"Train a single epoch\"\"\"\nstart = time.time()\ntotal_tokens = 0\ntotal_loss = 0\ntokens = 0\nn_accum = 0\nfor i, batch in enumerate(data_iter):\nout = model.forward(\nbatch.src, batch.tgt, batch.src_mask, batch.tgt_mask\n)\nloss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n# loss_node = loss_node / accum_iter\nif mode == \"train\" or mode == \"train+log\":\nloss_node.backward()\ntrain_state.step += 1\ntrain_state.samples += batch.src.shape[0]\ntrain_state.tokens += batch.ntokens\nif i % accum_iter == 0:\noptimizer.step()\noptimizer.zero_grad(set_to_none=True)\nn_accum += 1\ntrain_state.accum_step += 1\nscheduler.step()\ntotal_loss += loss\ntotal_tokens += batch.ntokens\ntokens += batch.ntokens\nif i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\nlr = optimizer.param_groups[0][\"lr\"]\nelapsed = time.time() - start\nprint(\n(\n\"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \"\n+ \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n)\n% (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)\n)\nstart = time.time()\ntokens = 0\ndel loss\ndel loss_node\nreturn total_loss / total_tokens, train_state\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary.\nSentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models, step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\nWe used the Adam optimizer (cite) with \\beta_1=0.9, \\beta_2=0.98 and \\epsilon=10^{-9}. We varied the learning rate over the course of training, according to the formula:\nlrate = d_{\\text{model}}^{-0.5} \\cdot \\min({step\\_num}^{-0.5}, {step\\_num} \\cdot {warmup\\_steps}^{-1.5})\nThis corresponds to increasing the learning rate linearly for the first warmup\\_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup\\_steps=4000.\nNote: This part is very important. Need to train with this setup of the model.\nExample of the curves of this model for different model sizes and for optimization hyperparameters.\ndef rate(step, model_size, factor, warmup):\n\"\"\"\nwe have to default the step to 1 for LambdaLR function\nto avoid zero raising to negative power.\n\"\"\"\nif step == 0:\nstep = 1\nreturn factor * (\nmodel_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n)\ndef example_learning_schedule():\nopts = [\n[512, 1, 4000], # example 1\n[512, 1, 8000], # example 2\n[256, 1, 4000], # example 3\n]\ndummy_model = torch.nn.Linear(1, 1)\nlearning_rates = []\n# we have 3 examples in opts list.\nfor idx, example in enumerate(opts):\n# run 20000 epoch for each example\noptimizer = torch.optim.Adam(\ndummy_model.parameters(), lr=1, betas=(0.9, 0.98), eps=1e-9\n)\nlr_scheduler = LambdaLR(\noptimizer=optimizer, lr_lambda=lambda step: rate(step, *example)\n)\ntmp = []\n# take 20K dummy training steps, save the learning rate at each step\nfor step in range(20000):\ntmp.append(optimizer.param_groups[0][\"lr\"])\noptimizer.step()\nlr_scheduler.step()\nlearning_rates.append(tmp)\nlearning_rates = torch.tensor(learning_rates)\n# Enable altair to handle more than 5000 rows\nalt.data_transformers.disable_max_rows()\nopts_data = pd.concat(\n[\npd.DataFrame(\n{\n\"Learning Rate\": learning_rates[warmup_idx, :],\n\"model_size:warmup\": [\"512:4000\", \"512:8000\", \"256:4000\"][\nwarmup_idx\n],\n\"step\": range(20000),\n}\n)\nfor warmup_idx in [0, 1, 2]\n]\n)\nreturn (\nalt.Chart(opts_data)\n.mark_line()\n.properties(width=600)\n.encode(x=\"step\", y=\"Learning Rate\", color=\"model_size:warmup:N\")\n.interactive()\n)\nexample_learning_schedule()\nDuring training, we employed label smoothing of value \\epsilon_{ls}=0.1 (cite). This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\nWe implement label smoothing using the KL div loss. Instead of using a one-hot target distribution, we create a distribution that has\nconfidence\nof the correct word and the rest of thesmoothing\nmass distributed throughout the vocabulary.\nclass LabelSmoothing(nn.Module):\n\"Implement label smoothing.\"\ndef __init__(self, size, padding_idx, smoothing=0.0):\nsuper(LabelSmoothing, self).__init__()\nself.criterion = nn.KLDivLoss(reduction=\"sum\")\nself.padding_idx = padding_idx\nself.confidence = 1.0 - smoothing\nself.smoothing = smoothing\nself.size = size\nself.true_dist = None\ndef forward(self, x, target):\nassert x.size(1) == self.size\ntrue_dist = x.data.clone()\ntrue_dist.fill_(self.smoothing / (self.size - 2))\ntrue_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\ntrue_dist[:, self.padding_idx] = 0\nmask = torch.nonzero(target.data == self.padding_idx)\nif mask.dim() > 0:\ntrue_dist.index_fill_(0, mask.squeeze(), 0.0)\nself.true_dist = true_dist\nreturn self.criterion(x, true_dist.clone().detach())\nHere we can see an example of how the mass is distributed to the words based on confidence.\n# Example of label smoothing.\ndef example_label_smoothing():\ncrit = LabelSmoothing(5, 0, 0.4)\npredict = torch.FloatTensor(\n[\n[0, 0.2, 0.7, 0.1, 0],\n[0, 0.2, 0.7, 0.1, 0],\n[0, 0.2, 0.7, 0.1, 0],\n[0, 0.2, 0.7, 0.1, 0],\n[0, 0.2, 0.7, 0.1, 0],\n]\n)\ncrit(x=predict.log(), target=torch.LongTensor([2, 1, 0, 3, 3]))\nLS_data = pd.concat(\n[\npd.DataFrame(\n{\n\"target distribution\": crit.true_dist[x, y].flatten(),\n\"columns\": y,\n\"rows\": x,\n}\n)\nfor y in range(5)\nfor x in range(5)\n]\n)\nreturn (\nalt.Chart(LS_data)\n.mark_rect(color=\"Blue\", opacity=1)\n.properties(height=200, width=200)\n.encode(\nalt.X(\"columns:O\", title=None),\nalt.Y(\"rows:O\", title=None),\nalt.Color(\n\"target distribution:Q\", scale=alt.Scale(scheme=\"viridis\")\n),\n)\n.interactive()\n)\nshow_example(example_label_smoothing)\nLabel smoothing actually starts to penalize the model if it gets very confident about a given choice.\ndef loss(x, crit):\nd = x + 3 * 1\npredict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d]])\nreturn crit(predict.log(), torch.LongTensor([1])).data\ndef penalization_visualization():\ncrit = LabelSmoothing(5, 0, 0.1)\nloss_data = pd.DataFrame(\n{\n\"Loss\": [loss(x, crit) for x in range(1, 100)],\n\"Steps\": list(range(99)),\n}\n).astype(\"float\")\nreturn (\nalt.Chart(loss_data)\n.mark_line()\n.properties(width=350)\n.encode(\nx=\"Steps\",\ny=\"Loss\",\n)\n.interactive()\n)\nshow_example(penalization_visualization)\nWe can begin by trying out a simple copy-task. Given a random set of input symbols from a small vocabulary, the goal is to generate back those same symbols.\ndef data_gen(V, batch_size, nbatches):\n\"Generate random data for a src-tgt copy task.\"\nfor i in range(nbatches):\ndata = torch.randint(1, V, size=(batch_size, 10))\ndata[:, 0] = 1\nsrc = data.requires_grad_(False).clone().detach()\ntgt = data.requires_grad_(False).clone().detach()\nyield Batch(src, tgt, 0)\nclass SimpleLossCompute:\n\"A simple loss compute and train function.\"\ndef __init__(self, generator, criterion):\nself.generator = generator\nself.criterion = criterion\ndef __call__(self, x, y, norm):\nx = self.generator(x)\nsloss = (\nself.criterion(\nx.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n)\n/ norm\n)\nreturn sloss.data * norm, sloss\nThis code predicts a translation using greedy decoding for simplicity.\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\nmemory = model.encode(src, src_mask)\nys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data)\nfor i in range(max_len - 1):\nout = model.decode(\nmemory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n)\nprob = model.generator(out[:, -1])\n_, next_word = torch.max(prob, dim=1)\nnext_word = next_word.data[0]\nys = torch.cat(\n[ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)], dim=1\n)\nreturn ys\n# Train the simple copy task.\ndef example_simple_model():\nV = 11\ncriterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\nmodel = make_model(V, V, N=2)\noptimizer = torch.optim.Adam(\nmodel.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9\n)\nlr_scheduler = LambdaLR(\noptimizer=optimizer,\nlr_lambda=lambda step: rate(\nstep, model_size=model.src_embed[0].d_model, factor=1.0, warmup=400\n),\n)\nbatch_size = 80\nfor epoch in range(20):\nmodel.train()\nrun_epoch(\ndata_gen(V, batch_size, 20),\nmodel,\nSimpleLossCompute(model.generator, criterion),\noptimizer,\nlr_scheduler,\nmode=\"train\",\n)\nmodel.eval()\nrun_epoch(\ndata_gen(V, batch_size, 5),\nmodel,\nSimpleLossCompute(model.generator, criterion),\nDummyOptimizer(),\nDummyScheduler(),\nmode=\"eval\",\n)[0]\nmodel.eval()\nsrc = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\nmax_len = src.shape[1]\nsrc_mask = torch.ones(1, 1, max_len)\nprint(greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=0))\n# execute_example(example_simple_model)\nNow we consider a real-world example using the Multi30k German-English Translation task. This task is much smaller than the WMT task considered in the paper, but it illustrates the whole system. We also show how to use multi-gpu processing to make it really fast.\nWe will load the dataset using torchtext and spacy for tokenization.\n# Load spacy tokenizer models, download them if they haven't been\n# downloaded already\ndef load_tokenizers():\ntry:\nspacy_de = spacy.load(\"de_core_news_sm\")\nexcept IOError:\nos.system(\"python -m spacy download de_core_news_sm\")\nspacy_de = spacy.load(\"de_core_news_sm\")\ntry:\nspacy_en = spacy.load(\"en_core_web_sm\")\nexcept IOError:\nos.system(\"python -m spacy download en_core_web_sm\")\nspacy_en = spacy.load(\"en_core_web_sm\")\nreturn spacy_de, spacy_en\ndef tokenize(text, tokenizer):\nreturn [tok.text for tok in tokenizer.tokenizer(text)]\ndef yield_tokens(data_iter, tokenizer, index):\nfor from_to_tuple in data_iter:\nyield tokenizer(from_to_tuple[index])\ndef build_vocabulary(spacy_de, spacy_en):\ndef tokenize_de(text):\nreturn tokenize(text, spacy_de)\ndef tokenize_en(text):\nreturn tokenize(text, spacy_en)\nprint(\"Building German Vocabulary ...\")\ntrain, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\nvocab_src = build_vocab_from_iterator(\nyield_tokens(train + val + test, tokenize_de, index=0),\nmin_freq=2,\nspecials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n)\nprint(\"Building English Vocabulary ...\")\ntrain, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\nvocab_tgt = build_vocab_from_iterator(\nyield_tokens(train + val + test, tokenize_en, index=1),\nmin_freq=2,\nspecials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n)\nvocab_src.set_default_index(vocab_src[\"<unk>\"])\nvocab_tgt.set_default_index(vocab_tgt[\"<unk>\"])\nreturn vocab_src, vocab_tgt\ndef load_vocab(spacy_de, spacy_en):\nif not exists(\"vocab.pt\"):\nvocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)\ntorch.save((vocab_src, vocab_tgt), \"vocab.pt\")\nelse:\nvocab_src, vocab_tgt = torch.load(\"vocab.pt\")\nprint(\"Finished.\\nVocabulary sizes:\")\nprint(len(vocab_src))\nprint(len(vocab_tgt))\nreturn vocab_src, vocab_tgt\nif is_interactive_notebook():\n# global variables used later in the script\nspacy_de, spacy_en = show_example(load_tokenizers)\nvocab_src, vocab_tgt = show_example(load_vocab, args=[spacy_de, spacy_en])\nFinished.\nVocabulary sizes:\n59981\n36745\nBatching matters a ton for speed. We want to have very evenly divided batches, with absolutely minimal padding. To do this we have to hack a bit around the default torchtext batching. This code patches their default batching to make sure we search over enough sentences to find tight batches.\ndef collate_batch(\nbatch,\nsrc_pipeline,\ntgt_pipeline,\nsrc_vocab,\ntgt_vocab,\ndevice,\nmax_padding=128,\npad_id=2,\n):\nbs_id = torch.tensor([0], device=device) # <s> token id\neos_id = torch.tensor([1], device=device) # </s> token id\nsrc_list, tgt_list = [], []\nfor (_src, _tgt) in batch:\nprocessed_src = torch.cat(\n[\nbs_id,\ntorch.tensor(\nsrc_vocab(src_pipeline(_src)),\ndtype=torch.int64,\ndevice=device,\n),\neos_id,\n],\n0,\n)\nprocessed_tgt = torch.cat(\n[\nbs_id,\ntorch.tensor(\ntgt_vocab(tgt_pipeline(_tgt)),\ndtype=torch.int64,\ndevice=device,\n),\neos_id,\n],\n0,\n)\nsrc_list.append(\n# warning - overwrites values for negative values of padding - len\npad(\nprocessed_src,\n(\n0,\nmax_padding - len(processed_src),\n),\nvalue=pad_id,\n)\n)\ntgt_list.append(\npad(\nprocessed_tgt,\n(0, max_padding - len(processed_tgt)),\nvalue=pad_id,\n)\n)\nsrc = torch.stack(src_list)\ntgt = torch.stack(tgt_list)\nreturn (src, tgt)\ndef create_dataloaders(\ndevice,\nvocab_src,\nvocab_tgt,\nspacy_de,\nspacy_en,\nbatch_size=12000,\nmax_padding=128,\nis_distributed=True,\n):\n# def create_dataloaders(batch_size=12000):\ndef tokenize_de(text):\nreturn tokenize(text, spacy_de)\ndef tokenize_en(text):\nreturn tokenize(text, spacy_en)\ndef collate_fn(batch):\nreturn collate_batch(\nbatch,\ntokenize_de,\ntokenize_en,\nvocab_src,\nvocab_tgt,\ndevice,\nmax_padding=max_padding,\npad_id=vocab_src.get_stoi()[\"<blank>\"],\n)\ntrain_iter, valid_iter, test_iter = datasets.Multi30k(\nlanguage_pair=(\"de\", \"en\")\n)\ntrain_iter_map = to_map_style_dataset(\ntrain_iter\n) # DistributedSampler needs a dataset len()\ntrain_sampler = (\nDistributedSampler(train_iter_map) if is_distributed else None\n)\nvalid_iter_map = to_map_style_dataset(valid_iter)\nvalid_sampler = (\nDistributedSampler(valid_iter_map) if is_distributed else None\n)\ntrain_dataloader = DataLoader(\ntrain_iter_map,\nbatch_size=batch_size,\nshuffle=(train_sampler is None),\nsampler=train_sampler,\ncollate_fn=collate_fn,\n)\nvalid_dataloader = DataLoader(\nvalid_iter_map,\nbatch_size=batch_size,\nshuffle=(valid_sampler is None),\nsampler=valid_sampler,\ncollate_fn=collate_fn,\n)\nreturn train_dataloader, valid_dataloader\ndef train_worker(\ngpu,\nngpus_per_node,\nvocab_src,\nvocab_tgt,\nspacy_de,\nspacy_en,\nconfig,\nis_distributed=False,\n):\nprint(f\"Train worker process using GPU: {gpu} for training\", flush=True)\ntorch.cuda.set_device(gpu)\npad_idx = vocab_tgt[\"<blank>\"]\nd_model = 512\nmodel = make_model(len(vocab_src), len(vocab_tgt), N=6)\nmodel.cuda(gpu)\nmodule = model\nis_main_process = True\nif is_distributed:\ndist.init_process_group(\n\"nccl\", init_method=\"env://\", rank=gpu, world_size=ngpus_per_node\n)\nmodel = DDP(model, device_ids=[gpu])\nmodule = model.module\nis_main_process = gpu == 0\ncriterion = LabelSmoothing(\nsize=len(vocab_tgt), padding_idx=pad_idx, smoothing=0.1\n)\ncriterion.cuda(gpu)\ntrain_dataloader, valid_dataloader = create_dataloaders(\ngpu,\nvocab_src,\nvocab_tgt,\nspacy_de,\nspacy_en,\nbatch_size=config[\"batch_size\"] // ngpus_per_node,\nmax_padding=config[\"max_padding\"],\nis_distributed=is_distributed,\n)\noptimizer = torch.optim.Adam(\nmodel.parameters(), lr=config[\"base_lr\"], betas=(0.9, 0.98), eps=1e-9\n)\nlr_scheduler = LambdaLR(\noptimizer=optimizer,\nlr_lambda=lambda step: rate(\nstep, d_model, factor=1, warmup=config[\"warmup\"]\n),\n)\ntrain_state = TrainState()\nfor epoch in range(config[\"num_epochs\"]):\nif is_distributed:\ntrain_dataloader.sampler.set_epoch(epoch)\nvalid_dataloader.sampler.set_epoch(epoch)\nmodel.train()\nprint(f\"[GPU{gpu}] Epoch {epoch} Training ====\", flush=True)\n_, train_state = run_epoch(\n(Batch(b[0], b[1], pad_idx) for b in train_dataloader),\nmodel,\nSimpleLossCompute(module.generator, criterion),\noptimizer,\nlr_scheduler,\nmode=\"train+log\",\naccum_iter=config[\"accum_iter\"],\ntrain_state=train_state,\n)\nGPUtil.showUtilization()\nif is_main_process:\nfile_path = \"%s%.2d.pt\" % (config[\"file_prefix\"], epoch)\ntorch.save(module.state_dict(), file_path)\ntorch.cuda.empty_cache()\nprint(f\"[GPU{gpu}] Epoch {epoch} Validation ====\", flush=True)\nmodel.eval()\nsloss = run_epoch(\n(Batch(b[0], b[1], pad_idx) for b in valid_dataloader),\nmodel,\nSimpleLossCompute(module.generator, criterion),\nDummyOptimizer(),\nDummyScheduler(),\nmode=\"eval\",\n)\nprint(sloss)\ntorch.cuda.empty_cache()\nif is_main_process:\nfile_path = \"%sfinal.pt\" % config[\"file_prefix\"]\ntorch.save(module.state_dict(), file_path)\ndef train_distributed_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config):\nfrom the_annotated_transformer import train_worker\nngpus = torch.cuda.device_count()\nos.environ[\"MASTER_ADDR\"] = \"localhost\"\nos.environ[\"MASTER_PORT\"] = \"12356\"\nprint(f\"Number of GPUs detected: {ngpus}\")\nprint(\"Spawning training processes ...\")\nmp.spawn(\ntrain_worker,\nnprocs=ngpus,\nargs=(ngpus, vocab_src, vocab_tgt, spacy_de, spacy_en, config, True),\n)\ndef train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config):\nif config[\"distributed\"]:\ntrain_distributed_model(\nvocab_src, vocab_tgt, spacy_de, spacy_en, config\n)\nelse:\ntrain_worker(\n0, 1, vocab_src, vocab_tgt, spacy_de, spacy_en, config, False\n)\ndef load_trained_model():\nconfig = {\n\"batch_size\": 32,\n\"distributed\": False,\n\"num_epochs\": 8,\n\"accum_iter\": 10,\n\"base_lr\": 1.0,\n\"max_padding\": 72,\n\"warmup\": 3000,\n\"file_prefix\": \"multi30k_model_\",\n}\nmodel_path = \"multi30k_model_final.pt\"\nif not exists(model_path):\ntrain_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config)\nmodel = make_model(len(vocab_src), len(vocab_tgt), N=6)\nmodel.load_state_dict(torch.load(\"multi30k_model_final.pt\"))\nreturn model\nif is_interactive_notebook():\nmodel = load_trained_model()\nOnce trained we can decode the model to produce a set of translations. Here we simply translate the first sentence in the validation set. This dataset is pretty small so the translations with greedy search are reasonably accurate.\nSo this mostly covers the transformer model itself. There are four aspects that we didn’t cover explicitly. We also have all these additional features implemented in OpenNMT-py.\n- BPE/ Word-piece: We can use a library to first preprocess the data into subword units. See Rico Sennrich’s subword-nmt implementation. These models will transform the training data to look like this:\n▁Die ▁Protokoll datei ▁kann ▁ heimlich ▁per ▁E - Mail ▁oder ▁FTP ▁an ▁einen ▁bestimmte n ▁Empfänger ▁gesendet ▁werden .\n- Shared Embeddings: When using BPE with shared vocabulary we can share the same weight vectors between the source / target / generator. See the (cite) for details. To add this to the model simply do this:\nif False:\nmodel.src_embed[0].lut.weight = model.tgt_embeddings[0].lut.weight\nmodel.generator.lut.weight = model.tgt_embed[0].lut.weight\n- Beam Search: This is a bit too complicated to cover here. See the OpenNMT-py for a pytorch implementation.\n- Model Averaging: The paper averages the last k checkpoints to create an ensembling effect. We can do this after the fact if we have a bunch of models:\ndef average(model, models):\n\"Average models into model\"\nfor ps in zip(*[m.params() for m in [model] + models]):\nps[0].copy_(torch.sum(*ps[1:]) / len(ps[1:]))\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\nWith the addtional extensions in the last section, the OpenNMT-py replication gets to 26.9 on EN-DE WMT. Here I have loaded in those parameters to our reimplemenation.\n# Load data and model for output checks\ndef check_outputs(\nvalid_dataloader,\nmodel,\nvocab_src,\nvocab_tgt,\nn_examples=15,\npad_idx=2,\neos_string=\"</s>\",\n):\nresults = [()] * n_examples\nfor idx in range(n_examples):\nprint(\"\\nExample %d ========\\n\" % idx)\nb = next(iter(valid_dataloader))\nrb = Batch(b[0], b[1], pad_idx)\ngreedy_decode(model, rb.src, rb.src_mask, 64, 0)[0]\nsrc_tokens = [\nvocab_src.get_itos()[x] for x in rb.src[0] if x != pad_idx\n]\ntgt_tokens = [\nvocab_tgt.get_itos()[x] for x in rb.tgt[0] if x != pad_idx\n]\nprint(\n\"Source Text (Input) : \"\n+ \" \".join(src_tokens).replace(\"\\n\", \"\")\n)\nprint(\n\"Target Text (Ground Truth) : \"\n+ \" \".join(tgt_tokens).replace(\"\\n\", \"\")\n)\nmodel_out = greedy_decode(model, rb.src, rb.src_mask, 72, 0)[0]\nmodel_txt = (\n\" \".join(\n[vocab_tgt.get_itos()[x] for x in model_out if x != pad_idx]\n).split(eos_string, 1)[0]\n+ eos_string\n)\nprint(\"Model Output : \" + model_txt.replace(\"\\n\", \"\"))\nresults[idx] = (rb, src_tokens, tgt_tokens, model_out, model_txt)\nreturn results\ndef run_model_example(n_examples=5):\nglobal vocab_src, vocab_tgt, spacy_de, spacy_en\nprint(\"Preparing Data ...\")\n_, valid_dataloader = create_dataloaders(\ntorch.device(\"cpu\"),\nvocab_src,\nvocab_tgt,\nspacy_de,\nspacy_en,\nbatch_size=1,\nis_distributed=False,\n)\nprint(\"Loading Trained Model ...\")\nmodel = make_model(len(vocab_src), len(vocab_tgt), N=6)\nmodel.load_state_dict(\ntorch.load(\"multi30k_model_final.pt\", map_location=torch.device(\"cpu\"))\n)\nprint(\"Checking Model Outputs:\")\nexample_data = check_outputs(\nvalid_dataloader, model, vocab_src, vocab_tgt, n_examples=n_examples\n)\nreturn model, example_data\n# execute_example(run_model_example)\nEven with a greedy decoder the translation looks pretty good. We can further visualize it to see what is happening at each layer of the attention\ndef mtx2df(m, max_row, max_col, row_tokens, col_tokens):\n\"convert a dense matrix to a data frame with row and column indices\"\nreturn pd.DataFrame(\n[\n(\nr,\nc,\nfloat(m[r, c]),\n\"%.3d %s\"\n% (r, row_tokens[r] if len(row_tokens) > r else \"<blank>\"),\n\"%.3d %s\"\n% (c, col_tokens[c] if len(col_tokens) > c else \"<blank>\"),\n)\nfor r in range(m.shape[0])\nfor c in range(m.shape[1])\nif r < max_row and c < max_col\n],\n# if float(m[r,c]) != 0 and r < max_row and c < max_col],\ncolumns=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n)\ndef attn_map(attn, layer, head, row_tokens, col_tokens, max_dim=30):\ndf = mtx2df(\nattn[0, head].data,\nmax_dim,\nmax_dim,\nrow_tokens,\ncol_tokens,\n)\nreturn (\nalt.Chart(data=df)\n.mark_rect()\n.encode(\nx=alt.X(\"col_token\", axis=alt.Axis(title=\"\")),\ny=alt.Y(\"row_token\", axis=alt.Axis(title=\"\")),\ncolor=\"value\",\ntooltip=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n)\n.properties(height=400, width=400)\n.interactive()\n)\ndef get_encoder(model, layer):\nreturn model.encoder.layers[layer].self_attn.attn\ndef get_decoder_self(model, layer):\nreturn model.decoder.layers[layer].self_attn.attn\ndef get_decoder_src(model, layer):\nreturn model.decoder.layers[layer].src_attn.attn\ndef visualize_layer(model, layer, getter_fn, ntokens, row_tokens, col_tokens):\n# ntokens = last_example[0].ntokens\nattn = getter_fn(model, layer)\nn_heads = attn.shape[1]\ncharts = [\nattn_map(\nattn,\n0,\nh,\nrow_tokens=row_tokens,\ncol_tokens=col_tokens,\nmax_dim=ntokens,\n)\nfor h in range(n_heads)\n]\nassert n_heads == 8\nreturn alt.vconcat(\ncharts[0]\n# | charts[1]\n| charts[2]\n# | charts[3]\n| charts[4]\n# | charts[5]\n| charts[6]\n# | charts[7]\n# layer + 1 due to 0-indexing\n).properties(title=\"Layer %d\" % (layer + 1))\ndef viz_encoder_self():\nmodel, example_data = run_model_example(n_examples=1)\nexample = example_data[\nlen(example_data) - 1\n] # batch object for the final example\nlayer_viz = [\nvisualize_layer(\nmodel, layer, get_encoder, len(example[1]), example[1], example[1]\n)\nfor layer in range(6)\n]\nreturn alt.hconcat(\nlayer_viz[0]\n# & layer_viz[1]\n& layer_viz[2]\n# & layer_viz[3]\n& layer_viz[4]\n# & layer_viz[5]\n)\nshow_example(viz_encoder_self)\nPreparing Data ...\nLoading Trained Model ...\nChecking Model Outputs:\nExample 0 ========\nSource Text (Input) : <s> Zwei Frauen in pinkfarbenen T-Shirts und <unk> unterhalten sich vor einem <unk> . </s>\nTarget Text (Ground Truth) : <s> Two women wearing pink T - shirts and blue jeans converse outside clothing store . </s>\nModel Output : <s> Two women in pink shirts and face are talking in front of a <unk> . </s>\ndef viz_decoder_self():\nmodel, example_data = run_model_example(n_examples=1)\nexample = example_data[len(example_data) - 1]\nlayer_viz = [\nvisualize_layer(\nmodel,\nlayer,\nget_decoder_self,\nlen(example[1]),\nexample[1],\nexample[1],\n)\nfor layer in range(6)\n]\nreturn alt.hconcat(\nlayer_viz[0]\n& layer_viz[1]\n& layer_viz[2]\n& layer_viz[3]\n& layer_viz[4]\n& layer_viz[5]\n)\nshow_example(viz_decoder_self)\nPreparing Data ...\nLoading Trained Model ...\nChecking Model Outputs:\nExample 0 ========\nSource Text (Input) : <s> Eine Gruppe von Männern in Kostümen spielt Musik . </s>\nTarget Text (Ground Truth) : <s> A group of men in costume play music . </s>\nModel Output : <s> A group of men in costumes playing music . </s>\ndef viz_decoder_src():\nmodel, example_data = run_model_example(n_examples=1)\nexample = example_data[len(example_data) - 1]\nlayer_viz = [\nvisualize_layer(\nmodel,\nlayer,\nget_decoder_src,\nmax(len(example[1]), len(example[2])),\nexample[1],\nexample[2],\n)\nfor layer in range(6)\n]\nreturn alt.hconcat(\nlayer_viz[0]\n& layer_viz[1]\n& layer_viz[2]\n& layer_viz[3]\n& layer_viz[4]\n& layer_viz[5]\n)\nshow_example(viz_decoder_src)\nPreparing Data ...\nLoading Trained Model ...\nChecking Model Outputs:\nExample 0 ========\nSource Text (Input) : <s> Ein kleiner Junge verwendet einen Bohrer , um ein Loch in ein Holzstück zu machen . </s>\nTarget Text (Ground Truth) : <s> A little boy using a drill to make a hole in a piece of wood . </s>\nModel Output : <s> A little boy uses a machine to be working in a hole in a log . </s>\nHopefully this code is useful for future research. Please reach out if you have any issues.\nCheers, Sasha Rush, Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak, Stella Biderman\n\n\n\nThe First Law of Complexodynamics\nA few weeks ago, I had the pleasure of attending FQXi’s Setting Time Aright conference, part of which took place on a cruise from Bergen, Norway to Copenhagen, Denmark. (Why aren’t theoretical computer science conferences ever held on cruises? If nothing else, it certainly cuts down on attendees sneaking away from the conference venue.) This conference brought together physicists, cosmologists, philosophers, biologists, psychologists, and (for some strange reason) one quantum complexity blogger to pontificate about the existence, directionality, and nature of time. If you want to know more about the conference, check out Sean Carroll’s Cosmic Variance posts here and here.\nSean also delivered the opening talk of the conference, during which (among other things) he asked a beautiful question: why does “complexity” or “interestingness” of physical systems seem to increase with time and then hit a maximum and decrease, in contrast to the entropy, which of course increases monotonically?\nMy purpose, in this post, is to sketch a possible answer to Sean’s question, drawing on concepts from Kolmogorov complexity. If this answer has been suggested before, I’m sure someone will let me know in the comments section.\nFirst, some background: we all know the Second Law, which says that the entropy of any closed system tends to increase with time until it reaches a maximum value. Here “entropy” is slippery to define—we’ll come back to that later—but somehow measures how “random” or “generic” or “disordered” a system is. As Sean points out in his wonderful book From Eternity to Here, the Second Law is almost a tautology: how could a system not tend to evolve to more “generic” configurations? if it didn’t, those configurations wouldn’t be generic! So the real question is not why the entropy is increasing, but why it was ever low to begin with. In other words, why did the universe’s initial state at the big bang contain so much order for the universe’s subsequent evolution to destroy? I won’t address that celebrated mystery in this post, but will simply take the low entropy of the initial state as given.\nThe point that interests us is this: even though isolated physical systems get monotonically more entropic, they don’t get monotonically more “complicated” or “interesting.” Sean didn’t define what he meant by “complicated” or “interesting” here—indeed, defining those concepts was part of his challenge—but he illustrated what he had in mind with the example of a coffee cup. Shamelessly ripping off his slides:\nEntropy increases monotonically from left to right, but intuitively, the “complexity” seems highest in the middle picture: the one with all the tendrils of milk. And same is true for the whole universe: shortly after the big bang, the universe was basically just a low-entropy soup of high-energy particles. A googol years from now, after the last black holes have sputtered away in bursts of Hawking radiation, the universe will basically be just a high-entropy soup of low-energy particles. But today, in between, the universe contains interesting structures such as galaxies and brains and hot-dog-shaped novelty vehicles. We see the pattern:\nIn answering Sean’s provocative question (whether there’s some “law of complexodynamics” that would explain his graph), it seems to me that the challenge is twofold:\n- Come up with a plausible formal definition of “complexity.”\n- Prove that the “complexity,” so defined, is large at intermediate times in natural model systems, despite being close to zero at the initial time and close to zero at late times.\nTo clarify: it’s not hard to explain, at least at a handwaving level, why the complexity should be close to zero at the initial time. It’s because we assumed the entropy is close to zero, and entropy plausibly gives an upper bound on complexity. Nor is it hard to explain why the complexity should be close to zero at late times: it’s because the system reaches equilibrium (i.e., something resembling the uniform distribution over all possible states), which we’re essentially defining to be simple. At intermediate times, neither of those constraints is operative, and therefore the complexity could become large. But does it become large? How large? How could we predict? And what kind of “complexity” are we talking about, anyway?\nAfter thinking on and off about these questions, I now conjecture that they can be answered using a notion called sophistication from the theory of Kolmogorov complexity. Recall that the Kolmogorov complexity of a string x is the length of the shortest computer program that outputs x (in some Turing-universal programming language—the exact choice can be shown not to matter much). Sophistication is a more … well, sophisticated concept, but we’ll get to that later.\nAs a first step, let’s use Kolmogorov complexity to define entropy. Already it’s not quite obvious how to do that. If you start, say, a cellular automaton, or a system of billiard balls, in some simple initial configuration, and then let it evolve for a while according to dynamical laws, visually it will look like the entropy is going up. But if the system happens to be deterministic, then mathematically, its state can always be specified by giving (1) the initial state, and (2) the number of steps t it’s been run for. The former takes a constant number of bits to specify (independent of t), while the latter takes log(t) bits. It follows that, if we use Kolmogorov complexity as our stand-in for entropy, then the entropy can increase at most logarithmically with t—much slower than the linear or polynomial increase that we’d intuitively expect.\nThere are at least two ways to solve this problem. The first is to consider probabilistic systems, rather than deterministic ones. In the probabilistic case, the Kolmogorov complexity really does increase at a polynomial rate, as you’d expect. The second solution is to replace the Kolmogorov complexity by the resource-bounded Kolmogorov complexity: the length of the shortest computer program that outputs the state in a short amount of time (or the size of the smallest, say, depth-3 circuit that outputs the state—for present purposes, it doesn’t even matter much what kind of resource bound we impose, as long as the bound is severe enough). Even though there’s a computer program only log(t) bits long to compute the state of the system after t time steps, that program will typically use an amount of time that grows with t (or even faster), so if we rule out sufficiently complex programs, we can again get our program size to increase with t at a polynomial rate.\nOK, that was entropy. What about the thing Sean was calling “complexity”—which, to avoid confusion with other kinds of complexity, from now on I’m going to call “complextropy”? For this, we’re going to need a cluster of related ideas that go under names like sophistication, Kolmogorov structure functions, and algorithmic statistics. The backstory is that, in the 1970s (after introducing Kolmogorov complexity), Kolmogorov made an observation that was closely related to Sean’s observation above. A uniformly random string, he said, has close-to-maximal Kolmogorov complexity, but it’s also one of the least “complex” or “interesting” strings imaginable. After all, we can describe essentially everything you’d ever want to know about the string by saying “it’s random”! But is there a way to formalize that intuition? Indeed there is.\nFirst, given a set S of n-bit strings, let K(S) be the number of bits in the shortest computer program that outputs the elements of S and then halts. Also, given such a set S and an element x of S, let K(x|S) be the length of the shortest program that outputs x, given an oracle for testing membership in S. Then we can let the sophistication of x, or Soph(x), be the smallest possible value of K(S), over all sets S such that\n- x∈S and\n- K(x|S) ≥ log2(|S|) – c, for some constant c. (In other words, one can distill all the “nonrandom” information in x just by saying that x belongs that S.)\nIntuitively, Soph(x) is the length of the shortest computer program that describes, not necessarily x itself, but a set S of which x is a “random” or “generic” member. To illustrate, any string x with small Kolmogorov complexity has small sophistication, since we can let S be the singleton set {x}. However, a uniformly-random string also has small sophistication, since we can let S be the set {0,1}n of all n-bit strings. In fact, the question arises of whether there are any sophisticated strings! Apparently, after Kolmogorov raised this question in the early 1980s, it was answered in the affirmative by Alexander Shen (for more, see this paper by Gács, Tromp, and Vitányi). The construction is via a diagonalization argument that’s a bit too complicated to fit in this blog post.\nBut what does any of this have to do with coffee cups? Well, at first glance, sophistication seems to have exactly the properties that we were looking for in a “complextropy” measure: it’s small for both simple strings and uniformly random strings, but large for strings in a weird third category of “neither simple nor random.” Unfortunately, as we defined it above, sophistication still doesn’t do the job. For deterministic systems, the problem is the same as the one pointed out earlier for Kolmogorov complexity: we can always describe the system’s state after t time steps by specifying the initial state, the transition rule, and t. Therefore the sophistication can never exceed log(t)+c. Even for probabilistic systems, though, we can specify the set S(t) of all possible states after t time steps by specifying the initial state, the probabilistic transition rule, and t. And, at least assuming that the probability distribution over S(t) is uniform, by a simple counting argument the state after t steps will almost always be a “generic” element of S(t). So again, the sophistication will almost never exceed log(t)+c. (If the distribution over S(t) is nonuniform, then some technical further arguments are needed, which I omit.)\nHow can we fix this problem? I think the key is to bring computational resource bounds into the picture. (We already saw a hint of this in the discussion of entropy.) In particular, suppose we define the complextropy of an n-bit string x to be something like the following:\nthe number of bits in the shortest computer program that runs in n log(n) time, and that outputs a nearly-uniform sample from a set S such that (i) x∈S, and (ii) any computer program that outputs x in n log(n) time, given an oracle that provides independent, uniform samples from S, has at least log2(|S|)-c bits, for some constant c.\nHere n log(n) is just intended as a concrete example of a complexity bound: one could replace it with some other time bound, or a restriction to (say) constant-depth circuits or some other weak model of computation. The motivation for the definition is that we want some “complextropy” measure that will assign a value close to 0 to the first and third coffee cups in the picture, but a large value to the second coffee cup. And thus we consider the length of the shortest efficient computer program that outputs, not necessarily the target string x itself, but a sample from a probability distribution D such that x is not efficiently compressible with respect to D. (In other words, x looks to any efficient algorithm like a “random” or “generic” sample from D.)\nNote that it’s essential for this definition that we imposed a computational efficiency requirement in two places: on the sampling algorithm, and also on the algorithm that reconstructs x given the sampling oracle. Without the first efficiency constraint, the complextropy could never exceed log(t)+c by the previous argument. Meanwhile, without the second efficiency constraint, the complextropy would increase, but then it would probably keep right on increasing, for the following reason: a time-bounded sampling algorithm wouldn’t be able to sample from exactly the right set S, only a reasonable facsimile thereof, and a reconstruction algorithm with unlimited time could probably then use special properties of the target string x to reconstruct x with fewer than log2(|S|)-c bits.\nBut as long as we remember to put computational efficiency requirements on both algorithms, I conjecture that the complextropy will satisfy the “First Law of Complexodynamics,” exhibiting exactly the behavior that Sean wants: small for the initial state, large for intermediate states, then small again once the mixing has finished. I don’t yet know how to prove this conjecture. But crucially, it’s not a hopelessly open-ended question that one tosses out just to show how wide-ranging one’s thoughts are, but a relatively-bounded question about which actual theorems could be proved and actual papers published.\nIf you want to do so, the first step will be to “instantiate” everything I said above with a particular model system and particular resource constraints. One good choice could be a discretized “coffee cup,” consisting of a 2D array of black and white pixels (the “coffee” and “milk”), which are initially in separated components and then subject to random nearest-neighbor mixing dynamics. (E.g., at each time step, we pick an adjacent coffee pixel and milk pixel uniformly at random, and swap the two.) Can we show that for such a system, the complextropy becomes large at intermediate times (intuitively, because of the need to specify the irregular boundaries between the regions of all-black pixels, all-white pixels, and mixed black-and-white pixels)?\nOne could try to show such a statement either theoretically or empirically. Theoretically, I have no idea where to begin in proving it, despite a clear intuition that such a statement should hold: let me toss it out as a wonderful (I think) open problem! At an empirical level, one could simply try to plot the complextropy in some simulated system, like the discrete coffee cup, and show that it has the predicted small-large-small behavior. One obvious difficulty here is that the complextropy, under any definition like the one I gave, is almost certainly going to be intractable to compute or even approximate. However, one could try to get around that problem the same way many others have, in empirical research inspired by Kolmogorov complexity: namely, by using something you can compute (e.g., the size of a gzip compressed file) as a rough-and-ready substitute for something you can’t compute (e.g., the Kolmogorov complexity K(x)). In the interest of a full disclosure, a wonderful MIT undergrad, Lauren Oullette, recently started a research project with me where she’s trying to do exactly that. So hopefully, by the end of the semester, we’ll be able to answer Sean’s question at least at a physics level of rigor! Answering the question at a math/CS level of rigor could take a while longer.\nPS (unrelated). Are neutrinos traveling faster than light? See this xkcd strip (which does what I was trying to do in the Deolalikar affair, but better).\nComment #1 September 23rd, 2011 at 2:23 pm\nWhat about Logical Depth, as defined by Charles H. Bennett (see http://en.wikipedia.org/wiki/Logical_depth and http://bit.ly/nh0bra for more details)? It seems to me that it is also a good proxy for complexity.\nComment #2 September 23rd, 2011 at 2:44 pm\nAs you demonstrate, the first law of complexodynamics is: complexity is inversely proportional to reason. This is also known as the KISS rule: Keep It Simple, Stupid.\nOF COURSE complexity/interestingness peaks in the middle of an evolution, because that’s when the system is EVOLVING. Complexity/interestingness is maximized when things are changing. Initial state: delta = 0, boring. Final state: equilibrium, boring. Intermediate states: that’s where all the interesting stuff is happening. The monotonic Second Law is the same thing as the monotonic passage of time. Entropy only reaches its maximum, and complexity/interestingness returns to its minimum, when time stops and all evolutions are static. See the coffee cup slide.\nComment #3 September 23rd, 2011 at 2:55 pm\nConsider the 2^n state space of some 3SAT equation, where each vector represents either {satisfiable, not satisfiable}. Take a 3SAT instance which maximizes the complextropy of such state space (over all state spaces of 3SAT instances). Maybe this could be useful to prove some lower bounds on SAT.\nComment #4 September 23rd, 2011 at 2:56 pm\nFascinating stuff, Scott! I wonder if strings of human language would fall into the “high-complextropy” category. It would be interesting to test this empirically once a good model is developed.\nComment #5 September 23rd, 2011 at 3:49 pm\nComplexodynamics #2:\nOF COURSE complexity/interestingness peaks in the middle of an evolution, because that’s when the system is EVOLVING.\nYeah, I completely agree that that’s the intuition! The hard part is to come up with some formal, quantitative measure of “complexity/interestingness” that matches that intuition. When you actually try to do that, you run into lots of issues that I don’t think are nearly as obvious, and which are what this post was about.\nComment #6 September 23rd, 2011 at 3:49 pm\nThanks for the plug, Scott! And thanks even more for thinking about the problem. (Although now I’ll have to do a less-superficial job on the blog post I was planning myself…)\nI’ll certainly need to think more carefully about the careful definitions you’ve suggested, although we’re certainly thinking along similar lines. (I even thought of using file-compression algorithms.) It would definitely be fun to have an actual argument for a clearly-stated principle.\nThere does seem to be one difference of approach that may or may not be important. Namely, I’m convinced of the importance of coarse-graining, which you don’t seem to bring into play at all. Note that I did actually propose an (admittedly informal) definition of “complexity” on slide 15:\nhttp://www.slideshare.net/seanmcarroll/setting-time-aright\nNamely, “the Kolmogorov complexity of the description of each *macrostate* of the system.”\nObviously that relies on a fixed coarse-graining supplied ahead of time, to partition the state space into macrostate equivalence classes. This makes some people nervous because it seems arbitrary, but to me it’s both legitimate and crucial (at least I suspect so). In the real world, we can stare as much as we want at that class of cream and coffee — we’re not going to end up specifying the microstate by measuring the position and momentum of every single molecule. Our eyes just don’t work that way. We have available only certain macroscopic features of the system, and that is ultimately where the coarse-graining comes from.\nThat may or may not be a minor point, I’m not sure. Certainly the most interesting questions are the ones you identified, to which I have no good answers — in what way does complexity develop, at what speeds, etc.\nComment #7 September 23rd, 2011 at 3:52 pm\nActually I should say two more things.\nFirst, while “Complexodynamics” is certainly right that there’s a sense in which complexity must go up and then down in this case, it’s far from clear how much it goes up and in what way. In the coffee cup, we could imagine completely uniform diffusion, which would keep the complexity pretty low. In fact, that probably happens in an isolated coffee cup; for this picture I reached in and poked it with a spoon. But there are other systems in which the complexity goes up appreciably, like the universe. Any would-be law better handle this difference.\nSecond, Raissa D’Souza (who studies real-world complex networks) pointed out to me at the conference that it’s likely that an honest graph of complexity vs. time isn’t nearly that smooth, as complexity can grow and then crash and grow again. Something else that a proper law of nature better be able to accommodate.\nComment #8 September 23rd, 2011 at 3:59 pm\nThanks, Sean! The difficulty is that I couldn’t figure out how to formalize the concept of a “macrostate” in any satisfactory way. However, I completely agree that one needs, if not that concept itself, then something else that plays the same role (since otherwise the entropy will essentially never go up, as I said in the post)! In my definition, the restriction to efficient sampling algorithms is what plays the role that coarse-graining might play in a more physics-based argument (i.e., it’s the thing that prevents us from saying that the entropy is small because of detailed regularities in the microstate that no one could ever notice in practice).\nComment #9 September 23rd, 2011 at 4:13 pm\nAlejandro Weinstein #1:\nWhat about Logical Depth, as defined by Charles H. Bennett\nGreat question! I’ve been extremely interested in Bennett’s logical depth measure for a while, and I considered discussing it in the post, ultimately deciding against.\nThe bottom line is that I think logical depth is not the right measure for this job, because in contrast to sophistication, I don’t see any intuitive reason why the depth should become large at intermediate times in (say) the coffee cup example!\nRecall that the logical depth of a string x is (essentially) the number of time steps taken by the shortest program that outputs x. Now, to describe the state of a coffee cup with little tendrils of milk that are developing in some probabilistic way, it seems to me that we want to describe the boundaries of the all-milk region, the all-coffee region, and the regions with various mixtures of the two. Once we’ve specified those boundaries, an algorithm can output a microstate for the coffee cup that’s macroscopically indistinguishable from the observed one, by sampling coffee elements with independent probability p and milk elements with probability 1-p in those regions that have been specified to have a (p,1-p) mixture.\nNow, the above will probably be a sophisticated/complextropic description in the sense of my post, since specifying the boundaries of the milk tendrils might require many bits. But it won’t be a deep description: once you’ve specified the boundaries, actually sampling a microstate can be done in nearly-linear time! Therefore the depth need not become large at any point during the mixing.\nIf the above argument is wrong, I’ll be extremely grateful to anyone who can explain why.\nComment #10 September 23rd, 2011 at 4:28 pm\nI think this is a case of selection bias. We pay attention to emergent complex phenomena and give them a name. We name stars, not amorphous dust clouds. We notice galaxies, not diffuse intergalactic gas. We notice the cloud that looks like a duck. Some processes produce complexity, some don’t. We just get interested in the ones that do.\nEntropy always wins in the end, so eventually the emergent complex system decays.\nIt’s not so clear that there’s anything to explain.\n@Sean – Isn’t your coarse-graining the same as Scott’s definition of S – the least complex set within which x is effectively random?\nComment #11 September 23rd, 2011 at 4:37 pm\nRe: Scott #9:\nI’m not sure I understand your argument correctly, but couldn’t also the configuration/arrangement of the mixed boundaries be itself logically deep? There could be a huge number of different (p_i,1 – p_i)’s in the mixing zone, and the shortest sampling program might require extra time that’s dependent on the number of different mixing zones.\nComment #12 September 23rd, 2011 at 4:41 pm\n“Foster Boondoggle” #10: What’s interesting is that, in denying that there’s anything to explain, you threw around phrases like “emergent complex system,” simply assuming that people would know what’s meant by them! But, as I tried to explain in the post, defining those concepts rigorously enough that we can prove theorems about them is the main challenge here!\nTo put it differently: can you write a computer program that takes as input a raw bit string, and that decides whether or not that string encodes “emergent complex behavior”? How do you do it?\nThe task might sound hopeless, but in the post I proposed a class of programs to do exactly that: namely, programs that calculate the “complextropy” as defined in terms of resource-bounded Kolmogorov complexity. You’re more than welcome to criticize my proposal or suggest a better one, but you can’t do so while blithely using words for which the entire problem is to define those words! 😀\nComment #13 September 23rd, 2011 at 4:46 pm\nHenry #11: Yes, it’s possible that the logical depth could become large at intermediate times, or that it could do so in some model systems / cellular automata but not in others. I just don’t see an inherent reason for that to happen: if it did happen, it would seem almost like an accident! Maybe I’m wrong though.\nComment #14 September 23rd, 2011 at 6:16 pm\nInteresting article, Scott. But I must say I’m surprised you attended such a fantastically self-indulgent conference as Setting Time Aright. Even though it seems to have been somewhat fruitful for you, the monumental waste and extravagance of such an enterprise should be deeply repugnant to any morally sophisticated person–which I have always considered you to be.\nComment #15 September 23rd, 2011 at 6:35 pm\n“Sampson Brass” #14:\nthe monumental waste and extravagance of such an enterprise should be deeply repugnant to any morally sophisticated person\nWTF? It’s not like any public money was spent on this—just FQXi’s (i.e., the Templeton Foundation’s). If they want to pay to send me on a nice cruise, I’m morally obligated to say no? Can you explain why?\nComment #16 September 23rd, 2011 at 6:56 pm\nGee, Scott, I just meant that regardless of where the money came from, it shouldn’t have been spent as it was, on a luxurious cruise through waters far from the homes of many of the participants. Sort of like how an argument can be made that it is immoral (irrespective of all other factors) for a rich person to spend his or her own money on, say, a $500,000 automobile. We’re talking about indulgence, waste, extravagance; I don’t see how that can be justified in a world where so many people have so little. Though for the record I’ll bet the cruise was a lot of fun and I’m not sure I would have had the moral courage to refuse an invitation had it been tendered me!\nComment #17 September 23rd, 2011 at 7:24 pm\nOne (very simple) way to think about the notion of “coarseness” in the coffee example would be to look at all possible scales. For each size parameter, divide the cup into cubes of that size, and average the milk-coffee content in each cube. Now look at the time vs Kolmogorov complexity graph for the “pixellated” cup of coffee that you get at this scale. At very small scale, the graph will be the increase in entropy from the 2nd law of thermodynamic (assuming that the process is randomized, or assuming that the process is in some sense pseudorandom and that we are using time-bounded Kolmogorov complexity); at a scale large enough that the cup is a single cell, the Kolmogorov complexity is constant.\nIf you consider the whole 3-dimensional scale-time-complexity graph, however, there will be some “bump” at middle scales.\nI don’t know what would be good computational definitions that would allow a more abstract treatment in which one would see the same qualitative phenomenon.\nNotion of samplability and distinguishability seem related enough: if, after averaging over a certain scale, you have small descriptive complexity, then you have a sampler that, in each cell, will simply produce a random mix of coffee and milk according to the relative density, and this will be indistinguishable from “reality” by an observer that is not able to resolve what happens within cells.\nComment #18 September 23rd, 2011 at 7:31 pm\nSampson #16:\nExcept that, uhh, the “far from the homes of many of the participants” part is no different than any academic conference anywhere! By the triangle inequality, you can’t have an international conference without it being far from someone‘s home… 🙂 Furthermore, only 2 out of 6 nights were spent on the ship, and I really doubt that this conference was significantly more expensive than an ordinary conference in a hotel (those are quite expensive as well). And the ship was the National Geographic Explorer, which is not exactly a typical luxury ship.\nSo, I think it boils to a question raised in the Ask Me Anything post: should all academic conferences should be cancelled? But if so, why start with academic conferences? Wouldn’t it be better first to request a halt to ordinary vacations?\nIncidentally, this happens to have been the first time in my life on a cruise of any kind. I’ve never been to the Caribbean or Hawaii, and in fact I haven’t gotten on a plane “purely” for vacation purposes in the last decade. Can you say the same? How much money do you spend on “extravagances”? However much it is, shame on you! 😉\nComment #19 September 23rd, 2011 at 7:42 pm\nHi Scott – I didn’t mean to dis your post. I found it quite interesting. But I was responding to the opening question: “why does ‘complexity’ or ‘interestingness’ of physical systems seem to increase with time and then hit a maximum and decrease?” My point was just that it’s only those systems that we study. For lots of other systems this wouldn’t be true.\nTake your cellular automata test case, except instead of using an updating rule that leads to simple diffusion, randomize also over the rules and initial states. Most of those systems will be utterly boring – e.g., all the black pixels will disappear, or the system will remain chaotic and random at all times. We won’t talk about them because there’s nothing much to say. But one of the systems will be Conway’s Life, which will be initially chaotic, but then evolve through some interesting intermediate state with gliders and perhaps other more complicated structures, before eventually settling down to blinkers, blocks and other entropic debris. That’s the system we pay attention to.\nResponding to the opening question: it’s not true in general. It is for some small subset of dynamical systems and initial conditions, and those are the ones we talk about.\nComment #20 September 23rd, 2011 at 7:55 pm\nScott,\nRe: Samson\nDon’t even bother trying to defend this to someone who’s obviously not ever going to accept it. What’s the point?\nIf Samson feels so strongly about this type of thing, he can certainty make sure he never does anything so morally corrupt.\nAnd, if it’s really a problem for him, he can stop following your blog and spend his valuable time dong something else.\nWhat is this guy? A monk?\nHave some fun, relax, be creative and continue to come up with great things, which you seem to have a knack for\nBy the way, I hope this was a family affair, although I know it’s hard to organize this sometimes.\nComment #21 September 23rd, 2011 at 7:56 pm\nI have to back Sampson up on this one, I’m afraid. We didn’t make it public, but the invited speakers were treated to lavish caviar breakfasts and complementary between-session footrubs. Which was fine, but I thought the hot and cold running champagne was a bit morally reprehensible.\nAlso, Scott, I notice that you have a tendency to tell “jokes” during your talks. This squanders valuable time that could be used to convey information rather than simply being enjoyable, and furthers your reputation as a moral monster. And here I thought better of you.\nComment #22 September 23rd, 2011 at 7:59 pm\nFoster #19: I agree that there are plenty of systems that never do anything “complex” or “interesting,” no matter how long you let them run. On the other hand, this is a rare case where I might need to side with Wolfram 🙂 : it seems to me that “complex behavior” is the rule rather than the exception. Crucially, in using the word “complex” here I don’t mean anything as grand as life or intelligence: all I mean is behavior (like that of Sean’s second coffee photo) that doesn’t manifestly fall into one of the two categories of\n(1) simple and predictable or\n(2) completely entropic and random.\nYou mention that “most” cellular automaton rules lead to something boring, but it seems to me that that’s an artifact of restricting the search space to a tiny set of rules (e.g., 1-dimensional, 2-state, nearest-neighbor). As you increase any of those parameters—e.g., the number of colors per pixel—I conjecture that the “interesting” rules will come to asymptotically dominate, and moreover that they’ll do so extremely quickly. It would be great to prove that, if it hasn’t been done already.\nComment #23 September 23rd, 2011 at 8:22 pm\nFoster,\nAs you you seem to be hinting (and thinking), the question is implicitly anthropic. That is, as observing participants in this universe, we find that the universe evolves this way because, if it didn’t, we never would have been part of it. To the extent that we imagine ourselves as external observers, we still find only this kind of universe worth talking about, because it is complex enough to accommodate us as observing participants (actively observing subsystems). That is, it resembles the universe in which we actually find ourselves, and which manifestly sustains our existence, at least for a while.\nThat sounds like another attempt to write off the question as mostly vacuous, but I have something else in mind. Such a universe is one in which we can think and do science. That is, for a significant period of time it sustains that peculiar combination of stability (reproducibility) and dynamism (and diversity) in which it makes sense to pose scientific questions and attempt to answer them. In particular, it prompts us—again, as participating subsystems—to seek stability and invariant patterns amidst and underlying change, and gives us some hope of identifying them before the clock runs out on our species.\nComment #24 September 23rd, 2011 at 8:37 pm\n[PS: Is there a Godwin’s Law for allusions to the Anthropic Principle? 🙂 ]\nComment #25 September 23rd, 2011 at 8:50 pm\nScott’s conjecture in #22 that “interesting” rules asymptotically dominate as CA algorithms become more complex is extremely interesting, if true. I actually don’t even have an intuition either way. Does anyone think something along those lines could be provable?\nComment #26 September 23rd, 2011 at 9:18 pm\nI know this isn’t the central issue, but as background for computer scientists, it would be very nice to have a precise formulation of the 2nd Law *as a statement about Cellular Automaton behavior*, as well as some sense of its scope.\nFor which CA rules/initial states does it hold true? is the Law provable? Is it ultimately a tautology, or not? Are we discussing a single notion of “macro-states”/”coarse-graining”/”pixellation”, or allowing for a more abstract setting? And so on.\nI’ve read a few web pieces by Wolfram and others on this issue, but none that satisfy me. Can anyone help out?\nComment #27 September 23rd, 2011 at 9:31 pm\nLike all life, we harvest energy from entropy gradients. This may be why the middle glass is most interesting to us. We can’t do anything with the glass on the right, so there’s no need to pay attention to it. The glass on the left is easy – I can shake it and generate electricity. But competition for such easy gradients pushes us to ever harder ones, and we evolved reward systems to help us.\nThis suggests invoking something like Landauer’s principle. Define complextropy for dynamic systems sampled in time. At each sample, observe the system and predict its next state. The bits you get right are counted toward your booty. Run your algorithm for free but in place, so you spend energy resetting registers and so forth. Complextropy could be t/(b-c), where b is the number of bits in your booty, c is the number you erased (energy cost), and t the number of samples. It will be negative for the glass on the right, small for the glass on the left, and large for the glass in the middle. Buy as much memory as you like for best performance, as long as it is small compared to b.\nThere may be a way to extend this to static examples like an *image* of a coffee cup. But starting from such examples may be trappy. They look interesting, perhaps only because we’ve learned to quickly recognize the dynamic systems they’re part of.\nComment #28 September 23rd, 2011 at 9:36 pm\nLet P(c|r) be the probability density of color c at position r. \\sum_c P(c|r) = 1. Let \\mu([a,b]) = b-a. Generalize \\mu(.) to a proper measure.\nCan’t you use the following non-sophisticated measure of sophistication?\n\\mu( range(P(c|r)) )\n(it’s zero initially and finally but not in between. You can average over all positions r if you want a bulk measure )\nComment #29 September 23rd, 2011 at 9:39 pm\nAddendum: by range(.), I meant the range of the function P(.|r) over all c at fixed r\nComment #30 September 23rd, 2011 at 9:59 pm\nSince no one has opened this can of worms, I’ll make the first neutrino comment.\nI don’t think the situation is as analogous to the Deolalikar case as people would think. I think this is actually far more probable of being valid (not that I am saying anything about how absolutely likely or not it is that it is valid). The first reason, is that you can’t ignore the fact that it is a post five sigma result, hell it’s post six sigma. Now, that doesn’t mean there’s not some unseen systematic errors, but these guys seem to be pretty good at what they do, and it appears that they have been working their asses off trying to find any potential systematic errors. In their arXiv paper they are careful to never use evocative “superluminal” language and express an extremely conservative viewpoint on the matter. Again, this doesn’t mean it’s not a systematic error, but if it is it would have to be slipping by a large number of very skeptical skilled professionals that are actively and sincerely hunting for it.\nThe next thing is that contrary to the popsci (and even some legit sci) word floating around, this does not violate SR! SR works fine with superluminal particles, but they open the possibility of retrocasual signals and either the mass must become imaginary OR both the energy and momentum must become imaginary. All of these things seem ridiculous, but given some of the radical revolutions physics has seen, its not entirely out of the question. Even with superluminal particles, retrocasual signals could end up being restricted in other ways, or perhaps a new picture of causality would be needed. In fact, there is already some theory results out there that show certain tachyons can’t be used to signal FTL.\nOf the other implications the least ridiculous (though still ridiculous) of these is the imaginary mass, or negative mass squared. Coincidentally or not, negative values for the mass squared of the neutrino were measured multiple independent times over a decade ago! In fact it seemed like everyone that tried measuring the square mass of the neutrino found it to be negative. The first of these cases had a lot of noise, but I believe some of the results eventual got to a few sigma. For some elusive reason that no one seems to know, no fuss was ever made over the results. It sparked some theoretical papers on superluminal neutrino models, but overall it faded away. Even the critics of the Opera result note these old results and admit that they can’t come up with a dismissal any better than “well we stopped hearing about it so it was probably nothing”.\nOn top of all this, their have been level headed people arguing that neutrinos are tachyonic for over 20 years, and there are a number of reasons why they did so. Furthermore, there is nothing in any law of physics directly forbidding tachyons as far as I know. The only things that do are the result of a number of dearly held intuitions being forced upon the laws of physics. And if the last century has taught us anything, it is that we shouldn’t get too cozy with our deepest intuitions.\nKeep your hand on your wallet, but be ready to take your credit card out if the time comes.\nComment #31 September 23rd, 2011 at 10:03 pm\nTo the extent that this is a “why” question, and not merely a matter of formally characterizing the complexity of the “middle” state, what about the role of gravity? After all, isn’t it the universal attraction of gravity, including the expansion of the universe as described in general relativity, that drives primordial matter towards this complicated non-equilibrium state? I’m surprised Sean Carroll didn’t bring this up.\nComment #32 September 23rd, 2011 at 10:03 pm\nrrtucci: Thanks for the interesting suggestion! However,\n(1) Won’t μ(range(P(c|r))) always be 0 in a discrete system like the one we’re considering, since there are only finitely many c’s and r’s, hence finitely many P(c|r)’s?\n(I agree that one could easily fix this problem, for example by coarse-graining over the unit interval.)\n(2) Your measure only makes sense given a probability measure over the microstates. However, I want a complextropy measure that (at least in principle) can be calculated for a specific microstate, and (a related requirement) makes sense even for deterministic dynamical systems. That was my motivation for bringing in Kolmogorov complexity. (Sorry I didn’t make that more explicit in the post!)\n(3) Your measure is (of course) rather tailored to the coffee cup example, and could be invalidated even by small changes to that example. For example, suppose I told you that there were equal amounts of coffee and milk, and that with probability 1/2 the milk started out on top of the coffee, and with probability 1/2 the coffee started out on top of the milk. In that case, symmetry considerations imply that P(c|r) would always be 1/2, for every c and r and at every time step. Yet it still seems intuitively like the complextropy starts out small, increases, and then decreases again.\nComment #33 September 23rd, 2011 at 10:11 pm\nPS: I should have said “…, along with the expansion of the universe as described in general relativity, …”.\nComment #34 September 23rd, 2011 at 10:25 pm\nJustin #30: I agree with almost everything you say (as usual, Sean Carroll did a great job summarizing the issues). But of course, during the Deolalikar affair, there were also lots of serious people making serious arguments for taking that claim seriously! It was only long experience with wrong P≠NP proofs that emboldened me to bet against.\nAs for the neutrinos, I’m obviously far from an expert, but am moved by the following two points:\n(1) Closed timelike curves seem to me to be a different order of strangeness from anything thus far discovered in physics—like maybe 1000 times stranger than relativity, QM, virtual particles, and black holes put together. And I don’t understand how one could have tachyonic neutrinos without getting CTCs as well—would anyone who accepts that possibility be kind enough to explain it to me?\n(2) As I understand it, the possibility of systematic errors in an experiment of this sort are legion, no matter how careful the experimenters are. And if there is a systematic error, then presumably there’s a 50% chance that it’s going to be in the direction that was reported! In other words: once someone decides to search for particles going ~1+10-5 times faster than the speed of light, the prior probability that they’ll find what look like such particles seems to me to be quite high, even under the assumption that no such particles exist.\nComment #35 September 23rd, 2011 at 10:48 pm\n“In answering Sean’s provocative question (whether there’s some “law of complexodynamics” that would explain his graph)”\nI’m gonna side with [Complexodynamics: Comment #2] here, as in:\nYeah, it’s called “Rolle’s Theorem”:\nIn calculus, Rolle’s theorem essentially states that a differentiable function which attains equal values at two distinct points must have a point somewhere between them where the first derivative (the slope of the tangent line to the graph of the function) is zero.\nComment #36 September 23rd, 2011 at 10:57 pm\nIThinkImClever: Your argument is “clever” but wrong! The constant zero function would also satisfy Rolle’s Theorem. Therefore, that theorem is manifestly irrelevant to the problem that I stated: explaining mathematically why the complextropy becomes large and positive at intermediate times.\nComment #37 September 24th, 2011 at 12:12 am\nJustin: I’m no physicist and I haven’t read the results about negative squared mass of neutrinos, so I’m quite underqualified to comment, but:\nI’m quite skeptical about the negative squared mass claims, because how would one even set up an experiment to measure imaginary mass? The caricatured method that comes to mind is this: measure the velocity of neutrino, plug in velocity into relativistic Lorentz equations, and “observe” imaginary mass! Please let me know if this resembles the actual experiments at all.\nIf the actual experiment were anything like this, then I believe we would have the same questions about the method of measuring velocities of neutrinos.\nThen, it would be hard to say that the results of the imaginary mass experiment support the results of FTL neutrino experiment, because the former relies on the latter!\n(Apologies for diverting comment thread from “complextropy”!)\nComment #38 September 24th, 2011 at 1:44 am\nSeems at least one physicist has had the same idea as you!\nChang Kee Jung, a neutrino physicist at Stony Brook University in New York, says he’d wager that the result is the product of a systematic error. “I wouldn’t bet my wife and kids because they’d get mad,” he says. “But I’d bet my house.”\n(Hat tip to hegemonicon on LessWrong.)\nComment #39 September 24th, 2011 at 1:46 am\n(I guess not really, as I don’t see anywhere where he’s announced that he actually *is* betting his house, and stated terms. But someone was going to say it. 🙂 )\nComment #40 September 24th, 2011 at 2:41 am\n@Scott: Comment #36\nOK, fine.\nBut note that I claimed only to ever be clever, and not right. 😉\nAlso, I won’t maintain here that the constant zero function case would actually cause the question to be trivial and uninteresting. 🙁\nAnyhow, after thinking about “complextropy” a bit more, I am now wondering:\n1) Why are we assuming that “equilibrium is simple”, when the length of the *minimum boolean expression* required to describe the ‘mixing’ system *exactly* at time step t grows rapidly as entropy increases?\n(BTW, how about these minimum boolean expressions as an objective, calculable criterion for “complextropy”, on say, your 2D array of black and white pixels?)\n2) In terms of universality (of whose threshold of acquirement is extremely low), once acquired by, or present in a system, where is there left to go? What is more ‘complex’ than a ‘universal computer’ on which these ‘particles’ are interacting on? Aren’t we ‘maxed out’ pretty early on in the game?\n3) Perhaps we need better definitions of the ‘players’ involved, especially physical ‘randomness’? Maybe, in the end, only pseudorandomness really exists, if indeed the universe is ultimately discrete at the lowest level, and everything has low Kolmogorov complexity.\n*As an aside, on the Neutrino Debacle: Yeah, again, time to again first invoke the KISS Principle and/or Murphy’s Law. It’s most likely a simple error.\nComment #41 September 24th, 2011 at 2:57 am\nw.r.t the Neutrino Debacle: I would now be cautious to taking a betting position against “superluminal neutrinos”, but ONLY because Nikola Tesla somewhat foresaw it, and Tesla was NEVER wrong. 😀\n“In 1901 Nikola Tesla was one the first to identify “radiant energy.” Tesla says that the source of this energy is our Sun. He concluded that the Sun emits small particles, each carrying so small of a charge, that they move with great velocity, exceeding that of light. Tesla further states that these particles are the neutron particles. Tesla believed that these neutron particles were responsible for all radioactive reactions. Radiant matter is in tune with these neutron particles. Radiant matter is simply a re-transmitter of energy from one state to another.”\n“All of my investigations seem to point to the conclusion that they are small particles, each carrying so small a charge that we are justified in calling them neutrons. They move with great velocity, exceeding that of light. More than 25 years ago I began my efforts to harness the cosmic rays and I can now state that I have succeeded in operating a motive device by means of them. I will tell you in the most general way, the cosmic ray ionizes the air, setting free many charges ions and electrons. These charges are captured in a condenser which is made to discharge through the circuit of the motor. I have hopes of building my motor on a large scale, but circumstances have not been favorable to carrying out my plan.”\nComment #42 September 24th, 2011 at 9:00 am\nJust a sentence or two about neutrinos. Even if neutrinos are tachyonic, you still have to explain why they aren’t traveling at speeds closer to the speed of light, given their low mass. To me, the really unlikely thing about this measurement is the magnitude of (v_neutrino – speed of light), though I’ll admit that the sign is weird too.\nComment #43 September 24th, 2011 at 2:35 pm\nThis topic has fascinated me for years, and I’m convinced (intuitively) that it will remain an open-ended problem.\nI was going to suggest the evolutionary, and then the anthropological aspect, but I see others here already have.\nThis leaves the “Edge of Chaos” aspect, which may help round out your research.\n1993—Melanie Mitchell & James Crutchfield & Peter Hraber—Dynamics, Computation, and the “Edge of Chaos”: A Re-Examination http://www.santafe.edu/research/working-papers/abstract/e5b0ef2ae9887b454ea8501f4a9568a7/\n2010—Thierry Mora & William Bialek—Are biological systems poised at criticality? http://arxiv.org/abs/1012.2242\nI believe the Kolmogorov-based approach is a dead end, as we inherently lack the necessary context within the vast space of possible evolutionary trajectories to predict which mathematical combinations will represent synergistic and persistent, thus “interestingly complex” novel structures.\nAs to the anthropological question of why *we* find certain structures “interestingly complex”, I believe this is doable in theory, and to the extent it is achievable it could provide a useful building-block for comparative modeling of systems of values and ethics.\nComment #44 September 24th, 2011 at 2:57 pm\nI started to comment on Scott’s CTC worry, but it turned into a blog post:\nhttp://blogs.discovermagazine.com/cosmicvariance/2011/09/24/can-neutrinos-kill-their-own-grandfathers/\nComment #45 September 24th, 2011 at 4:07 pm\n@Justin Dove: Despite the fact that superluminal signalling may not be possible for certain classes of tachyons, from what I understand, if the result is correct, superluminal signalling shouldn’t be that hard – the 0 bit could be represented as a small number of neutrinos being sent (and detected) while the 1 bit could be a much larger number of neutrinos being sent (and detected). Varying this, one should be able to send superluminal messages.\nComment #46 September 24th, 2011 at 4:12 pm\nJef Allbright #43:\nI believe the Kolmogorov-based approach is a dead end, as we inherently lack the necessary context within the vast space of possible evolutionary trajectories to predict which mathematical combinations will represent synergistic and persistent, thus “interestingly complex” novel structures.\nLook, there’s clearly the issue that, while the second coffee cup in Sean’s picture is more “interesting” than the first or third cups, it’s still not very interesting! (No offense, Sean.)\nSo it would be nice to have a complextropy/interestingness measure that assigned an even higher score to, for example, the coffee being drunk by a baboon wearing a top-hat.\nHowever, I took at as obvious that the goal here was not to construct an “ultimate theory of interestingness” in one go—yes, I agree, that sounds pretty hopeless!—but merely to do somewhat better than entropy as an interestingness criterion, by matching our intuition that the second cup is more interesting than either the first or the third. I should have made that clearer in the post.\nComment #47 September 24th, 2011 at 4:35 pm\n(Why are people talking about neutrinos in response to a post that’s not at all about neutrinos?)\nI’ve decided to cast my vote for the “logical depth” version. Assume we have some finite system evolving according to some rules. If the system has a simple initial state, and if the rules are “interesting”, and if the system’s rules don’t discard information, then the logical depth will tend to increase linearly at first: the shortest description of the current state will be to give the initial state and the amount of time that’s passed. However, after a long time (on the order of the number of possible states of the system, in the worst case), it will no longer be worth it to describe the state via the entire history. At this point the logical depth will at least hit a ceiling. Again assuming the rules are sufficiently “interesting”, the logical depth will drop back down to near 0, because at this stage the system state will look random (and that’ll be it’s shortest description).\nA word on my assumptions.\nFirst, the assumption that we don’t discard information. I make this assumption because it’s needed for thermodynamics in the first place: if information ever went away, entropy could decrease. I think it’s also needed for my result here.\nThe relevant definition of “interesting” may sound like a sticking point. Intuitively, I mean that as the state evolves, it doesn’t stay simple. (The cream doesn’t stay at the top of the coffee, nor does it fall to the bottom in a straight line.) A decent definition for my purposes is that any state can reach a large portion of other states. (IE, for any initial state we choose, if we keep running the simulation for long enough, it will pass through at least 1/4 of the possible configurations– 1/4 being chosen arbitrarily). Since the vast majority of states have high kolmogorov complexity, this is enough to guarantee that the Kolmogorov complexity will increase over time and eventually hit the upper bound for the system, ie, the complexity of a randomly chosen configuration for that system. At this point the logical depth of a state will almost always be on the order of the log of the number of system states.\nWill the logical depth exceed the log of the number of system states before that, though? I suspect so, because I suspect the best way to describe the system will be its history for a long time (on the order of the number of system states), and for as long as that’s the case, depth increases linearly. However, I’m not totally sure I can prove this based on my current definition of “interesting rules”. (They may not be interesting enough!)\nComment #48 September 24th, 2011 at 4:48 pm\nThis seems to me to be about coarse-graining, the picture of the cup on the left consists mostly of black and white pixels, neatly ordered.\nThe one in the middle is more interesting because it consists of mixed pixels of different colors, while the one on the right again has a simple description (all pixels are ‘brown’).\nBut of course the brown pixels hide the many possible microstates of black and white little drops. So the simple description (but with high entropy) is due to coarse-graining and ‘averaging’ of microstates.\nSo the question is why coarse-grained descriptions play such an important role and I would say the answer is that we human beings don’t care about the statistics of microstates but instead pay most attention to what our eyes report and they are coarse-graining in a particular way – optimized by evolution to make ‘important’ stuff interesting to us and boring stuff (=high entropy stuff) uninteresting to us.\nComment #49 September 24th, 2011 at 6:41 pm\n@ comment #8 (& #48):\nI think we don’t have to take the terms macro/micro too literally. Course-graining, to me, is about partial information. We can’t fully observe the state of the coffee, and a “macro-state” is just a set of states that we can’t distinguish from one another. So, to define entropy for deterministic systems, we make them partially observable. This is functionally similar to making them stochastic, since it means that we have uncertainty about the system.\nComment #50 September 24th, 2011 at 7:42 pm\nThere’s a really nice collection of work by Bill Bialek and collaborators on “predictive information” defined as “the mutual information between the past and the future of a time series.” In it they “argue that the divergent part of [the predictive information] provides the unique measure for the complexity of dynamics underlying a time series.”\nThis paper is very well written and, with a thorough discussion of its relation to previous work, is also a useful reference for those interested in these issues:\nhttp://www.princeton.edu/~wbialek/our_papers/bnt_01a.pdf\nComment #51 September 24th, 2011 at 9:05 pm\n@Henry The old experiments did *not* measure the mass in that way. They measured the mass squared “directly” using analysis of the beta spectrum of tritium decay. I’m not going to pretend to understand it completely, but there’s a large amount of literature and references at http://arxiv.org/pdf/0909.2104v1. The results were almost all (with a few exceptions) consistent with the possibility of positive mass squared values (within an error bar), but they nearly unanimously favored the negative side.\nAs time has gone on they have gotten closer to c, but oddly the absolute measurements are still often coming up negative. Now these results are not very interesting given the uncertainty, but the coincidence of the superluminal claims makes them fun to toy with at least.\n@Scott I agree. The main difference I find with the Deolalikar situation is that this is a claim coming from a large group of people that *weren’t* looking for this, as far as I can tell (as opposed to one person that was). Furthermore, I’ve also read that they have spent months shaking their heads in disbelief and looking for systematic errors with the help of metrology folks. I get the impression that they had no intention of publicly announcing this, but ultimately it came to the point where they couldn’t find the error (if it exists) and needed to open it up to the peer community.\nAs far as CTC’s go, you could always go with Lorentz violations, but honestly I find that probably even more unappetizing. The other thing is to envoke some sort of consistency principle that forces ctc’s to only exist in consistent ways.\nAgain, I’m by no means claiming to believe this claim is true. But I just think its worth much more serious care and considerations then many other such incidents.\nComment #52 September 24th, 2011 at 10:31 pm\n@Wolfgang may be on to something. If you look at the JPEG encoding (using DCT) of the image of the coffee, the less interesting cups have higher coefficients in the upper left and lower right of the DCT matrix. The interesting middle cup has lots more going on in the middle of the matrix. (You don’t get this effect with PNG or GIF images of coffee, so it may just be serendipity that Sean used JPEG.)\nJPEG works by breaking an image into 8×8 pixel squares and doing a transformation that measures the visual complexity in the horizontal and vertical directions. The top left coefficient of the matrix is basically the average value of all the pixels and the bottom right a more or less checkerboard mix. From a distance, these two extremes both correspond to something that looks like gray. If you want interesting structure, you need stuff in the middle of the matrix.\nComment #53 September 25th, 2011 at 2:15 am\nScott @ 22, Sean Carroll @ 25:\nI’d conjecture the opposite, that higher-dimensional (more complex rules because larger neighborhoods) spaces of CA rule sets have smaller subspaces containing “interesting” rules. If the “edge of chaos” is a real phenomenon (and there was some controversy about that the last time I looked), then it represents a hypersurface in the rule space around which “interesting” rules cluster, and that’s going to be a smaller percentage of the total space as the dimensionality increases.\nIn any case I rather doubt we’ll ever have a way to determine the complexity of a CA rule in less time than it takes to execute it; that would seem to violate the undecidability of the Halting Problem.\nComment #54 September 25th, 2011 at 2:49 am\nDear Scott, these are nice thoughts. Here are a few comments.\n1) The idea that the complexity of a system increases in intermediate states is nice. Kolgomorov complexity appears to be much too complex notion of complexity to express this idea. Maybe some notions from ergodic theory (according to which both deterministic sequences and very simple random sequemces are “simple”) can be more relevant. (So perhaps “complex” should mean that the system does not behave deterministically; where by laws of large numbers simple random processes do behave deterministically.)\n2) A related idea is that in physical systems, complex behavior is witnessed near criticality. Namely, if the system depending on some parameter t witnesses a phase transition for t=T_0 then it is considerably more “complex” when t is close to T_0.\n3) There is some difficulty with the assertion that equilibrium states are “simple” with what we know about possible complexity of equilibrium states, be it in the context of quantum ground states, or in the context of protein folding.\nIt is not clear that we can usiversally claim that the equilibrium states must be “simple”.\n4) One idea that I found appealing is that for certain purposes clocks of physical systems should be scaled in a way that the rate of time at a time T is inversly proportional to the “complexity” of the system at time T. So time passes slowly at interesting times and quickly at boring times.\nComment #55 September 25th, 2011 at 3:06 am\nThe difference between pictures 1,3 and 2 is the ability to predict color of the “cap”, based on their neighbors. The color distribution is almost everywhere uniform. In the “middle cap”, (I mean most interesting one) the knowledge of the “color” (say, some coding of possible configurations) for the “box” at any scale have no/minimal predictive power at distant “box” of the same scale, leading to situation similar to fractal images.\nComment #56 September 25th, 2011 at 4:57 am\nThere is, however, a difference between the Deolalikar affair and the current ftl neutrinos issue. The first one was original research claiming something groundbreaking, and attempting to supporting it in a scientific manner, even if the proof is not actually correct. The second is the case of the media trying to make news from a strange experimental result where the physicists apparently don’t claim to have found anything groundbreaking, which phenomenon is illustrated in the strip “http://www.phdcomics.com/comics/archive.php?comicid=1174”.\nComment #57 September 25th, 2011 at 11:01 am\nI feel free to ignore the fact that it’s a “post six sigma result” because the number of sigmas is not a measure of reliability. It’s closer to a measure of how bad your initial assumptions are—not just the null hypothesis, but also anything regarding the experimental design. If it’s true that there are many possible systemic errors in the experiment (I’m nowhere near a physicist, so I can’t say), then that’s most likely what all those sigmas reflect.\nComment #58 September 25th, 2011 at 1:55 pm\nFor the record, I’ve long believed that there’s a similar argument to be made for an informational (or, in your language, complexodynamical) sweet spot in art. We don’t seem to derive a strong aesthetic response to paintings that are either too simple or too complex, because the former are boring and the latter are noise. There does seem to be a middle ground in which a painting contains the “right” amount of information.\nOf course, any such measurement is tricky. If I turn the Mona Lisa upside down, or break it into 1″x1″ tiles and permute them, I have a design that’s very close in terms of information, but which may produce a quite different aesthetic response.\nOf course, this is all pseudoscientific daydreaming until someone (not me) can produce a suitable mathematical definition, combined with a psychological validation. I will note that Jim Bumgardner did say something very similar in a short essay on information theory and art (http://www.mungbeing.com/issue_3.html?page=9#235), though using the language of Shannon entropy rather than Kolmogorov complexity.\nComment #59 September 25th, 2011 at 2:16 pm\nComplexodynamics: While this is clearly an interesting direction, there is a major problem likely(?) to to prevent any useful results:\nThere is no reason to think there is ANY scalar (as opposed to multidimensional) function that meaningfully captures the notion of complexity (let along “interesting-ness”).\nThe situation is very different in thermodynamics, where entropy S has a reasonable, not arbitrary, definition. Key thermodynamic concepts are defined as derivatives of S, or with respect to S, and they agree with measurable quantities defining heat engines, etc.\nGrasping for a workable definition of complexity seems rather like trying to define IQ.\nWhy discuss neutrinos right now? Well, because it would be a once in a century event. My (purely intuitive) estimate on the likelihood of this holding up: one in a million.\nComment #60 September 25th, 2011 at 5:11 pm\n“Grasping for a workable definition of complexity seems rather like trying to define IQ.”\nYeah, I’m pretty sure Wolfram would have accomplished this, if at all possible, in his 20+ years and ‘million mouse miles’ of researching complexity for his tome, but I’m still rather happy with his book as an overall informal definition of this “Complexodynamics”.\nComment #61 September 25th, 2011 at 6:01 pm\nRaoul #59: Once again, the key point is that I had no pretensions to define “complexity” for every possible purpose—only for the specific purpose of answering Sean’s question about the three coffee cups.\nFor some reason, many, many commenters seem to have missed that, and assumed that I was trying to do something much broader than I was.\nSpeaking of which, IThinkImClever #60: I’ve read Wolfram’s entire tome, and as far as I remember, he never once discusses the question of Sean that this post is about.\n(More generally, Wolfram never offers any formal definition of complexity—his notion of complexity is analogous to Justice Potter Stewart’s “I know it when I see it” definition of pornography. His attitude toward people who try to formalize and prove things, as opposed just to eyeballing thousands of CA printouts, is one of open disdain.)\nComment #62 September 25th, 2011 at 7:13 pm\nWell, as a quick response, I’d say that while Wolfram never focused on any *one* specific question in NKS, I think he did succeed in his overall general goal of elucidating the origins of complexity, and why it occurs at all.\nAnd you’re right, in sections where he mentions definitions (e.g. Defining Complexity, pg.557), he only considers how it *might* be formally defined, given our limited processes of perception and analysis.\nAnd while I wouldn’t ever have a disdain at formalizing and proving things, I am with Wolfram for now in just accepting as axiom that you never need to go beyond the elementary CA’s, as processes will only ever evolve behaviours that can be ‘captured’ by the 4 atomic/primitive patterns they exhibit: simple, repetitive, complex, random.\nBut that being said, I think your attempt at formalizing Sean’s question is quite clever, using the formally defined concept of Sophistication within Kolmogorov Complexity.\nI think we all see what you are trying to get at, but of course we are naturally gonna go off on tangents. 🙂\nComment #63 September 25th, 2011 at 8:06 pm\nIThinkI’mClever,\n“Yeah, I’m pretty sure Wolfram would have accomplished this, if at all possible, in his 20+ years and ‘million mouse miles’ of researching complexity for his tome, . . .”\nSo, you’re saying it’s impossible because Wolfram hasn’t accomplished it, because it’s “impossible” — really, your “pretty sure” about all of this?\nWhat is it do think went wrong; do the the laws of physics say it’s impossible? Or, is it that humans just aren’t bright enough to “really” figure it out?\nAnd, if we haven’t, why is that, because if we had it would have been this guy Wolfram who would have manged to figured it out already?\nWhat is it exactly you’re trying to say — other than being just a little bit off-putting? 😉\nMaybe, just maybe, notwithstanding his past accomplishments, Wolfram simply has the wrong approach.\nAs for me, I think we will accomplish defining complexity — around about the same time as we get to a good working definition of creativity and create AI Programs.\nI know, this doesn’t seem to be close at hand, but I’m hoping for a surprise.\nComment #64 September 25th, 2011 at 8:35 pm\nWell, it’s not impossible because Wolfram hasn’t accomplished it. Anything\n“So, you’re saying it’s impossible because Wolfram hasn’t accomplished it, because it’s “impossible” — really, your “pretty sure” about all of this?”\nNo. Anything’s possible, right? I think I was just giving my informal and humble views on the topic.\n“What is it do think went wrong; do the the laws of physics say it’s impossible? Or, is it that humans just aren’t bright enough to “really” figure it out?”\nWell, I think there exists uncomputable and irreducible things, as well as concepts that can’t be contained in a few symbols.\n“And, if we haven’t, why is that, because if we had it would have been this guy Wolfram who would have manged to figured it out already?”\nAlthough he can be a bit too ‘proud’ at times, I recognize and respect Wolfram’s genius on the topic of complexity in general.\n“What is it exactly you’re trying to say — other than being just a little bit off-putting?”\nReally? I was just putting forth my limited views on the matter. I guess I’m not as clever as I once thought.\n“Maybe, just maybe, notwithstanding his past accomplishments, Wolfram simply has the wrong approach.”\nMaybe. But as he puts it, the bits are the bits. You can’t argue with all of his results, at least.\n“As for me, I think we will accomplish defining complexity — around about the same time as we get to a good working definition of creativity and create AI Programs.”\nYeah, me too. But for now, it’s between simple and random. 😉\nComment #65 September 25th, 2011 at 10:17 pm\nScott,\nHave we met?… I ask as I sit absently scribbling expanded-dimensionality SR diagrams on what I’ve belatedly realize is the backside of CSAIL memo paper I picked up when I last visited the Seussaplex (yes, that’s my term, but I did get laughs with it and I don’t _think_ anyone was offended… 🙂\n(Do you know Russ T? I always love hearing updates on the work he’s doing on on expanding regions of stability. It’s a mathematical approach that’s giving results intriguingly similar to biological systems, even though the underlying mechanism are entirely different, as Russ always notes. That kind of unexpected parallelism is simply intriguing!)\nI thought I understood Kolmogorov complexity. Actually, I _still_ think I understand it, since I used to like to give the example of how an arbitrary string pulled deep from within pi can in principle be generated by a very short program, but good luck on figuring that out if you don’t know it’s from pi ahead of time!\nThe trouble is that on first whack, I just don’t get where you are heading in trying to link that particular idea with the emergence of “interesting” complexity in our universe.\nIf your main point is that there is likely a deep connection between seemingly mundane data compression and the workings of the physics of our universe as it stands, and that this link also ties in to how intelligent systems work, I’m absolutely with you, you’re preachin’ to the choir [SC you never read that line], etc.\nIn fact, I would go so far as to say that I think a deeper understanding of multilevel information factoring — data compression — needs to play a major role in the science of intelligence if we ever want to build robotic systems as small, fast, and energy-efficient as say insects.\nSo, I clearly need to read your ideas more closely and will try to do so soon. I’ve only skimmed, and I know it.\nMy blog question to you would be this: If you had to give a two-line pitch of your most critical insight or postulate to an expert — not just to some random person on an elevator! — what would it be? You could use equations, big words, small words, cryptic, or simple, just as long as it captures what you think is most important.\nMaybe it’s already in there, in which case your best reply to my query may be “Go back and read paragraph x, you skxawng!” I do believe in and try to practice ego-free paper reading whenever possible… 🙂\nCheers,\nTerry Bollinger\nComment #66 September 25th, 2011 at 11:17 pm\nOK, since I apparently came off as unproductive to some readers, let me try to make up for it by giving my ‘more formal’ take on ‘complexodynamics”:\nSo it would seem that any formal definition of “complexodynamics” would necessarily require that *localized structures* can exist, that is, eventually appear and evolve and die, giving “complextropy”, within the underlying representation, say, bits here.\nAnd it would appear that Scott’s Sophistication measure magically “picks out” these families of bit-strings that are not too simple, not too random, but just right: they have provable clusters of *localized structure*. (I say ‘magically’ here because we don’t know their origins. How were they ultimately constructed?)\nSo what’s going on in the coffee cups? Brownian motion. Random motion. But why should this random motion produce ‘interestingness’, or rather, temporary localized structures, at all?\nWell, as time passes, the partitioned 0’s and 1’s randomly interact and trespass in each other’s initial territories. And since this interaction is indeed a random process, we *should* expect that localized structures will eventually be produced, otherwise the ‘mixing’ will occur uniformly, and hence not fit the definition of randomness, thus contradicting the brownian fluid flow under which the particles operate!\nSo the middle cup is ‘sophisticated’ simply because of how random processes work: flipping a fair coin does not go 0,1,0,1,…,0,1. It eventually produces temporary streaks of repetitions, or in the coffee case, temporary localized structures of ‘milk tendrils’.\nBut alas, as more time passes, the localized structures are forced to ultimately interact with each other, becoming highly interconnected and “smeared”, and thus disappear from effective measure.\nHence, I conclude that randomness is one origin of complexity, as it effectively takes simplicity from one end of the spectrum to the other: from partitioned, ordered elements to mixed, disordered elements.\nComment #67 September 26th, 2011 at 12:40 am\nI’ll go and disagree with commenter #2 and your reply. Change plays a role of course, but it’s not the main point. If it were, you could define complexity as the time derivative of entropy, which doesn’t make much sense. Of course you can define what you want but you can come up with a lot of systems that have a steep increase in entropy without being complex in any way that we’d naively think of as complex. On that note, I also have an issue with Sean’s image of the cafe au lait, as to me it looks more like chaos than complexity.\nIn any case, I think Sean is right when he points out that coarse graining is important. I think the issue of change is to be found more in spatial heterogeneity. What you want is local deviations from a straight evolution towards increasing entropy, something that maintains a low entropy on the expense of entropy increase elsewhere. Again the problem is that there are systems that might have that without actually being complex, so that won’t do either, but I think that’s where one would start from.\nComment #68 September 26th, 2011 at 1:22 am\nNice work. But please don’t corrupt it with cosmological theories that might be wrong.\nMy favorite cosmological model, not requiring any new physics and resulting in huge conceptual simplifications, is one where the universe is infinite and matter is fractally distributed with all momenta equally represented at large enough scales. An eternal smash up because there is always a bigger and faster structure bearing down on us. Perhaps our pocket of the universe is the result of such a collision. I do believe that entropy is hard to deal with when the system is infinite. But it seems to me that if you take our local universe out to about 15 billion light years in radius and collide it with another similar or bigger pocket at 0.9999999c, the result might look like what we call the big bang. The situation would not change much even if both pockets had previously entered heat-death.\nComment #69 September 26th, 2011 at 3:17 am\nDear Scott:\nYou said: “First, some background: we all know the Second Law, which says that the entropy of any closed system tends to increase with time until it reaches a maximum value.”\nTwo points:\n(i) The Second Law applies to the _thermodynamic_ _isolated_ systems, not to _all_ _closed_ systems.\nInasmuch as an information-theoretical view of entropy might perhaps differ in some manner from the real universe (i.e., from the inductive context and scope of the observations on which the Second Law was originally based), one might expect the law not to hold in an exactly similar manner in all the respects.\n(ii) Off-hand, I take it that the Second Law says only that the entropy of an isolated system either remains constant or infinitesimally increases with an infinitesimal passage of time. _If_ an isolated system initially has such a level (or quantity) of entropy that it can increase with the passage of time, _then_ it will increase. The entropy’s actually reaching a maximum value, starting from a lower value, is not a part of the statement of the Second Law itself. The Second Law is equally applicable to the case wherein you have an asymptotically increasing entropy that never in fact reaches “the” (limiting) maximum value.\nI have yet to read the rest of your (to me somewhat) interesting post as also all the previous comments, and so, might come back later on once again if I have any ((at least to me) interesting) comments to make. What, however, caught the eye was your inclusion of a graph which seems remarkably similar to the one which is widely studied in the chaos and nonlinear systems theory.\nBest,\n–Ajit\n[E&OE]\nComment #70 September 26th, 2011 at 3:58 am\nI would just like to comment that mkatkov (#55) and Kaleberg (#52) are pointining in directions that we know to be related.\nWhen discussing boolean functions on the Hamming cube, the sensitivity of a function g is defined to be $\\max_{x\\in\\left\\{ 0,1\\right\\} ^{n}}|\\{y\\sim x:g(x)\\neq g(y)\\}|$\nWhen the sensitivity of a function is low, we can anticipate the value of g on a point give its neighbours’ values. This is basically what mkatkov (#55) suggested.\nOn the other hand Kaleberg (#52) suggested that the Fourier spectrum of a function tells us something about its complexity (=interestingness). Going back to boolean functions, one can define the degree of a boolean function as a real polynomial, using its Fourier coefficients.\nWe know that those two notions are related (at least for boolean functions), more precisely, that $deg(g) \\leq sensitivity(f)$ [Nisan & Szegedy, 94].\nComment #71 September 26th, 2011 at 5:43 am\nEntropy does not increases monotonically. Even if be entropy you really mean “average entropy”, it does not increase monotonically outside the Markowian approximation.\nIt is not true that the Second Law says that the entropy of any closed system tends to increase with time until it reaches a maximum value. You seem to confound closed with isolated systems. Indeed, the Wikipedia article that you link uses the correct term “isolated”.\nThe proof that the middle picture is the more complex structure is easy, when one notices that left and rigth pictures correspond to equilibrium states. Macroscopically, complexity can be measured as deviation from equilibrium. And in very far from equilibrium regimes, appear dissipative structures, which are highly complex structures. Even a Nobel Prize was given for that. A microscopic characterization of dissipative structures is also possible.\nI would add that the characterization low complexity –> high complexity –> low complexity of your example is an special case, valid when the external flows are small, there is not bifurcation points, neither memory effects. Really complex system are much more complex.\nComment #72 September 26th, 2011 at 6:57 am\nWalter J Freeman III had a really good idea about complexity in his book Neurodynamics; An Exploration of Mesoscopic Brain Dynamics. [W. J. Freeman (2000)London UK: Springer-Verlag]. He observed that “self organizing” phenomena seemed to contradict the Second Law of Thermodynamics by taking on more order as time progresses. What he observed was that these phenomena were not destroying entropy, but rather they were able to organize because they were a locally cheaper way to create it. He used as an example a hurricane. This is an exaggeration of the convection rolls that are typically used as examples of self organizing phenomena. Hurricanes brew up whenever there is a sufficient amount of heat in ocean water. Without that heat and its attendant potential difference, there is no hurricane. As far as I have been able to observe, all organized systems function as accelerated entropy creation exploits.\nComment #73 September 26th, 2011 at 8:50 am\nTerry Bollinger #65:\nHave we met?\nIf we have, I don’t remember.\nDo you know Russ T?\nSure.\nThe trouble is that on first whack, I just don’t get where you are heading in trying to link that particular idea with the emergence of “interesting” complexity in our universe.\nI should have said this explicitly in this post, but the idea of linking Kolmogorov complexity with thermodynamics isn’t new at all. I didn’t invent it. In fact, in the standard textbook An Introduction to Kolmogorov Complexity and Its Applications, by Li and Vitanyi, there’s an entire chapter about this connection.\nPersonally, I’ve long thought that Kolmogorov complexity provides probably the most “objective,” mathematically clearest way to understand the notion of entropy—since basically, what it lets you do is talk about the “entropy” of an individual object, without reference to any hypothesized ensemble from which the object was drawn, or any hypothesized coarse-grained structure on the object. (Though actually, what you really want here is resource-bounded Kolmogorov complexity—see the several paragraphs in the post about this.) So it was completely natural for me to try to answer Sean’s question using some variation on Kolmogorov complexity as well.\nMy blog question to you would be this: If you had to give a two-line pitch of your most critical insight or postulate to an expert — not just to some random person on an elevator! — what would it be?\nThinking this question over, I quickly realized two things:\n(1) I don’t have a “most critical insight or postulate.”\n(2) It’s just as well that I don’t, since the people I can think of who do have a “most critical insight or postulate,” are the ones who I’d consider spouters and shnoods!\nI mean, obviously I think things like computational complexity and quantum computing are pretty important, since otherwise I wouldn’t spend my career studying them. But I wouldn’t call the importance of these topics my “most critical insight or postulate”—since among other things, I didn’t invent them and am far from the only person who studies them!\nIf you’re willing to read a few hundreds or thousands of lines instead of two, you can try:\nNP-complete Problems and Physical Reality\nWhy Philosophers Should Care About Computational Complexity\nMy research statement (from about 5 years ago)\nComment #74 September 26th, 2011 at 9:05 am\nPeter #68:\nNice work. But please don’t corrupt it with cosmological theories that might be wrong.\nAs opposed to cosmological theories that are almost certainly wrong? 😉\nSeriously, FWIW nothing I wrote depends on specific details of the Big Bang model—just that the universe starts in a low-entropy state and then heads toward thermal equilibrium.\nComment #75 September 26th, 2011 at 9:12 am\nJuan #71:\nThe proof that the middle picture is the more complex structure is easy, when one notices that left and rigth pictures correspond to equilibrium states.\n[Game show buzzer] Sorry, completely wrong, try again! The left picture does not correspond to an equilibrium state, and can’t possibly do so, since it evolves into the second picture.\nAs for the distinction between a “closed” and an “isolated” system: I don’t know what that distinction is; would anyone care to explain it? In any case, whatever the distinction is, I really don’t see how anything else in the post could have depended on it—just replace the word “closed” by the word “isolated” the one incidental place where it appears.\nComment #76 September 26th, 2011 at 10:48 am\n> Why aren’t theoretical computer science conferences\n> ever held on cruises? If nothing else, it certainly\n> cuts down on attendees sneaking away from the\n> conference venue.\nI memorably attended a theoretical computer science conference (well sort of: CADE, one year) which was advertised as in a hotel on the beach at the Barrier Reef.\nWhen you got there, you realised that this was all true, but that actually to get to the reef required an organised tour in a large boat (the conference outing of course) and the hotel was on the beach in a mining town where there was _nothing_ to do but attend the conference (or play video poker in the next room).\nIt is still my definition of perfect conference organisation.\nComment #77 September 26th, 2011 at 1:39 pm\nSorry, but the left picture represents an *unstable* equilibrium and any initial fluctuation will almost surely trigger the departure of the system from this initial state towards the final stable equilibrium state (the right picture). This is the reason which it evolves 🙂\nThe condition of equilibrium is dS=0 and the condition of stability is d^2S<0. This is similar to mechanics where both stable and unstable equilibrium states exist.\nIndeed, the process that you are considering is just a mixing process. Virtually any textbook on *equilibrium* thermodynamics explain how to obtain the mixing functions (e.g. entropy of mixing, enthalpy of mixing…) from the initial and final equilibrium states.\nThe intermediate states of the mixing process can be studied using the tools of *nonequilibrium* thermodynamics.\nRegarding your doubt, an isolated system is one in which neither energy nor mass can flow in or out. In a closed system, mass cannot flow in or out but energy can be added or removed. Contrary to what is said in this blog, the Second Law does not say that the entropy of any *closed* system tends to increase with time; the law says that (average) entropy can increase, decrease, or remain constant in function of the heat flow with the surrounds. As said above, the Wikipedia link correctly uses the term \"isolated\" in its discussion of the law.\nAs said also above, complexity can be characterized as departure from equilibrium. And in very farm from equilibrium regimes, very complex structures named dissipative arise. Microscopically this is also true. At equilibrium regimes, the dynamics is trivial and the physicochemical properties of the systems are well-described by statistical methods. Outside equilibrium that is not true. The more you depart from equilibrium, the more non-trivial is the dynamics and statistical methods lose importance.\nThis is the reason that works by Kolmogorov, Sinai, and others have very little impact on the physics and chemistry of nonequilibrium (The KS entropy is almost not used). Whereas works by Shanon are known to give incorrect answers to well-studied problems.\nA quantitative measure of complexity is given by the degree of contraction needed to describe the system. Roughly the right picture needs a description that is about a half of the left picture, and this itself is about a half of the needed for the intermediate states.\nComment #78 September 26th, 2011 at 2:22 pm\nJuan #77: No, the system on the left doesn’t represent an equilibrium at all, not even an unstable one. It’s going to start to mix even in “ideal” conditions, because of the random molecular motion of both the milk particles and the coffee particles—so it’s not at all analogous to (say) a needle balanced on its tip. This is a simple point about which you’re completely, totally, flat-out wrong.\nAs for the distinction you’ve drawn between “closed” and “isolated”—that the first allows transfer of energy but not mass, while the second doesn’t allow transfer of either—that seems to me like a somewhat silly distinction, one that doesn’t even make sense in a relativistic universe. It seems obvious that the sense of “closed” that’s relevant for the Second Law is that nothing should go in or out.\nComment #79 September 26th, 2011 at 4:09 pm\nScott, Sean:\nI am just a layman, so much of this is above my head, however it would seem that Cosma Shalizi would be the person to ask on issues of complexity:\nhttp://cscs.umich.edu/~crshalizi/notebooks/complexity-measures.html\nComment #80 September 26th, 2011 at 5:11 pm\nScott #73:\nAh, that helps. If your piece felt a bit open-ended to me, from your answer I gather it’s because you intended it more as a mini-intro to a topic that already has a large and fairly mature literature. I’ll read it as such.\n“… you can try:\n– NP-complete Problems and Physical Reality\n– Why Philosophers Should Care About Computational Complexity\n– My research statement (from about 5 years ago)”\nWell, you may not be a shnood, but you just did an awfully good job of answering my question in a short, three-line response that is fully understandable to experts. Hmm!\n(A serious question on that: Wouldn’t Maxwell and many other famous physicists qualify as big-time shnoods under that definition? A set of only four equations is pretty doggone presumptuous… what a shnood! And then there was that guy who stirred up such a ruckus with an equation that had only two variables and one constant…)\nBTW, to describe myself I had to extend your list of ad hominem phrases a bit. I am clearly an extralusionary faux-intelligence. I an expert in nothing, and I apply nothing to everything, always!\nAs an EFI (effies always create acronyms) the question of whether or not for a given random string there exists an efficient program-like compression has always struck me — and still strikes me — as a question that is likely a lot close to the leaf nodes of physical reality that to its core. I realize you are using such ideas very differently, however, so I’ll see of any of your materials can help me see it differently.\nCheers,\nTerry Bollinger\nP.S. — Wolfram? Seriously? Wow.\nOK: Science is all about prediction. So a serious question: What exactly did Wolfram’s massive tome (or the massive international follow-up) predict about the physical world?\nI’m not aware of a single physically meaningful prediction anywhere in his book, and I went through every blasted page of it when it first came out. To the best of my knowledge, follow-up work has not changed this status.\nSome terminology here:\nWhen you specify how to predict something, AND refine that prediction by using physical evidence, it’s called science.\nWhen you instead postulate that you have found a principle that is the key to understanding the entire universe, but you then realize you are missing the critical piece needed to make it work, so you train thousands of people about your insight and encourage them to have faith that the missing piece is out there waiting for them to help find, it’s called…\n[Please, no shouts of “string theory” from the peanut gallery, I’m trying to make a real point here!… 🙂 ]\nComment #81 September 26th, 2011 at 5:50 pm\nTerry Bollinger #80:\nActually, I was trying to address Sean’s new question about finding a natural complexity measure that increases and then decreases in examples like the coffee cup. As I said in the third paragraph:\nMy purpose, in this post, is to sketch a possible answer to Sean’s question, drawing on concepts from Kolmogorov complexity.\nBut in order to do that, I first had to discuss the already-known connection between Kolmogorov complexity and entropy. I’m sorry if that wasn’t clearer.\nI completely disagree that Einstein and Maxwell were shnoods by my definition! As it happens, journalists did constantly ask Einstein to do things like “summarize your main idea in one sentence,” and he gave them humorous non-answers in response. That’s because he realized that translating a nontrivial idea in math or physics into “slogan” form could only lead to egregious misconceptions in someone who didn’t already know the idea. E.g., suppose he told the journalists “my main ideas are that mass is energy, and that space and time are part of the same structure,” like a salesman or politician repeating his catchphrases. People would then get an illusion of understanding that was much worse than no understanding at all. If you’re comfortable with abstract concepts, you can probably go from never having heard of special relativity to genuinely understanding something about it in only an hour, but most people aren’t interested enough to spend even that hour.\nAs for Wolfram, nine years ago I actually wrote the first critical academic-style review of his book, one month after it came out. In the review, I mostly focused on some incorrect claims of Wolfram’s about quantum mechanics (in the process, proving a simple corollary of Bell’s Theorem that Conway and Kochen would later call the “Free Will Theorem” 🙂 ), but there’s also some stuff in there about complexity and pseudorandomness.\nComment #82 September 26th, 2011 at 5:51 pm\nI don’t think it is hard to answer Sean’s question about the three coffee cups using the notion of coarse-graining. I don’t think it is possible to define entropy without referring to the notion of coarse-graining. (I know you attempted that here, but I am not convinced about your definition. I even checked Li-Vitanyi, but they don’t seem to get rid of coarse-graining either.)\nPersonally, I’ve long thought that Kolmogorov complexity provides probably the most “objective,” mathematically clearest way to understand the notion of entropy—since basically, what it lets you do is talk about the “entropy” of an individual object, without reference to any hypothesized ensemble from which the object was drawn, or any hypothesized coarse-grained structure on the object.\nThe coarse-grained structure is not hypothesized. A crucial property of any kind of observer is that she can’t distinguish between all microstates of her surroundings. I am not very familiar with statistical mechanics, but I have a quite strong intuition that statistical mechanics is nothing but understanding all the logical implications of this single fact. Of which there are many.\nComment #83 September 26th, 2011 at 5:55 pm\nAlex #79:\nit would seem that Cosma Shalizi would be the person to ask on issues of complexity\nI agree that Cosma is a great person to ask! But what would make anyone think that there could be one person to ask, on any subject as complex as complexity? 🙂\nComment #84 September 26th, 2011 at 6:24 pm\nHi there!\nI think I came up with an easy way to measure what is intended to measure. I was trying to read all the comments to check out if someone had had the same idea, so I wouldn’t repeat it; but there are really a lot of comments and tomorrow I must wake up early… so I just hope I’m not repeating anything. If so: sorry for the bother 😉\nInteresting complexity shows up when dynamics of different scales are mixed up together, so let’s take a look at the scales of the milk-coffee by Fourier transforming the milk density $M(h)$ and the coffee density $C(h)$, where $h$ is the height. We should obtain in either case the Fourier coefficients $B_w$ of the step function, where $B_w$ is the coefficient corresponding to the frequency $w$. Let’s normalize the coefficients such that $sum(B_w)=1 ==> b_w = B_w/sum(B_w)$ and reinterpret them as some weird probabilities (just imagine that we have a device which is capable of measuring one and only one spatial-frequency $w$ of the density distributions each time that it measures, and that the probability of measuring each frequency is given by $P(w)=b_w$). Then, I propose as a measure of the complexity of the system the entropy of $P(w)$.\nMy guess is that this quantity will obey the desired behavior. At the beginning we would have the series expansion of the step function, which has very concrete values ($P(w)$ would be a sum of Dirac’s deltas) and a relatively low entropy (low compared with what’s coming). As the milk diffuses down, the sharp distributions $P(w)$ should degenerate yielding a $P(w)$ where more $b_w$ are different from zero, so the entropy of $P(w)$ should be larger. Eventually, milk and coffee are equally distributed and the density is flat all over the cup. The Fourier transform of this thing would only have one component and the entropy would be minimal. This state will also have a lower complexity than the initial one, by the way.\nThis should be easy to generalize to 3D. A final remark would be that any single system should have fluctuations, thus complexity could increase and decrease several times before reaching the steady state. The law should be an averaged one, as for the usual entropy.\n**Sorry for using so many times the word ‘measure’.\nComment #85 September 26th, 2011 at 7:36 pm\nScott #81:\nOK, I’ve finally got the emphasis on the theme picture and the mixed-complexity domain down! It’s obvious, I know, but your emphasis helps anyway. I’ll keep an eye out for your initial framework as I read through the theory.\nAnd yes, I was playing devil’s advocate by calling Maxwell a shnood. I knew very well that was not what you intended, but I was curious as to your response (which was quite persuasive!) If you look back at my original email, I tried to be very careful not to ask for super-simplification, but instead for some guidance on “here’s what I’m really hoping you will understand after you read through all of this.”\nInterestingly, I also did a very early review of ANKoS for a software magazine. I’m wondering now if I read yours back then; your description sounds familiar. I’ll look it if I can.\nBlog limit reached for the day (week?), real life calls.\nCheers,\nTerry\nComment #86 September 26th, 2011 at 9:42 pm\nI think it might have something to do with our universe’s rate of expansion. It’s fast(strong?) enough to pull distance galaxies away from each other, yet it’s not quite strong enough to pull particles that makes up milk and coffee apart from each other, at least not for another quintillion years. Imagine a universe with much higher rate of expansion, or one with much weaker 4 forces of nature. It wouldn’t necessarily evolve “complex” structures we observe in our universe. Having said that, the idea of “complexity” might very well be in the domain of physics, modeled as a function of other cosmic parameters, and universal peak complexity predicted.\nComment #87 September 27th, 2011 at 12:09 am\nAs usual, my DAH (Devil’s Advocate Hat) is on. This is convenient, because it allows you to comment on anything without doing the work to really understanding it. Thus I will proceed to disparage the notion of using Kolmogorov Complexity (KC) for anything but entertainment.\nMath is a subject where a couple of interesting definitions and a few theorems can launch a subfield such as KC. I have never studied KC are tried any calculations that might give me some insight, but a brief reading of the subject suggests that it started as a joke, and today a lot of people are not in on it.\nMy intuition starts with the question: “How could you ever actually compute anything?”.\nFurthermore, the KC of things would change as knowledge in other fields progresses. For example, what is the KC of\nδ = 4.66920160910299067185320382…, and\nα = 2.502907875095892822283902873218… ?\nThese are Feigenbaum’s constants (http://en.wikipedia.org/wiki/Feigenbaum_constants). A couple of decades ago, no one knew anything about these numbers. With the concept of analyzing discrete dynamical systems by bifurcation diagrams in hand, these can be calculated with a short program. So, did KC(δ) and KC(α) drop dramatically 20 odd years ago?\nFinally, using KC reminds me of physics arguments that use the wave function for the universe. Sure, there must be such a thing, but it is hard to say much about it.\nOn the other side of the coin, the theorems and proofs in basic KC are rather similar to those in many fields of TCS, and many SO readers might not think of these as a joke.\nBTW, V.I. Arnol’d, who made major contributions to many areas of math (and died just last year) was a student of Kolmogorov. Does anyone know if he discussed KC?\nComment #88 September 27th, 2011 at 12:40 am\n(continuation) I failed to include a key point:\nGiven that there is a recently discovered short program for δ and α, now take any arbitrary string S, and calculate KC(S) and suppose this is a large number. Who is to say that tomorrow a new theory, perhaps arising from algorithms to gamble on FarmVille, won’t provide a short program for S, in which case KC(S) is now a small number? How could you know if this might happen?\nMy intuition is that the entire concept of KC is “ill-posed”, to borrow a term from PDE.\nIn the interest of “full disclosure”, I must mention that often in the past I have thought some topic was a bunch of hooey until I understood it, after which I thought is was profound, just like listening to Lenard Cohen.\nComment #89 September 27th, 2011 at 4:05 am\nRaoul: This is indeed one of those cases where if you understood more, you’d see why your dismissal was wrong. And unlike with (say) art, music, or religion, the reasons why your dismissal is wrong can be articulated in words.\nContrary to what you say, K(x) is not undefinable: I’ll define it right now, as the length of the shortest prefix-free program (in some fixed universal programming language) that prints x and then halts! K(x) is uncomputable, but that’s a completely different issue, and something that’s been understood since the 1960s.\nBasically, what K(x) lets you do is give a clear, observer-independent meaning to the loose notion of there “not existing any patterns” in a string. Already from that statement, it’s completely obvious that K(x) is going to be hard to compute—for as you correctly point out, detecting the existence or nonexistence of patterns is hard!\n(Though contrary to what you say, K(Feigenbaum’s constant) didn’t suddenly become small when Feigenbaum defined the constant, any more than 42038542390523059230 suddenly became composite when I wrote it down, probably for the first time in human history! Please don’t tell me that you’re unable to distinguish between mathematical truths and our knowledge of them.)\nThe key point is that, even without being able to compute K(x) for most x’s, you can still use the definition of K(x) to give meaning to hundreds of intuitions that otherwise would’ve remained forever at a handwaving level. For example:\n“The overwhelming majority of strings are patternless.”\n“If a short computer program outputs a patternless string, then it can only be doing so by generating the string randomly.”\nAnd many, many less obvious statements—every one of which can be upgraded to a theorem once you have a mathematical definition of “patternless.”\nFurthermore, the idea of Kolmogorov complexity has actually inspired some important experimental work! For example, if you could compute K, then you could compute the “similarity” between two DNA sequences D1 and D2 by comparing\nK(D1)+K(D2) to K(D1,D2).\nOf course you can’t compute K, but you can compute useful upper bounds on it. For example, let G(x) be the number of bits in the gzip compression of the string x. Then comparing\nG(D1)+G(D2) to G(D1,D2)\nturns out to be a very useful way to measure similarity between DNA sequences.\nIt’s really no different from how, even though we can never tell whether a curve in the physical world is continuous or not (since that would require infinitely precise measurements), the mathematical theories dealing with continuity (e.g., calculus, topology) can still be used in physics in all sorts of ways.\nComment #90 September 27th, 2011 at 4:32 am\nDear Scott:\nI have now gone through your post. Here are my (obviously) layman’s comments. (I am acutely aware of my ignorance of these topics, but, what the heck—this is just a blog-comment.)\n1. First, look at the broad “structure” of the problem. With the progress of time, the entropy increases monotonically. However, in order to capture the “interesting” thing, you need to construct some other function that first increases, slows down its growth, goes through a hump, and then decreases.\nFlip the graph vertically so that the hump becomes a “U”, and now it’s easier to see that this can be taken as a typical engg. optimization problem: You have to imagine _two_ factors: one that increases monotonically; another that decreases monotonically; then add the two together, and the combined function gives you the required “U” shape. For instance, the total potential energy of a mass-spring system acted upon by an applied force, or the total PE of a diatomic molecule (with attractive and repulsive forces).\n(Another way to look at the hump is as the function that describes the slope of the curve in the logistic growth model, or of the S-growth curve of biology. However, the engg. optimization version brings out the necessity of having to have two oppositely acting factors more explicitly/easily.)\n2. As the spring-mass model shows, you can have one of the factors increasing/decreasing linearly so long as the other factor compensates for it by decreasing/increasing more strongly than linearly, so as to ultimately produce a hump (and not a straight-line).\nThus, a linear increase of computational entropy could also be OK.\nHowever, it seems obvious that for the physical diffusion process, the entropy should increase logarithmically. (The decreasing factor in diffusion is: the driving force (or the conc. gradient).)\n3. Continuing with the hand-waving, methinks, in your idea, the algorithm that reconstructs x given the sampling oracle seems to play the part of measuring the “driving force” of the diffusion model.\n4. As I said, I am quite ignorant of the actual CS theory with which this blog post of yours is mainly concerned. However, since I have already said so much, let me venture some brain-storming, with a goal of getting something concrete out of this reply.\nConsider a game. Imagine a computer screen having its upper half filled with a meaningful text, and an empty bottom half. Recall those viruses that would tear away the on-screen characters/pixel-colors and drop them at random. Instead of characters, suppose the program drops down blocks of them (strings?) after chopping the text at random.\nMain question: Does this game adequately bring the three coffee-cups problem into the realm of the theoretical computer science?\nSecondary question: How does one deal with those words which, when chopped, produce word-fragments that still are meaningful by themselves? Or does one simply wish the problem away by simply redefining the problem to refer only to the original set of words i.e. by restricting the reference dictionary?\n5. Finally, a couple of asides (about which I happen to be much more confident): (i) The objection you raised concerning the relativistic interconversion of mass and energy is irrelevant to the definitions of the closed and isolated thermodynamic systems. Closed systems are closed to the flow/exchange of matter, not of mass/energy. (ii) In the three coffee-cups problem, inasmuch as you don’t care for any other interaction of the glass-contents with the rest of the universe (and in fact exclusively focus only on the spatial rearrangements of the two constituents (including formation of tendrils, droplets, etc.)), it does qualify to be abstractly described as an isolated system—there is no exchange of anything with the surroundings to be considered here. Think about it this way: The glass-contents effectively make for the entirety of your (abstract) universe. Now, recall that the universe as a whole always forms an isolated system.\nBest,\n–Ajit.\n[E&OE]\nComment #91 September 27th, 2011 at 10:20 am\nBy the left picture, I am taking an initial state at t = 0 just when the mixing starts (it seems that you also think the same when write “It’s going to start to mix”). It is rather evident that this is the case when the macroscopic flows are zero and a trivial computation gives dS = 0, which is the condition that DEFINES thermodynamic equilibrium. Indeed, any textbook writting dS >= 0 that I know emphasizes that the equality holds for equilibrium states. Evidently for 0 < t = t_rel the system is again in an equilibrium state (t_rel being the characteristic time needed to achieve equilibrium).\nIt seems evident to me that you are confounding equilibrium with homogeneous (an homogeneous system is an equilibrium system but the inverse is not needly true). All the processes studied in equilibrium thermodynamics textbooks belong to the kind “Initial equilibrium state” –> “Final equilibrium state” and those textbooks contains chapters devoted to the study of mixing proccesses. Those chapters explain how to obtain enthalpy of mixing, the entropy of mixing, etc. as the difference between final and initial equilibrium states. There is very little more than I can say to, except maybe to draw an entropy vs time plot for your milk-coffee system :-). Note: your diagram for the evolution of the entropy of Universe is wrong as well.\nThe same about the distinction between “closed” and “isolated” systems. This distinction is standard and appears in any textbook of thermodynamics or chemical thermodynamics that I know (a list can be given under request). Several online resources discuss this also (including the Wikipedia). The fact, this is the FIRST time that you hear about the existence of this distinction and of the correct form of the second law says a lot of… Your claim of that the Second Law says that “the entropy of any closed system tends to increase with time” is just plain wrong. Fortunately, the same Wikipedia article that you link does not make your notorious mistake, altough some readers could get the false impression that Wikipedia is suporting you. No it is not.\nOf course, the laws of thermodynamics also work in a relativistic context. Well-informed people as Misner, Thorne, and Wheeler correctly note that the relativistic law (dS/dtau)>=0 is only valid when there is not heat flow in a system at constant composition. I.e. when the system is ISOLATED. In a closed system (dS/dtau) can be positive, negative, or zero (J_S = J_Q/T).\nYour last remark is also vacuous. First because when studying the thermodynamics of the Universe we study it locally (by technical reasons that I will omit here) and saying that local entropy increases for closed systems is plain wrong. Second, because nobody has proved that Universe as a whole is an isolated system. In fact there are cosmological models where the Universe is considered an open thermodynamic system and matter born obtained from an initial unstability in a quantum spacetime without singularity involving an intermediate de Sitter regime.\nRegarding the definition of complexity, your informational theory approach, even if finally satisfactory cannot provide a measure of the complexity of the dinamical laws nor of the involved time scales in the different dinamical regimes. For example, your approach to discretize the “cofee cup” to pick an adjacent coffee pixel and milk pixel uniformly at random swaping both only works if we want a description of the process for time scales much larger than a tipical memory scale t_mem and if we work with not too large gradients. Better tools are already at your hands.\nRegards.\nComment #92 September 27th, 2011 at 11:19 am\nThere is some cool work on this kind of thing where you try to evaluate something similar to the complextropy of a string x by looking for the smallest probabilistic finite state machine that can generate x.\nI suppose this boils down to roughly the same thing as the paragraph beginning “As a first step, let’s use Kolmogorov complexity to define entropy…”?\nComment #93 September 27th, 2011 at 11:30 am\nRegarding Bennett’s logical depth versus (Moshe Koppel’s original notion of) sophistication: under certain formalizations of the notions, these are actually equivalent. Of course, Koppel’s notion of sophistication looks much different than Antunes and Fortnow. It’s been a while since I’ve read the Antunes/Fortnow paper, so I forget the nuances of the connection to Koppel’s work.\nIn particular, studying infinite sequences instead of finite strings, one can qualitatively define some sequences as deep or sophisticated, and the rest as not. There are two ways that Bennett defined depth and two ways that Koppel defined sophistication for infinite sequences. One of these ways results in depth being equivalent to sophistication. The key difference is whether we allow short programs for prefixes of an infinite sequence S to be any string, or whether we require that they themselves all are prefixes of a single infinite sequence X that is a “compressed representation” of S (despite being infinite itself).\nThe first notion, strong depth, is this: an infinite sequence S is strongly deep if, for all constants c, all computable time bounds t:N –> N, and all but finitely many n, every program that prints S[1..n] in time t(n) is at least c bits larger than K(S[1..n]). Surprisingly, this is shown by Bennett (and later more formally by Juedes, Lathrop and Lutz, using Levin’s Coding Theorem) to be equivalent to requiring that all t(n)-fast programs for S[1..n] are themselves compressible by at least c bits (i.e., K(p) {0,1}*, and all but finitely many n, if U(p,x) = S[1..n], then |x| > K(S[1..n]) + c.\nIt turns out that the strongly deep sequences coincide with the strongly sophisticated sequences.\nThe connection between these notions is that each defines the complexity of S in terms of the Kolmogorov complexity of prefixes of S. When considering programs that define the Kolmogorov complexity of various different prefixes of S, these programs need not have any relation to one another, even though their outputs are all prefixes of the same infinite sequence. We could instead require that the short programs we consider are prefixes of one another; i.e., we demand that there be a single infinite sequence, the prefixes of which can be used to compute prefixes of S. This leads to “weak depth” and “weak sophistication”.\nA sequence is weakly deep if for all constants c, all computable time bounds t:N –> N, and all infinite sequences X, for all but finitely many n, if U^t(X[1..m]) = S[1..n], then m > K(S[1..n]) + c. This turns out to be equivalent to stating that S is not truth-table reducible to any Martin-Loef random sequence.\nA sequence is weakly sophisticated if for all constants c, all total programs p, and all infinite sequences X, for all but finitely many n, if U(p,X[1..m]) = S[1..n], then m > K(S[1..n]). Again, this looks like strong sophistication as defined above, but requiring that all the candidate fast, short programs for prefixes of S themselves be prefixes of a single infinite sequence X.\nPhilippe Moser and I tried unsuccessfully for a while (specifically, for the length of one bus ride from Siena to Rome) to prove that weak depth is equivalent to weak sophistication, without success.\nI’m not sure if any of this translates to meaningful statements about the depth or sophistication of finite strings. I know the infinite sequences versions of depth and sophistication inspired Antunes and Fortnow’s work on these concepts for finite strings, but they may have made enough changes that the concepts no longer coincide significantly.\nComment #94 September 27th, 2011 at 11:33 am\nFor some reason the following paragraph was chopped from my comment. Mentally insert it before “It turns out that…” in my post above:\nWhat I will call strong sophistication is this (letting U denote a universal TM taking two input strings, p a program and an input for p): an infinite sequence S is strongly sophisticated if, for all constants c, all programs p representing a total computable function p:{0,1}* –> {0,1}*, and all but finitely many n, if U(p,x) = S[1..n], then |x| > K(S[1..n]) + c.\nComment #95 September 27th, 2011 at 11:41 am\nJuan: The philosopher David Chalmers recently wrote a paper proposing that, when two people are locked in a dispute that they suspect is purely over words, what they should do is ban the problematic words from the conversation and then see whether any substantive disagreement remains. With your kind indulgence, I’d like to use this dispute as a test case for his method.\nWith “closed” vs. “isolated,” we don’t even have to use his method. It’s obvious that I was using the word “closed” to mean exactly the same thing you mean by “isolated,” and that indeed, the weaker property that you mean by “closed” is something that’s completely irrelevant to this discussion and that I couldn’t possibly have meant. So I’m happy to follow you and the Wikipedia entry in using “isolated.”\nRegarding “equilibrium,” I have to say you’re using that word in a way that I don’t recognize from any discussion of probability or ergodic theory I’ve ever seen, but that you claim is standard. Fine. So then let’s use the term “max-entropy” for the type of equilibrium that you don’t get out of once you’re in it: i.e., the type of equilibrium that coffee is in after it’s stirred but not before, and that the universe will be in after it degenerates into radiation, but was not in at the moment of the big bang. Then anywhere I use the word “equilibrium” in the post, you can substitute “max-entropy.”\nNow what are the actual, substantive disagreements that remain?\nComment #96 September 27th, 2011 at 2:35 pm\nPete:\n“I suppose this boils down to roughly the same thing as the paragraph beginning “As a first step, let’s use Kolmogorov complexity to define entropy…”?”\nNot really, since the work you’re referring to (Crutchfield’s epsilon-machines) is only defined for ensembles whereas Scott’s post here is about individual objects. However, Gell-Mann’s “effective complexity”, being the Kolmogorov complexity of the distribution an ensemble is drawn from, has apparently been shown to be essentially equivalent to Crutchfield’s statistical complexity measure (the entropy of the state-occupation probabilities of the epsilon-machine) by Karoline Wiesner in a talk at ECCS’11.\nKaroline has also recently co-authored a paper on measures of complexity, discussing their pros and cons: http://www.maths.bris.ac.uk/~enxkw/Publications_files/Ladyman_Complex_2011.pdf . However, like Grassberger ( http://www.springerlink.com/content/l25007637531552j/ ) and many others before, the paper takes the stance that measures of complexity must be for ensembles, not individual objects, and so does not directly address the questions raised in this blog post.\nComment #97 September 27th, 2011 at 3:43 pm\nA genuine confusion: how does this “complexity is highest in the middle” argument work with cyclic systems?\n(Eg. a clock reaction like http://en.wikipedia.org/wiki/Belousov%E2%80%93Zhabotinsky_reaction)\nComment #98 September 28th, 2011 at 1:23 am\n“And I don’t understand how one could have tachyonic neutrinos without getting CTCs as well—would anyone who accepts that possibility be kind enough to explain it to me?”\nThe straightforward and simple solution is a preferred frame. It is hidden as long as Lorentz symmetry is exact, but becomes observable if there are small violations of Lorentz symmetry.\nThere are independent arguments in favour of a preferred frame anyway, see my homepage.\nComment #99 September 28th, 2011 at 6:53 am\nPeter van Emde Boas drew my attention to this blog. On monday October 2nd Amos Golan and I organize a one day conference on Philosophy of Information at the the Info-Metrics Institute in Washington DC exactly on this issue.\nSee: http://www.american.edu/cas/economics/info-metrics/workshop/program-2011-october.cfm\nOn background information regarding Kolmogorov complexity and processes.\nSee:\nP.W. Adriaans , Between Order and Chaos: The Quest for Meaningful Information, Theory of Computing Systems, Volume 45 , Issue 4 (July 2009), Special Issue: Computation and Logic in the Real World; Guest Editors: S. Barry Cooper, Elvira Mayordomo and Andrea Sorbi Pages 650-674, 2009\nand\nP.Adriaans and P. Van Emdo Boas, Computation, Information, and the Arrow of Time, In COMPUTABILITY IN CONTEXT Computation and Logic in the Real World, edited by S Barry Cooper (University of Leeds, UK) & Andrea Sorbi (Universita degli Studi di Siena, Italy) (pp 1-17),\nhttp://ebooks.worldscinet.com/ISBN/9781848162778/9781848162778_0001.html\nAs I am travelling I have little time to react on the contents of this blog now, but hope to find time to do this later,\nCheers Pieter Adriaans\nComment #100 September 28th, 2011 at 7:35 am\nA little correction to what I said in my comment #90 above. (And isn’t there always at least one?)\nThe example of the spring-mass system doesn’t fit what I was trying to point out. The strain energy of the system by itself produces a bowl/well (a “U”)—i.e., even without considering the linear change to the PE of the system as effected by the applied force.\nInstead, what we really needed was two different factors such that each by itself produces only a monotonically increasing/decreasing effect, i.e. not a hump/well when taken alone, even though their combined effect produces a hump/well.\nIn contrast, the model of a diatomic molecule does show the required behavior. (The spring-mass system does not, even if the spring is taken to be nonlinear.)\nI stand corrected.\nComment #101 September 28th, 2011 at 11:46 pm\nScott is correct that when I understand more about KC I appreciate it more. (I was afraid that was going to happen!).\nDecades of understanding things by working through the standard examples has not prepared me for thinking about things where you can’t calculate anything! That reminds me of my brother-in-law’s definition of an engineer: A technician who can’t fix anything.\nComment #102 September 28th, 2011 at 11:49 pm\nScott #89:\nYou said:\n… K(x) lets you do is give a clear, observer-independent meaning to the loose notion of there “not existing any patterns” in a string… detecting the existence or nonexistence of patterns is hard! … you can still use the definition of K(x) to give meaning to hundreds of intuitions that otherwise would’ve remained forever at a handwaving level… [e.g.] “The overwhelming majority of strings are patternless.” … “If a short computer program outputs a patternless string, then it can only be doing so by generating the string randomly.”\nFascinating comments that I seriously am trying to resist for right now…\nBut a couple observations, what the heck, that are likely very well covered, but may be of interest to some readers anyway:\n— Proximity regions: Regions “near” extreme compressions (e.g. pi subsequences), where nearness is defined by the code brevity of a transformation, are also highly compressed, if not by quite as much. E.g. pi subsequences are highly compressed, but for very long pi subsequences, the integers on quite some distance to either side are also highly compressed because they can be accessed by simple addition or subtraction functions that can be very short in comparison to a very long pi subsequences. This effect applies until the added or subtracted numbers become so long that they also become comparable in size to the pi subsequence… unless they too have their own compressions. The result becomes very complex and recursive, and “simple” statements about the absence of K. compressions becomes a lot trickier. After going through that in my head, I’m now realizing how naive my original vision of a very sparse set of flagpoles with high compression ratios was. K. compressions beget huge ranges of “nearby” compressions via further composition with shorter transforms. The resulting landscape is not just a few flagpoles, but is likely quite craggy, with the nature of the “cragginess” rather open as more short “generalization” transforms are added or explored.\n— Why do K’s exist at all? If you start with very small integers and move slowly up in digit length, when and where does the concept of a K compression meaningfully begin? I would assume there is an inverse relationship with abstraction. The number 1 is extraordinarily useful because it can represent almost anything at all, provided only that the accessing entity is itself highly complex and provides huge context (“push the red button”). [This is the “V in time” model of context I talked about earlier in a CV blog entry; meaning is always provided by earlier-in-time duplications and separation of context.] Some folks tend to call that kind of versatility “abstraction,” though you could certainly describe it other ways.\n–“random” just invokes external complexity, so I don’t know what that really says except that your “computer” becomes very complicated indeed. Normal computers enforce “well behaved” memory concepts that are… well, a bit boring if you compare them to the more heuristic thinks that seem to go on in biology to achieve efficient computation.\nA very, very complicated computer can of course make any string it wants to (or doesn’t want to, if it allows in external sources of randomness).\nEnough, I’m just idly exploring points of curiosity without having even read your references. Invariances still strike me as more critical to fundamental physics, with “interesting” complexity emerging from V splits back in time. Think e.g. how little information would mean if every electron had different physics. It is the invariance of their properties that enables higher levels of interesting complexity — all of which I assume is below the level (?) that K. theory works at?\nCheers,\nTerry\nP.S. Pieter Adriaans #99: Interesting seminar title and talks (quantum & new info? hmm!), but the links in the agenda did not work, at least not for me.\nComment #103 September 28th, 2011 at 11:55 pm\nPieter Adriaans #99: Correction, the links in the agenda are now working! Something corrected itself somewhere, not sure on which end.\nComment #104 September 29th, 2011 at 3:26 am\nDear Scott,\nYour main idea should work.\nOver the past 20-odd minutes (a time period much shorter than what it took me to type this reply down), I just manually performed the empirical version, with a 4X4 square cup, with the upper two rows black, the bottom two rows white, and a deterministic rule. At each time step, the deterministic rule pushes one black square one cell down (provided certain conditions are met, of course.) The rule does ultimately take the system “down” to a chess-board configuration (alternate B&W) as the final state. (The rule is such that it halts the process once it reaches a chess-board state—which happens to have the maximum entropy.)\nThere are in all 32 boundaries between _adjacent_ squares. (We drop the 8 boundaries at, say the right and the bottom edges, for symmetry: to ensure a possibility of getting infinite lattice via periodic repetition.) Define a boundary between two adjacent similar squares (BB or WW) as similar (S), and those between two adjacent dissimilar squares as dissimilar (D).\nA graph of the total number of D boundary segments vs. time does empirically show an inflection. (Rapid increase, slow increase, (no increase?—this being a manual thing, I did not check all the intermediate configurations), slow increase, rapid increase.) Hence, (some normalized) “one” minus the rate of growth of number of D segments should give you a hump. (The rate of growth itself will give a bowl/well.)\nAside: The total of number of S segments follows a symmetrically opposite trend. Not surprising: D + S = 32, a constant.\nSince I am a novice in TCS, I was happy with imagining how the RLE would work as the compression method.\nIn the initial stages, moving a black square down increases the compressed size. By destroying the “contiguous-ness” pattern, it should increase the upper bound of KC in any compression method, because most of the stuff is contiguous. In the final stages, moving a black square down increases the “alternativity” pattern, and so it should once again decrease the upper bound of KC. So, the upper bound of KC must be high somewhere in between. Can there be some fluctuations in the upper bound of KC in the intermediate stages? I am afraid this could perhaps be the case. Manual checks require real^real hard work! In any case, “as everyone knows,” the entropy only increases throughout the time.\nIs the decrease in (the upper bound of) KC symmetrical around the half-time? I have no idea how to estimate this thing.\nOf what use is an upper bound if we don’t know the lower bounds? No idea.\nSo, I am happy to leave these two questions to the TCS proper guys.\nThat’s about the empirical part. If something strikes me on the theoretical side, will let you know. (But you do know better than wanting to wait for my reply, don’t you?)\nJust one aside before leaving. At least for this system, I have a logic whereby one could (I think) show that having a probabilistic evolution rule would not lead to an overall different outcome, though in the probabilistic case, there could be fluctuations in the middle (or more of them, if fluctuations are present also in a bounded-KC measure for the deterministic case). This should not be an issue. On the contrary, it might help make the things in the middle even more interesting to people like Sean!\nI think I am done with this thread (though I will continue to read the comments here for a while). I will have a look at your paper once it is ready.\nBest,\n–Ajit\n[E&OE]\nComment #105 September 29th, 2011 at 9:36 am\nThanks by this discussion. First, let me correct a mistake in my last post: “Evidently for 0 =0 does not hold globally and your basic assumption would be invalid.\nIf universe is isolated, as most experts believe, then the law holds globally. However, unless you have a new functional expression for S, you cannot compute the rate because Universe is not globally homogeneous. The standard procedure in cosmological thermodynamics is to work locally using the standard functional expression for S (See Misner, Thorne, and Wheeler). But locally, the law (dS/dtau)>=0 is only valid if you assume constant composition and not local heat flow (Misner, Thorne, and Wheeler emphasize that they are assuming *both*). Evidently composition and temperature are not constants in our Universe.\nWhat you call “max-entropy” correspond to a stable equilibrium, defined by dS=0 (equilibrium) and d^2S<0 (stability) see #78. Evidently, the latter condition implies that S is a maximum.\nRegarding your definition of complexity, it cannot differentiate situations of different complexity. Your definition only could be valid for time scales above t_mem. That is why your evolution law is so simple. When studying the fine-details of the evolution, your cell approach \"subject to random nearest-neighbor mixing dynamics\" is not valid. My question remains: why would I abandon a definition of complexity in term of the degree of contraction (and the characteristic time scales) by the your own?\nThere is much more room for our disagreement than I can write here. For instance, your attempt to study the mixing process of the “coffee cup” by discretizing it and describing the state by the number of black and white pixels (the “coffee” and “milk”) is only valid for mixing processes involving slow and small chemical gradients. Your approach is an excellent approximation for the calm mixing of the coffee and milk at your home, but it fails for studying 'violent' mixings as those what happened at very early cosmological scales.\nThe thermodynamic and hydrodynamic equations found in Misner, Thorne, and Wheeler are only valid for slow and small gradients in temperature, composition, pressure… The whole formalism would give wrong values for the entropy and complexity of the Universe at early times. A possibility to study fast processes and strong gradients is to use the laws of *extended thermodynamics* (see the celebrated monograph by Jou, Casas-Vázquez, and Lebon).\nThe main point here is that at early times complexity is larger and you need to use more complex equations (plus an extended state space) for describing processes. As a consequence, the red line in your above figure about complexity is not correct.\nRegards.\nComment #106 September 29th, 2011 at 10:45 am\nHi Juan, just a quick correction: neither of the figures is mine. As I wrote in the post, both were taken directly from a talk by the physicist and cosmologist Sean Carroll, author of From Eternity to Here. So, you might need to take up your physics disagreements with him…\nComment #107 October 1st, 2011 at 1:13 pm\nI have another definition for complexity.\nAssume state description of a physical system converted to a string. Both Kolmogorov Complexity and Shannon Entropy would give max values in the equilibrium state in the end, same as entropy. But notice the difference, even though reproducing a particular random string require max length UTM program, a very short program can reproduce arbitrary strings which has the same SE/KC value as that random string.\nSo here is a new definition of complexity of a physical system at state t:\nLength of the minimal UTM program that can reproduce random strings (w/ uniform probability) which has the same SE or KC value as the string of state t description.\n(Assuming the UTM has access to a random number generator.)\nWith this definition the complexity value would me max around the middle of time frame of a system and would be minimal at the ordered start and the max entropy end.\nComment #108 October 1st, 2011 at 3:35 pm\nI have to be honest and say I just simply did not have the time to read through all 106 previous comments, though many sounded interesting.\nAt any rate, one of the things that struck me at the conference (and which is what led to my continued harping on the entropy thing until I was shouted down by David Albet [:)]) is that we have a tendency to introduce unintentional bias even into our definitions.\nNow, I like the idea of Kolmogorov complexity (a colleague of mine in the Math Dept. studies Kolmogorov entropy and I had a student a few years back who was working with me to investigate how we could use it as a generalization). But I think that we really ought to go back even further to define what these things mean.\nThe conference (combined with reading a bunch of E.T. Jaynes’ work) got me to start work on developing a toy universe theory in which I stripped out absolutely everything including pre-conceived notions (e.g. how can we say an empty universe is Minkowski when there’s no way to operationally verify this for such a universe since, by definition, there’s nothing in it to do the verification? And what about an empty universe with different physical laws?). In essence, I’m trying to think like Jaynes a bit – don’t take information for granted that really isn’t there.\nSo, in short, I think there are unintended (hidden) assumptions and biases that go into our definitions and it’s something we need to be more aware of.\nComment #109 October 1st, 2011 at 10:02 pm\nOne more thought. I think we tend to misunderstand entropy. First, we should be more careful than to go on “intuition,” á là Sean’s milk example above. If we did that we’d all be Aristotelians. Second, I think it is more instructive to think of entropy as a measure of the number of possible configurations of a system. In that context it makes complete sense to see the early universe as low entropy and a sputtering soup of low-energy particles in the distant future as having a high entropy. It’s just like having a box of Legos: if some of the Legos are assembled, it reduces the number of things you can build with what’s left. But if they’re all disassembled then you can build almost anything! That being said, I think the entropy of the universe at that point will have reached some kind of maximum.\nComment #110 October 4th, 2011 at 8:24 pm\nOn logical depth, its increase at intermediate times, and its relation to sophistication and other complexity measures.\nThis is primarily a comment the logical depth of finite strings and its relation to Scott’s discussion (original post and comment 9) of the coffee cup experiment. Abram Demski (comment 47) made some similar points, and Dave Doty (comment 93) nicely summarizes the theory of logical depth and sophistication for infinite strings.\n“Logical depth” (http://www.research.ibm.com/people/b/bennetc/utmx.ps) formalizes the complexity of an object’s plausible causal history, as the time required for a deterministic universal computer to generate (a digitized representation of) the object from a nearly incompressible algorithmic description. Requiring the description to be incompressible or nearly so excludes descriptions that are computable from a still simpler descriptions, which would violate Occam’s razor.\nIn Scott’s coffee cup example, the intermediate state has a visibly complex pattern at the milk-coffee interface. This arises from a nontrivial physical process, whose simulation would require a nontrivial computation. I think the state illustrated in the middle picture is rather more complicated than could be reproduced by a discrete diffusion process of randomly swapping adjacent voxels, and likely involves convective turbulence as well as diffusion. The interface is logically deep because any near-incompressible program for generating it would need to approximate this physical evolution. The program might comprise a few bits describing the relevant dynamical laws (thermal expansion, heat conduction, fluctuations, and hydrodynamics), a few more bits describing the simple initial condition (cold milk over hot coffee), and many bits representing stochastic (thermal, chaotic, or quantum) influences on the evolution, leading to features such as the precise location of the tendrils that would occur differently if the experiment were repeated. By contrast, the final equilibrium state of the coffee is logically shallow, despite its longer temporal history, because an alternative computation could short-circuit the system’s actual evolution and generate the structure simply by calculating the thermodynamic equilibrium state under the prescribed boundary conditions. A fast-running, near-incompressible program would suffice to do so. Informally speaking, the intermediate state is deep because it contains internal evidence of a complicated causal history, while the final state is shallow because such evidence has been obliterated by the equilibration process.\nLogical depth is a fundamentally dynamic measure, having units of time or machine cycles, in contrast to minimal program size (Kolmogorov\ncomplexity) which is measured in informational units of bits. Program size enters into the definition of logical depth, but only in a secondary role, to quantify the unlikelihood that the object originated more quickly than its logical depth (an object x is defined to have logical depth d with b bits confidence iff all programs to compute it in time less than t are compressible by at least b bits). By contrast, some other proposed measures of nontrivial complexity, such as “sophistication” and “computational depth” are informational quantities like Kolmogorov complexity, but strive to measure only the nontrivial part of an object’s information content. Thus, like logical depth, they assign low complexity to random strings.\nSophistication (Koppel 88) does so by considering near-minimal-sized descriptions of x consisting of two parts, a “program” part p specifying a total function and a “data” part d giving an input to this function. The minimal size of p such that pd is a near-minimal description of x then defines the object’s sophistication, thereby formalizing the notion of the information content of the simplest computable ensemble within which x is a typical. Thus defined, sophistication is not very useful at sub-busy-beaver levels of run time, where it remains O(1) (to see this, let p be a constant program specifying a truncated version of the universal function, with run time bounded by some rapidly-growing computable function of d).\n“Computational depth” (Antunes et al ’06, ’07) refers to a family of complexity measures that combine time and program size into a single quantity by taking the difference between an object’s time-penalized and plain Kolmogorov complexities. With a logarithmic penalty function—the usual choice—computational depth often corresponds roughly to subjective notions of complexity at lower levels of complexity, but underestimates the complexity of deeper objects, especially those with a small amount of deeply buried redundancy. Conversely, with an inverse busy-beaver penalty function, computational depth approximates sophistication, reflecting the equivalence between truth-table reducibility and Turing reducibility in recursively-bounded time, and is similarly insensitive to merely exponential levels of subjective complexity.\nI think these infelicities of sophistication and computational depth stem from the procrustean attempt to combine time-complexity and program size into a single scalar measure. The proper role of program size is as a certifier of plausibility or significance. If considerations of program size justify a statistically significant inference about an object’s causal history, then the dynamic complexity associated with that inference can be confidently imputed to the object itself. That is the approach of logical depth.\nComment #111 October 7th, 2011 at 11:05 pm\nThose interested in measures of complexity should have a look at papers of Bialek, Nemenman and Tishby:\narXiv:physics/0103076\narXiv:physics/0007070\nComment #112 October 7th, 2011 at 11:17 pm\nCharles Bennett: Thanks so much for your wonderfully-informative comment, and sorry for the delay in responding!\nThe issue you mentioned with Koppel’s sophistication measure—that it remains O(1) if we allow arbitrary programs—is something I worried about as well. But as discussed in the post, I think this problem can be solved simply by restricting the minimization to “simple” or “efficient” programs, for almost any definition of “simple” or “efficient” one likes. And crucially, restricting attention to efficient programs is something that I’d want to do anyway, for at least two separate reasons:\n1. For me, an important requirement for any “complextropy” measure is that one be able to write actual computer programs to (crudely) approximate it, in at least some cases of physical interest. So the simpler the class of programs we’re dealing with, the better.\n2. While this might reflect nothing more than complexity-theoretic prejudice, I worry about the relevance to physics of any running time greater than exponential (or certainly doubly-exponential). So for example, if an n-bit string x could be generated by a log(n)-bit program that runs in 22^2^n time, but was indistinguishable from random by any o(n)-bit program running in less time, I’d be tempted to call x “incompressible for all physically-relevant purposes.”\nNow, regarding the question of whether the logical depth becomes large at intermediate times in the coffee cup example. Thanks so much for explaining your intuition for why it should become large! My basic worry is the following: how do we know that there isn’t some equally-compact program that will output the intermediate states in a short (near-linear) amount of time? Such a program could conceivably work, not by simulating the whole causal history of the coffee cup, but by some more direct method that first “sketched” the milk tendrils and then filled in more details as needed. Can you give me an argument why that sort of program would require more hard-coded entropy than the sort of program you described?\nOf course, you might point out that the same objection could be raised against any other “complextropy” measure! For example, how do I know that my resource-bounded sophistication measure, or Sean Carroll’s measure based on coarse-graining, will actually become large at intermediate times in the coffee-cup system? But for my and Sean’s measures, I know how to write practical computer programs that give, admittedly not the measures themselves, but certain crude approximations to them.\nFor Sean’s measure, what you do is to first “blur” or “smear” your bitmap, then feed the resulting image to some standard compression program (like gzip or pkzip) and measure the size of the compressed file. For the sophistication-based measure, what you do is to first compress your bitmap using some two-part code, then measure the size of the first part of the code only.\nOver the past few weeks, my student Lauren Oullette has actually been coding up these crude approximations, and seeing how they behave in a simulated coffee cup. So far, we’ve found that the coarse-graining-based measure does indeed increase and then decrease as Sean sketched on his slide, though there are a few surprises that we’re still trying to understand better. I’m looking forward to seeing what happens with the sophistication-based measure.\nSo, let me close with the following question for you: can you give us a “crude approximation” to logical depth that we can actually calculate in practice, analogous to the “crude approximations” to coarse-grained entropy and resource-bounded sophistication that I sketched above? If so, we’ll be thrilled to code that up as well and compare it against the other two!\nComment #113 October 25th, 2011 at 5:32 am\nWith regard to Kolmogorov complexity and entropy, note that there’s a whole development in the theory of dynamical systems showing how to connect Kolmogorov complexity with Shannon entropy. This goes all the way back to the 60s, when Zvonkin and Levin were trying to apply Kolmogorov’s idea to various fields and see if they yield a better understanding of what was going on in those fields. Then in the 70s a formal result was proved bu Brudno. Now, I believe there’s an unrealized potential for applying Kolmogorov’s approach to system identification in quantum physics: how does the observer know what the system is that he’s going to measure, and this problem of observer identity vs system identity actually makes use of the Zvonkin-Levin connection. This is all highly speculative, of course, but I thought it was worth mentioning in this discussion about entropy and Kolmogorov complexity.\nComment #114 October 31st, 2011 at 6:15 pm\nScott,\nSince you made the mistake of mentioning the faster-than-light neutrinos, this lets me mention a question that is actually closer to your line of work. You had shown if one has closed-time like curves then in that framework BQP=P=PSPACE. But, if it turned out that neutrinos were tachyons this would not in practice let one do this since it would be really difficult to send more than a tiny number of bits back using neutrinos. This leads to a question which I’m not sure how to make more precise, but essentially is: how restrictive can one be about how many bits one is allowed to send back and still be able to solve PSPACE problems in polynomial time? An obvious similar question is for problems in NP.\nHow natural this question is may be a function of what happens with the neutrinos. I agree that the claim is probably incorrect.\nComment #115 November 12th, 2011 at 5:06 am\n“Why does “complexity” or “interestingness” of physical systems seem to increase with time and then hit a maximum and decrease, in contrast to the entropy, which of course increases monotonically?”\nWhat an neat question!\nWhat are some other examples of closed physical systems that evolve on this sort of “interesting roller coaster of complexity”? The milk is a self-obsessed coagulate trying to keep its surface in tact. When a bit of the surface gets messed up, other milk moves in right in behind it for a swirly, tendril dance party. It seems there’s a lot of thermodynamic potential to drive complexity in the initial conditions…like hot and cold weather fronts about to form a tornado…but with a more interesting boundary. I think the prevalence of so many complex interfaces in our Universe between environments on every scale is astounding, and that’s a source of a lot of turbulence. I’m having trouble putting to rest the thought that the pattern of the coffee in the middle of mixing tickles our brain, sort of moving through the sweet-spot of how we compress the pattern, massaging our mental models. It’s okay to think of the positions of the milk molecules requiring more bits to specify once they start swirling? Were they informed that there was a cover charge at the dance party? All because we can’t compress them with as simple of a model on our computer? They’re saying don’t worry, the caffeine hasn’t given them the nervous shakes so much to forget where they are. Aren’t they just going to dance like any other simple program that produces beautiful, complex, nested, repetitive structures before the heat gets to them and everything turns into an orgy?\nHave you ever left a cup of coffee with cream on your desk for a day and a half? The patterns that the cream forms on the surface are great!\nComment #116 December 7th, 2011 at 2:47 pm\nI’m a bit late with this comment, but I just noticed something. You defined K(S) as “the number of bits in the shortest computer program that outputs the elements of S and then halts”, but elsewhere (in particular, in the paper you linked from Gács, Tromp, and Vitányi), the equivalent notion is defined not in terms of a program that outputs members of S, but in terms of a program that decides S. Was that intentional, and if so, why make that distinction? Could you use the decision version to define a notion of complextropy along the same lines of what you did, and would it differ in any meaningful way?\nComment #117 December 30th, 2011 at 2:28 am\n[…] to Favorites Building in part on my talk at the time conference, Scott Aaronson has a blog post about entropy and complexity that you should go read right now. It’s similar to one I’ve been contemplating myself, […]\nComment #118 December 30th, 2011 at 1:11 pm\n@116:\nIt seems like he meant to say “decides”, since he later refers to an oracle for membership in S. Not sure.\n@Scott: You mention using gzip to estimate K(), but it fails badly in a specific case:\n–Take a batch of random numbers or other high-entropy file of length N\n–Concatenate it with itself\n–Run through gzip\nThe result was ~2N when I tried it, rather than ~N (give or take a constant factor) as you would expect for the Kolmogorov complexity. It seems that gzip doesn’t handle large-scale patterns in the data well, which could be relevant to complexity / complextropy research. I can guess why not (memory constraints, probably) but it still seems problematic. Does LZMA or another more sophisticated algorithm handle this better?\nComment #119 February 12th, 2012 at 8:12 pm\n[…] by Sean Carroll's FQXi presentation and has been subsequently discussed by Scott Aaronson (see here and here) and Charlie […]\nComment #120 February 25th, 2012 at 5:58 pm\n[…] content, as opposed to its mere randomness. A recent example is Scott Aaronson’s notion of complextropy, defined roughly as the number of bits in the smallest program for a universal computer to […]\nComment #121 March 16th, 2012 at 10:44 pm\n[…] with Scott, is there any relation to his “complexodynamics” framework here? We congratulate him and Robert Wood on winning the Alan T. Waterman Award. See also this recent […]\nComment #122 June 19th, 2012 at 6:57 am\n[…] complexity is desirable, but what I think you have in mind is somewhat related to this blog entry:http://www.scottaaronson.com/blo…Comment Loading… • Post • 4:57am Add […]\nComment #123 July 25th, 2012 at 4:25 pm\nScott (and Charles (Bennett)),\nI think your concern about the applicability of complexity measures is legitimate. Myself and some colleagues of mine have been trying to use and evaluate information theoretic measures such as Kolmogorov Complexity and Logical Depth in real-world situations, in your words “crude approximations” of at least the “simplest versions”.\nWe recently published a paper (in the journal of Complexity) showing how Logical Depth can be successfully (with care) applied (the greatest challenge was the instability of timing functions in modern computers).\nYou may want to have a look at the paper (Image Characterization and Classification by Physical Complexity) available online at:\nhttp://arxiv.org/abs/1006.0051\n(we used the term “physical complexity” from Charles Bennett’s own suggestion in his original paper).\nThe results are quite interesting, first it is shown that Logical Depth (LD) does measure different properties compared to Kolmogorov Complexity (K) alone (that is LD assigns intuitively simple and random objects to the lowest complexity values, unlike K that assigns random objects the greatest complexity). Second, we also show that LD seems to classify images containing objects of different apparent complexities in a reasonable way, one in agreement with our intuition of what is complex versus random and simple.\nSincerely,\nHector Zenil\nP.s. We were also tackling the question you made about the rate of complex behaviour in increasingly larger rule spaces of cellular automata, I will keep you updated if you are interested.\nComment #124 September 18th, 2012 at 11:52 am\nAMS:\n> You mention using gzip to estimate K(), but it fails badly in a specific case: 1. Take a batch of random numbers or other high-entropy file of length N 2. Concatenate it with itself 3. Run through gzip.\n> The result was ~2N when I tried it, rather than ~N…I can guess why not (memory constraints, probably) but it still seems problematic. Does LZMA or another more sophisticated algorithm handle this better?\nYeah, I’d guess look-ahead is the main limit. But you can easily see for yourself using 7ZIP, which while not *perfect* does do a lot better than gzip apparently does. Here’s a script and output, and we’ll run it on file size from hundred kb to scores of mb range:\n$ for i in {1..5000}; do echo $RANDOM; done > test.txt && cat test.txt test.txt > test2.txt && 7z a -t7z -m0=lzma -mx=9 -mfb=64 -ms=on test.txt.7z test.txt && 7z a -t7z -m0=lzma -mx=9 -mfb=64 -ms=on test2.txt.7z test2.txt && du -ch test.txt test.txt.7z test2.txt test2.txt.7z; rm test*.txt*\n…\n224K test.txt\n16K test.txt.7z\n448K test2.txt\n16K test2.txt.7z\n704K total\n16K and 16K. Fantastic. Let’s get bigger:\n$ for i in {1..500000}; do echo $RANDOM; done > test.txt && cat test.txt test.txt > test2.txt && 7z a -t7z -m0=lzma -mx=9 -mfb=64 -ms=on test.txt.7z test.txt && 7z a -t7z -m0=lzma -mx=9 -mfb=64 -ms=on test2.txt.7z test2.txt && du -ch test.txt test.txt.7z test2.txt test2.txt.7z; rm test*.txt*\n…\n23M test.txt\n1.2M test.txt.7z\n46M test2.txt\n1.3M test2.txt.7z\n71M total\n1.2M and 1.3M. Good, but not perfect.\n$ for i in {1..1000000}; do echo $RANDOM; done > test.txt && cat test.txt test.txt > test2.txt && 7z a -t7z -m0=lzma -mx=9 -mfb=64 -ms=on test.txt.7z test.txt && 7z a -t7z -m0=lzma -mx=9 -mfb=64 -ms=on test2.txt.7z test2.txt && du -ch test.txt test.txt.7z test2.txt test2.txt.7z; rm test*.txt*\n…\n47M test.txt\n2.4M test.txt.7z\n93M test2.txt\n2.4M test2.txt.7z\n144M total\nNote that the 2 compressed files are very close – 2.4M and 2.4M. Great.\nBut we *can* blow the lookback window if we keep increasing the file size! Here is what happens if we double the file sizes yet again:\n$ for i in {1..2000000}; do echo $RANDOM; done > test.txt && cat test.txt test.txt > test2.txt && 7z a -t7z -m0=lzma -mx=9 -mfb=64 -ms=on test.txt.7z test.txt && 7z a -t7z -m0=lzma -mx=9 -mfb=64 -ms=on test2.txt.7z test2.txt && du -ch test.txt test.txt.7z test2.txt test2.txt.7z; rm test*.txt*\n…\n93M test.txt\n4.8M test.txt.7z\n186M test2.txt\n9.5M test2.txt.7z\n293M total\nWhups! Now test.txt.7z is 4.8M and test2.txt.7z is… 9.5M. A little less than twice as large.\n—————————————————————————–\nScott:\n> In the interest of a full disclosure, a wonderful MIT undergrad, Lauren Oullette, recently started a research project with me where she’s trying to do exactly that. So hopefully, by the end of the semester, we’ll be able to answer Sean’s question at least at a physics level of rigor! Answering the question at a math/CS level of rigor could take a while longer.\nI’ve been following the RSS feed for a long time, but I don’t recall any followup. How did the research go?\nComment #125 January 20th, 2014 at 3:57 pm\n[…] an intro to entropy on Wikipedia, and also see this blog post that they referred to in their […]\nComment #126 September 29th, 2016 at 3:27 pm\n[…] The First Law of Complexodynamics – Unfortunately, as we defined it above, sophistication still doesn’t do the job. For deterministic systems, the problem is the same as the one pointed out earlier for Kolmogorov complexity: we can always describe the system’s state after t time … […]\nComment #127 November 24th, 2016 at 12:47 pm\n[…] in part on my talk at the time conference, Scott Aaronson has a blog post about entropy and complexity that you should go read right now. It’s similar to one I’ve been contemplating myself, […]\n\n\n\nThe Unreasonable Effectiveness of Recurrent Neural Networks\nThere’s something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent network for Image Captioning. Within a few dozen minutes of training my first baby model (with rather arbitrarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times. What made this result so shocking at the time was that the common wisdom was that RNNs were supposed to be difficult to train (with more experience I’ve in fact reached the opposite conclusion). Fast forward about a year: I’m training RNNs all the time and I’ve witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me. This post is about sharing some of that magic with you.\nWe’ll train RNNs to generate text character by character and ponder the question “how is that even possible?”\nBy the way, together with this post I am also releasing code on Github that allows you to train character-level language models based on multi-layer LSTMs. You give it a large chunk of text and it will learn to generate text like it one character at a time. You can also use it to reproduce my experiments below. But we’re getting ahead of ourselves; What are RNNs anyway?\nRecurrent Neural Networks\nSequences. Depending on your background you might be wondering: What makes Recurrent Networks so special? A glaring limitation of Vanilla Neural Networks (and also Convolutional Networks) is that their API is too constrained: they accept a fixed-sized vector as input (e.g. an image) and produce a fixed-sized vector as output (e.g. probabilities of different classes). Not only that: These models perform this mapping using a fixed amount of computational steps (e.g. the number of layers in the model). The core reason that recurrent nets are more exciting is that they allow us to operate over sequences of vectors: Sequences in the input, the output, or in the most general case both. A few examples may make this more concrete:\nAs you might expect, the sequence regime of operation is much more powerful compared to fixed networks that are doomed from the get-go by a fixed number of computational steps, and hence also much more appealing for those of us who aspire to build more intelligent systems. Moreover, as we’ll see in a bit, RNNs combine the input vector with their state vector with a fixed (but learned) function to produce a new state vector. This can in programming terms be interpreted as running a fixed program with certain inputs and some internal variables. Viewed this way, RNNs essentially describe programs. In fact, it is known that RNNs are Turing-Complete in the sense that they can to simulate arbitrary programs (with proper weights). But similar to universal approximation theorems for neural nets you shouldn’t read too much into this. In fact, forget I said anything.\nIf training vanilla neural nets is optimization over functions, training recurrent nets is optimization over programs.\nSequential processing in absence of sequences. You might be thinking that having sequences as inputs or outputs could be relatively rare, but an important point to realize is that even if your inputs/outputs are fixed vectors, it is still possible to use this powerful formalism to process them in a sequential manner. For instance, the figure below shows results from two very nice papers from DeepMind. On the left, an algorithm learns a recurrent network policy that steers its attention around an image; In particular, it learns to read out house numbers from left to right (Ba et al.). On the right, a recurrent network generates images of digits by learning to sequentially add color to a canvas (Gregor et al.):\nThe takeaway is that even if your data is not in form of sequences, you can still formulate and train powerful models that learn to process it sequentially. You’re learning stateful programs that process your fixed-sized data.\nRNN computation. So how do these things work? At the core, RNNs have a deceptively simple API: They accept an input vector x\nand give you an output vector y\n. However, crucially this output vector’s contents are influenced not only by the input you just fed in, but also on the entire history of inputs you’ve fed in in the past. Written as a class, the RNN’s API consists of a single step\nfunction:\nrnn = RNN()\ny = rnn.step(x) # x is an input vector, y is the RNN's output vector\nThe RNN class has some internal state that it gets to update every time step\nis called. In the simplest case this state consists of a single hidden vector h\n. Here is an implementation of the step function in a Vanilla RNN:\nclass RNN:\n# ...\ndef step(self, x):\n# update the hidden state\nself.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n# compute the output vector\ny = np.dot(self.W_hy, self.h)\nreturn y\nThe above specifies the forward pass of a vanilla RNN. This RNN’s parameters are the three matrices W_hh, W_xh, W_hy\n. The hidden state self.h\nis initialized with the zero vector. The np.tanh\nfunction implements a non-linearity that squashes the activations to the range [-1, 1]\n. Notice briefly how this works: There are two terms inside of the tanh: one is based on the previous hidden state and one is based on the current input. In numpy np.dot\nis matrix multiplication. The two intermediates interact with addition, and then get squashed by the tanh into the new state vector. If you’re more comfortable with math notation, we can also write the hidden state update as \\( h_t = \\tanh ( W_{hh} h_{t-1} + W_{xh} x_t ) \\), where tanh is applied elementwise.\nWe initialize the matrices of the RNN with random numbers and the bulk of work during training goes into finding the matrices that give rise to desirable behavior, as measured with some loss function that expresses your preference to what kinds of outputs y\nyou’d like to see in response to your input sequences x\n.\nGoing deep. RNNs are neural networks and everything works monotonically better (if done right) if you put on your deep learning hat and start stacking models up like pancakes. For instance, we can form a 2-layer recurrent network as follows:\ny1 = rnn1.step(x)\ny = rnn2.step(y1)\nIn other words we have two separate RNNs: One RNN is receiving the input vectors and the second RNN is receiving the output of the first RNN as its input. Except neither of these RNNs know or care - it’s all just vectors coming in and going out, and some gradients flowing through each module during backpropagation.\nGetting fancy. I’d like to briefly mention that in practice most of us use a slightly different formulation than what I presented above called a Long Short-Term Memory (LSTM) network. The LSTM is a particular type of recurrent network that works slightly better in practice, owing to its more powerful update equation and some appealing backpropagation dynamics. I won’t go into details, but everything I’ve said about RNNs stays exactly the same, except the mathematical form for computing the update (the line self.h = ...\n) gets a little more complicated. From here on I will use the terms “RNN/LSTM” interchangeably but all experiments in this post use an LSTM.\nCharacter-Level Language Models\nOkay, so we have an idea about what RNNs are, why they are super exciting, and how they work. We’ll now ground this in a fun application: We’ll train RNN character-level language models. That is, we’ll give the RNN a huge chunk of text and ask it to model the probability distribution of the next character in the sequence given a sequence of previous characters. This will then allow us to generate new text one character at a time.\nAs a working example, suppose we only had a vocabulary of four possible letters “helo”, and wanted to train an RNN on the training sequence “hello”. This training sequence is in fact a source of 4 separate training examples: 1. The probability of “e” should be likely given the context of “h”, 2. “l” should be likely in the context of “he”, 3. “l” should also be likely given the context of “hel”, and finally 4. “o” should be likely given the context of “hell”.\nConcretely, we will encode each character into a vector using 1-of-k encoding (i.e. all zero except for a single one at the index of the character in the vocabulary), and feed them into the RNN one at a time with the step\nfunction. We will then observe a sequence of 4-dimensional output vectors (one dimension per character), which we interpret as the confidence the RNN currently assigns to each character coming next in the sequence. Here’s a diagram:\nFor example, we see that in the first time step when the RNN saw the character “h” it assigned confidence of 1.0 to the next letter being “h”, 2.2 to letter “e”, -3.0 to “l”, and 4.1 to “o”. Since in our training data (the string “hello”) the next correct character is “e”, we would like to increase its confidence (green) and decrease the confidence of all other letters (red). Similarly, we have a desired target character at every one of the 4 time steps that we’d like the network to assign a greater confidence to. Since the RNN consists entirely of differentiable operations we can run the backpropagation algorithm (this is just a recursive application of the chain rule from calculus) to figure out in what direction we should adjust every one of its weights to increase the scores of the correct targets (green bold numbers). We can then perform a parameter update, which nudges every weight a tiny amount in this gradient direction. If we were to feed the same inputs to the RNN after the parameter update we would find that the scores of the correct characters (e.g. “e” in the first time step) would be slightly higher (e.g. 2.3 instead of 2.2), and the scores of incorrect characters would be slightly lower. We then repeat this process over and over many times until the network converges and its predictions are eventually consistent with the training data in that correct characters are always predicted next.\nA more technical explanation is that we use the standard Softmax classifier (also commonly referred to as the cross-entropy loss) on every output vector simultaneously. The RNN is trained with mini-batch Stochastic Gradient Descent and I like to use RMSProp or Adam (per-parameter adaptive learning rate methods) to stablilize the updates.\nNotice also that the first time the character “l” is input, the target is “l”, but the second time the target is “o”. The RNN therefore cannot rely on the input alone and must use its recurrent connection to keep track of the context to achieve this task.\nAt test time, we feed a character into the RNN and get a distribution over what characters are likely to come next. We sample from this distribution, and feed it right back in to get the next letter. Repeat this process and you’re sampling text! Lets now train an RNN on different datasets and see what happens.\nTo further clarify, for educational purposes I also wrote a minimal character-level RNN language model in Python/numpy. It is only about 100 lines long and hopefully it gives a concise, concrete and useful summary of the above if you’re better at reading code than text. We’ll now dive into example results, produced with the much more efficient Lua/Torch codebase.\nFun with RNNs\nAll 5 example character models below were trained with the code I’m releasing on Github. The input in each case is a single file with some text, and we’re training an RNN to predict the next character in the sequence.\nPaul Graham generator\nLets first try a small dataset of English as a sanity check. My favorite fun dataset is the concatenation of Paul Graham’s essays. The basic idea is that there’s a lot of wisdom in these essays, but unfortunately Paul Graham is a relatively slow generator. Wouldn’t it be great if we could sample startup wisdom on demand? That’s where an RNN comes in.\nConcatenating all pg essays over the last ~5 years we get approximately 1MB text file, or about 1 million characters (this is considered a very small dataset by the way). Technical: Lets train a 2-layer LSTM with 512 hidden nodes (approx. 3.5 million parameters), and with dropout of 0.5 after each layer. We’ll train with batches of 100 examples and truncated backpropagation through time of length 100 characters. With these settings one batch on a TITAN Z GPU takes about 0.46 seconds (this can be cut in half with 50 character BPTT at negligible cost in performance). Without further ado, lets see a sample from the RNN:\n“The surprised in investors weren’t going to raise money. I’m not the company with the time there are all interesting quickly, don’t have to get off the same programmers. There’s a super-angel round fundraising, why do you can do. If you have a different physical investment are become in people who reduced in a startup with the way to argument the acquirer could see them just that you’re also the founders will part of users’ affords that and an alternation to the idea. [2] Don’t work at first member to see the way kids will seem in advance of a bad successful startup. And if you have to act the big company too.”\nOkay, clearly the above is unfortunately not going to replace Paul Graham anytime soon, but remember that the RNN had to learn English completely from scratch and with a small dataset (including where you put commas, apostrophes and spaces). I also like that it learns to support its own arguments (e.g. [2], above). Sometimes it says something that offers a glimmer of insight, such as “a company is a meeting to think to investors”. Here’s a link to 50K character sample if you’d like to see more.\nTemperature. We can also play with the temperature of the Softmax during sampling. Decreasing the temperature from 1 to some lower number (e.g. 0.5) makes the RNN more confident, but also more conservative in its samples. Conversely, higher temperatures will give more diversity but at cost of more mistakes (e.g. spelling mistakes, etc). In particular, setting temperature very near zero will give the most likely thing that Paul Graham might say:\n“is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same”\nlooks like we’ve reached an infinite loop about startups.\nShakespeare\nIt looks like we can learn to spell English words. But how about if there is more structure and style in the data? To examine this I downloaded all the works of Shakespeare and concatenated them into a single (4.4MB) file. We can now afford to train a larger network, in this case lets try a 3-layer RNN with 512 hidden nodes on each layer. After we train the network for a few hours we obtain samples such as:\nPANDARUS:\nAlas, I think he shall be come approached and the day\nWhen little srain would be attain'd into being never fed,\nAnd who is but a chain and subjects of his death,\nI should not sleep.\nSecond Senator:\nThey are away this miseries, produced upon my soul,\nBreaking and strongly should be buried, when I perish\nThe earth and thoughts of many states.\nDUKE VINCENTIO:\nWell, your wit is in the care of side and that.\nSecond Lord:\nThey would be ruled after this chamber, and\nmy fair nues begun out of the fact, to be conveyed,\nWhose noble souls I'll have the heart of the wars.\nClown:\nCome, sir, I will make did behold your worship.\nVIOLA:\nI'll drink it.\nRemember, all the RNN knows are characters, so in particular it samples both speaker’s names and the contents. Sometimes we also get relatively extented monologue passages, such as:\nVIOLA:\nWhy, Salisbury must find his flesh and thought\nThat which I am not aps, not a man and in fire,\nTo show the reining of the raven and the wars\nTo grace my hand reproach within, and not a fair are hand,\nThat Caesar and my goodly father's world;\nWhen I was heaven of presence and our fleets,\nWe spare with hours, but cut thy council I am great,\nMurdered and by thy master's ready there\nMy power to give thee but so much as hell:\nSome service in the noble bondman here,\nWould show him to her wine.\nKING LEAR:\nO, if you were a feeble sight, the courtesy of your law,\nYour sight and several breath, will wear the gods\nWith his heads, and my hands are wonder'd at the deeds,\nSo drop upon your lordship's head, and your opinion\nShall be against your honour.\nI can barely recognize these samples from actual Shakespeare :) If you like Shakespeare, you might appreciate this 100,000 character sample. Of course, you can also generate an infinite amount of your own samples at different temperatures with the provided code.\nWikipedia\nWe saw that the LSTM can learn to spell words and copy general syntactic structures. Lets further increase the difficulty and train on structured markdown. In particular, lets take the Hutter Prize 100MB dataset of raw Wikipedia and train an LSTM. Following Graves et al., I used the first 96MB for training, the rest for validation and ran a few models overnight. We can now sample Wikipedia articles! Below are a few fun excerpts. First, some basic markdown output:\nNaturalism and decision for the majority of Arab countries' capitalide was grounded\nby the Irish language by [[John Clair]], [[An Imperial Japanese Revolt]], associated\nwith Guangzham's sovereignty. His generals were the powerful ruler of the Portugal\nin the [[Protestant Immineners]], which could be said to be directly in Cantonese\nCommunication, which followed a ceremony and set inspired prison, training. The\nemperor travelled back to [[Antioch, Perth, October 25|21]] to note, the Kingdom\nof Costa Rica, unsuccessful fashioned the [[Thrales]], [[Cynth's Dajoard]], known\nin western [[Scotland]], near Italy to the conquest of India with the conflict.\nCopyright was the succession of independence in the slop of Syrian influence that\nwas a famous German movement based on a more popular servicious, non-doctrinal\nand sexual power post. Many governments recognize the military housing of the\n[[Civil Liberalization and Infantry Resolution 265 National Party in Hungary]],\nthat is sympathetic to be to the [[Punjab Resolution]]\n(PJS)[http://www.humah.yahoo.com/guardian.\ncfm/7754800786d17551963s89.htm Official economics Adjoint for the Nazism, Montgomery\nwas swear to advance to the resources for those Socialism's rule,\nwas starting to signing a major tripad of aid exile.]]\nIn case you were wondering, the yahoo url above doesn’t actually exist, the model just hallucinated it. Also, note that the model learns to open and close the parenthesis correctly. There’s also quite a lot of structured markdown that the model learns, for example sometimes it creates headings, lists, etc.:\n{ { cite journal | id=Cerling Nonforest Department|format=Newlymeslated|none } }\n''www.e-complete''.\n'''See also''': [[List of ethical consent processing]]\n== See also ==\n*[[Iender dome of the ED]]\n*[[Anti-autism]]\n===[[Religion|Religion]]===\n*[[French Writings]]\n*[[Maria]]\n*[[Revelation]]\n*[[Mount Agamul]]\n== External links==\n* [http://www.biblegateway.nih.gov/entrepre/ Website of the World Festival. The labour of India-county defeats at the Ripper of California Road.]\n==External links==\n* [http://www.romanology.com/ Constitution of the Netherlands and Hispanic Competition for Bilabial and Commonwealth Industry (Republican Constitution of the Extent of the Netherlands)]\nSometimes the model snaps into a mode of generating random but valid XML:\n<page>\n<title>Antichrist</title>\n<id>865</id>\n<revision>\n<id>15900676</id>\n<timestamp>2002-08-03T18:14:12Z</timestamp>\n<contributor>\n<username>Paris</username>\n<id>23</id>\n</contributor>\n<minor />\n<comment>Automated conversion</comment>\n<text xml:space=\"preserve\">#REDIRECT [[Christianity]]</text>\n</revision>\n</page>\nThe model completely makes up the timestamp, id, and so on. Also, note that it closes the correct tags appropriately and in the correct nested order. Here are 100,000 characters of sampled wikipedia if you’re interested to see more.\nAlgebraic Geometry (Latex)\nThe results above suggest that the model is actually quite good at learning complex syntactic structures. Impressed by these results, my labmate (Justin Johnson) and I decided to push even further into structured territories and got a hold of this book on algebraic stacks/geometry. We downloaded the raw Latex source file (a 16MB file) and trained a multilayer LSTM. Amazingly, the resulting sampled Latex almost compiles. We had to step in and fix a few issues manually but then you get plausible looking math, it’s quite astonishing:\nHere’s another sample:\nAs you can see above, sometimes the model tries to generate latex diagrams, but clearly it hasn’t really figured them out. I also like the part where it chooses to skip a proof (“Proof omitted.”, top left). Of course, keep in mind that latex has a relatively difficult structured syntactic format that I haven’t even fully mastered myself. For instance, here is a raw sample from the model (unedited):\n\\begin{proof}\nWe may assume that $\\mathcal{I}$ is an abelian sheaf on $\\mathcal{C}$.\n\\item Given a morphism $\\Delta : \\mathcal{F} \\to \\mathcal{I}$\nis an injective and let $\\mathfrak q$ be an abelian sheaf on $X$.\nLet $\\mathcal{F}$ be a fibered complex. Let $\\mathcal{F}$ be a category.\n\\begin{enumerate}\n\\item \\hyperref[setain-construction-phantom]{Lemma}\n\\label{lemma-characterize-quasi-finite}\nLet $\\mathcal{F}$ be an abelian quasi-coherent sheaf on $\\mathcal{C}$.\nLet $\\mathcal{F}$ be a coherent $\\mathcal{O}_X$-module. Then\n$\\mathcal{F}$ is an abelian catenary over $\\mathcal{C}$.\n\\item The following are equivalent\n\\begin{enumerate}\n\\item $\\mathcal{F}$ is an $\\mathcal{O}_X$-module.\n\\end{lemma}\nThis sample from a relatively decent model illustrates a few common mistakes. For example, the model opens a \\begin{proof}\nenvironment but then ends it with a \\end{lemma}\n. This is an example of a problem we’d have to fix manually, and is likely due to the fact that the dependency is too long-term: By the time the model is done with the proof it has forgotten whether it was doing a proof or a lemma. Similarly, it opens an \\begin{enumerate}\nbut then forgets to close it. We observed that these became less common with larger/better models, but nonetheless, these are the kinds of mistakes that come up.\nLinux Source Code\nI wanted to push structured data to its limit, so for the final challenge I decided to use code. In particular, I took all the source and header files found in the Linux repo on Github, concatenated all of them in a single giant file (474MB of C code) (I was originally going to train only on the kernel but that by itself is only ~16MB). Then I trained several as-large-as-fits-on-my-GPU 3-layer LSTMs over a period of a few days. These models have about 10 million parameters, which is still on the lower end for RNN models. The results are superfun:\n/*\n* Increment the size file of the new incorrect UI_FILTER group information\n* of the size generatively.\n*/\nstatic int indicate_policy(void)\n{\nint error;\nif (fd == MARN_EPT) {\n/*\n* The kernel blank will coeld it to userspace.\n*/\nif (ss->segment < mem_total)\nunblock_graph_and_set_blocked();\nelse\nret = 1;\ngoto bail;\n}\nsegaddr = in_SB(in.addr);\nselector = seg / 16;\nsetup_works = true;\nfor (i = 0; i < blocks; i++) {\nseq = buf[i++];\nbpf = bd->bd.next + i * search;\nif (fd) {\ncurrent = blocked;\n}\n}\nrw->name = \"Getjbbregs\";\nbprm_self_clearl(&iv->version);\nregs->new = blocks[(BPF_STATS << info->historidac)] | PFMR_CLOBATHINC_SECONDS << 12;\nreturn segtable;\n}\nThe code looks really quite great overall. Of course, I don’t think it compiles but when you scroll through the generate code it feels very much like a giant C code base. Notice that the RNN peppers its code with comments here and there at random. It is also very good at making very few syntactic errors. For example, it uses strings properly, pointer notation, etc. It also opens and closes brackets {[\ncorrectly and learns to indent its code very well. A common error is that it can’t keep track of variable names: It often uses undefined variables (e.g. rw\nabove), declares variables it never uses (e.g. int error\n), or returns non-existing variables. Lets see a few more examples. Here’s another snippet that shows a wider array of operations that the RNN learns:\n/*\n* If this error is set, we will need anything right after that BSD.\n*/\nstatic void action_new_function(struct s_stat_info *wb)\n{\nunsigned long flags;\nint lel_idx_bit = e->edd, *sys & ~((unsigned long) *FIRST_COMPAT);\nbuf[0] = 0xFFFFFFFF & (bit << 4);\nmin(inc, slist->bytes);\nprintk(KERN_WARNING \"Memory allocated %02x/%02x, \"\n\"original MLL instead\\n\"),\nmin(min(multi_run - s->len, max) * num_data_in),\nframe_pos, sz + first_seg);\ndiv_u64_w(val, inb_p);\nspin_unlock(&disk->queue_lock);\nmutex_unlock(&s->sock->mutex);\nmutex_unlock(&func->mutex);\nreturn disassemble(info->pending_bh);\n}\nstatic void num_serial_settings(struct tty_struct *tty)\n{\nif (tty == tty)\ndisable_single_st_p(dev);\npci_disable_spool(port);\nreturn 0;\n}\nstatic void do_command(struct seq_file *m, void *v)\n{\nint column = 32 << (cmd[2] & 0x80);\nif (state)\ncmd = (int)(int_state ^ (in_8(&ch->ch_flags) & Cmd) ? 2 : 1);\nelse\nseq = 1;\nfor (i = 0; i < 16; i++) {\nif (k & (1 << 1))\npipe = (in_use & UMXTHREAD_UNCCA) +\n((count & 0x00000000fffffff8) & 0x000000f) << 8;\nif (count == 0)\nsub(pid, ppc_md.kexec_handle, 0x20000000);\npipe_set_bytes(i, 0);\n}\n/* Free our user pages pointer to place camera if all dash */\nsubsystem_info = &of_changes[PAGE_SIZE];\nrek_controls(offset, idx, &soffset);\n/* Now we want to deliberately put it to device */\ncontrol_check_polarity(&context, val, 0);\nfor (i = 0; i < COUNTER; i++)\nseq_puts(s, \"policy \");\n}\nNotice that in the second function the model compares tty == tty\n, which is vacuously true. On the other hand, at least the variable tty\nexists in the scope this time! In the last function, notice that the code does not return anything, which happens to be correct since the function signature is void\n. However, the first two functions were also declared void\nand did return values. This is again a form of a common mistake due to long-term interactions.\nSometimes the model decides that it’s time to sample a new file. This is usually a very amusing part: The model first recites the GNU license character by character, samples a few includes, generates some macros and then dives into the code:\n/*\n* Copyright (c) 2006-2010, Intel Mobile Communications. All rights reserved.\n*\n* This program is free software; you can redistribute it and/or modify it\n* under the terms of the GNU General Public License version 2 as published by\n* the Free Software Foundation.\n*\n* This program is distributed in the hope that it will be useful,\n* but WITHOUT ANY WARRANTY; without even the implied warranty of\n* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n*\n* GNU General Public License for more details.\n*\n* You should have received a copy of the GNU General Public License\n* along with this program; if not, write to the Free Software Foundation,\n* Inc., 675 Mass Ave, Cambridge, MA 02139, USA.\n*/\n#include <linux/kexec.h>\n#include <linux/errno.h>\n#include <linux/io.h>\n#include <linux/platform_device.h>\n#include <linux/multi.h>\n#include <linux/ckevent.h>\n#include <asm/io.h>\n#include <asm/prom.h>\n#include <asm/e820.h>\n#include <asm/system_info.h>\n#include <asm/setew.h>\n#include <asm/pgproto.h>\n#define REG_PG vesa_slot_addr_pack\n#define PFM_NOCOMP AFSR(0, load)\n#define STACK_DDR(type) (func)\n#define SWAP_ALLOCATE(nr) (e)\n#define emulate_sigs() arch_get_unaligned_child()\n#define access_rw(TST) asm volatile(\"movd %%esp, %0, %3\" : : \"r\" (0)); \\\nif (__type & DO_READ)\nstatic void stat_PC_SEC __read_mostly offsetof(struct seq_argsqueue, \\\npC>[1]);\nstatic void\nos_prefix(unsigned long sys)\n{\n#ifdef CONFIG_PREEMPT\nPUT_PARAM_RAID(2, sel) = get_state_state();\nset_pid_sum((unsigned long)state, current_state_str(),\n(unsigned long)-1->lr_full; low;\n}\nThere are too many fun parts to cover- I could probably write an entire blog post on just this part. I’ll cut it short for now, but here is 1MB of sampled Linux code for your viewing pleasure.\nGenerating Baby Names\nLets try one more for fun. Lets feed the RNN a large text file that contains 8000 baby names listed out, one per line (names obtained from here). We can feed this to the RNN and then generate new names! Here are some example names, only showing the ones that do not occur in the training data (90% don’t):\nRudi Levette Berice Lussa Hany Mareanne Chrestina Carissy Marylen Hammine Janye Marlise Jacacrie Hendred Romand Charienna Nenotto Ette Dorane Wallen Marly Darine Salina Elvyn Ersia Maralena Minoria Ellia Charmin Antley Nerille Chelon Walmor Evena Jeryly Stachon Charisa Allisa Anatha Cathanie Geetra Alexie Jerin Cassen Herbett Cossie Velen Daurenge Robester Shermond Terisa Licia Roselen Ferine Jayn Lusine Charyanne Sales Sanny Resa Wallon Martine Merus Jelen Candica Wallin Tel Rachene Tarine Ozila Ketia Shanne Arnande Karella Roselina Alessia Chasty Deland Berther Geamar Jackein Mellisand Sagdy Nenc Lessie Rasemy Guen Gavi Milea Anneda Margoris Janin Rodelin Zeanna Elyne Janah Ferzina Susta Pey Castina\nYou can see many more here. Some of my favorites include “Baby” (haha), “Killie”, “Char”, “R”, “More”, “Mars”, “Hi”, “Saddie”, “With” and “Ahbort”. Well that was fun. Of course, you can imagine this being quite useful inspiration when writing a novel, or naming a new startup :)\nUnderstanding what’s going on\nWe saw that the results at the end of training can be impressive, but how does any of this work? Lets run two quick experiments to briefly peek under the hood.\nThe evolution of samples while training\nFirst, it’s fun to look at how the sampled text evolves while the model trains. For example, I trained an LSTM of Leo Tolstoy’s War and Peace and then generated samples every 100 iterations of training. At iteration 100 the model samples random jumbles:\ntyntd-iafhatawiaoihrdemot lytdws e ,tfti, astai f ogoh eoase rrranbyne 'nhthnee e\nplia tklrgd t o idoe ns,smtt h ne etie h,hregtrs nigtike,aoaenns lng\nHowever, notice that at least it is starting to get an idea about words separated by spaces. Except sometimes it inserts two spaces. It also doesn’t know that comma is amost always followed by a space. At 300 iterations we see that the model starts to get an idea about quotes and periods:\n\"Tmont thithey\" fomesscerliund\nKeushey. Thom here\nsheulke, anmerenith ol sivh I lalterthend Bleipile shuwy fil on aseterlome\ncoaniogennc Phe lism thond hon at. MeiDimorotion in ther thize.\"\nThe words are now also separated with spaces and the model starts to get the idea about periods at the end of a sentence. At iteration 500:\nwe counter. He stutn co des. His stanted out one ofler that concossions and was\nto gearang reay Jotrets and with fre colt otf paitt thin wall. Which das stimn\nthe model has now learned to spell the shortest and most common words such as “we”, “He”, “His”, “Which”, “and”, etc. At iteration 700 we’re starting to see more and more English-like text emerge:\nAftair fall unsuch that the hall for Prince Velzonski's that me of\nher hearly, and behs to so arwage fiving were to it beloge, pavu say falling misfort\nhow, and Gogition is so overelical and ofter.\nAt iteration 1200 we’re now seeing use of quotations and question/exclamation marks. Longer words have now been learned as well:\n\"Kite vouch!\" he repeated by her\ndoor. \"But I would be done and quarts, feeling, then, son is people....\"\nUntil at last we start to get properly spelled words, quotations, names, and so on by about iteration 2000:\n\"Why do what that day,\" replied Natasha, and wishing to himself the fact the\nprincess, Princess Mary was easier, fed in had oftened him.\nPierre aking his soul came to the packs and drove up his father-in-law women.\nThe picture that emerges is that the model first discovers the general word-space structure and then rapidly starts to learn the words; First starting with the short words and then eventually the longer ones. Topics and themes that span multiple words (and in general longer-term dependencies) start to emerge only much later.\nVisualizing the predictions and the “neuron” firings in the RNN\nAnother fun visualization is to look at the predicted distributions over characters. In the visualizations below we feed a Wikipedia RNN model character data from the validation set (shown along the blue/green rows) and under every character we visualize (in red) the top 5 guesses that the model assigns for the next character. The guesses are colored by their probability (so dark red = judged as very likely, white = not very likely). For example, notice that there are stretches of characters where the model is extremely confident about the next letter (e.g., the model is very confident about characters during the http://www. sequence).\nThe input character sequence (blue/green) is colored based on the firing of a randomly chosen neuron in the hidden representation of the RNN. Think about it as green = very excited and blue = not very excited (for those familiar with details of LSTMs, these are values between [-1,1] in the hidden state vector, which is just the gated and tanh’d LSTM cell state). Intuitively, this is visualizing the firing rate of some neuron in the “brain” of the RNN while it reads the input sequence. Different neurons might be looking for different patterns; Below we’ll look at 4 different ones that I found and thought were interesting or interpretable (many also aren’t):\nOf course, a lot of these conclusions are slightly hand-wavy as the hidden state of the RNN is a huge, high-dimensional and largely distributed representation. These visualizations were produced with custom HTML/CSS/Javascript, you can see a sketch of what’s involved here if you’d like to create something similar.\nWe can also condense this visualization by excluding the most likely predictions and only visualize the text, colored by activations of a cell. We can see that in addition to a large portion of cells that do not do anything interpretible, about 5% of them turn out to have learned quite interesting and interpretible algorithms:\nAgain, what is beautiful about this is that we didn’t have to hardcode at any point that if you’re trying to predict the next character it might, for example, be useful to keep track of whether or not you are currently inside or outside of quote. We just trained the LSTM on raw data and it decided that this is a useful quantitity to keep track of. In other words one of its cells gradually tuned itself during training to become a quote detection cell, since this helps it better perform the final task. This is one of the cleanest and most compelling examples of where the power in Deep Learning models (and more generally end-to-end training) is coming from.\nSource Code\nI hope I’ve convinced you that training character-level language models is a very fun exercise. You can train your own models using the char-rnn code I released on Github (under MIT license). It takes one large text file and trains a character-level model that you can then sample from. Also, it helps if you have a GPU or otherwise training on CPU will be about a factor of 10x slower. In any case, if you end up training on some data and getting fun results let me know! And if you get lost in the Torch/Lua codebase remember that all it is is just a more fancy version of this 100-line gist.\nBrief digression. The code is written in Torch 7, which has recently become my favorite deep learning framework. I’ve only started working with Torch/LUA over the last few months and it hasn’t been easy (I spent a good amount of time digging through the raw Torch code on Github and asking questions on their gitter to get things done), but once you get a hang of things it offers a lot of flexibility and speed. I’ve also worked with Caffe and Theano in the past and I believe Torch, while not perfect, gets its levels of abstraction and philosophy right better than others. In my view the desirable features of an effective framework are:\n- CPU/GPU transparent Tensor library with a lot of functionality (slicing, array/matrix operations, etc. )\n- An entirely separate code base in a scripting language (ideally Python) that operates over Tensors and implements all Deep Learning stuff (forward/backward, computation graphs, etc)\n- It should be possible to easily share pretrained models (Caffe does this well, others don’t), and crucially\n- NO compilation step (or at least not as currently done in Theano). The trend in Deep Learning is towards larger, more complex networks that are are time-unrolled in complex graphs. It is critical that these do not compile for a long time or development time greatly suffers. Second, by compiling one gives up interpretability and the ability to log/debug effectively. If there is an option to compile the graph once it has been developed for efficiency in prod that’s fine.\nFurther Reading\nBefore the end of the post I also wanted to position RNNs in a wider context and provide a sketch of the current research directions. RNNs have recently generated a significant amount of buzz and excitement in the field of Deep Learning. Similar to Convolutional Networks they have been around for decades but their full potential has only recently started to get widely recognized, in large part due to our growing computational resources. Here’s a brief sketch of a few recent developments (definitely not complete list, and a lot of this work draws from research back to 1990s, see related work sections):\nIn the domain of NLP/Speech, RNNs transcribe speech to text, perform machine translation, generate handwritten text, and of course, they have been used as powerful language models (Sutskever et al.) (Graves) (Mikolov et al.) (both on the level of characters and words). Currently it seems that word-level models work better than character-level models, but this is surely a temporary thing.\nComputer Vision. RNNs are also quickly becoming pervasive in Computer Vision. For example, we’re seeing RNNs in frame-level video classification, image captioning (also including my own work and many others), video captioning and very recently visual question answering. My personal favorite RNNs in Computer Vision paper is Recurrent Models of Visual Attention, both due to its high-level direction (sequential processing of images with glances) and the low-level modeling (REINFORCE learning rule that is a special case of policy gradient methods in Reinforcement Learning, which allows one to train models that perform non-differentiable computation (taking glances around the image in this case)). I’m confident that this type of hybrid model that consists of a blend of CNN for raw perception coupled with an RNN glance policy on top will become pervasive in perception, especially for more complex tasks that go beyond classifying some objects in plain view.\nInductive Reasoning, Memories and Attention. Another extremely exciting direction of research is oriented towards addressing the limitations of vanilla recurrent networks. One problem is that RNNs are not inductive: They memorize sequences extremely well, but they don’t necessarily always show convincing signs of generalizing in the correct way (I’ll provide pointers in a bit that make this more concrete). A second issue is they unnecessarily couple their representation size to the amount of computation per step. For instance, if you double the size of the hidden state vector you’d quadruple the amount of FLOPS at each step due to the matrix multiplication. Ideally, we’d like to maintain a huge representation/memory (e.g. containing all of Wikipedia or many intermediate state variables), while maintaining the ability to keep computation per time step fixed.\nThe first convincing example of moving towards these directions was developed in DeepMind’s Neural Turing Machines paper. This paper sketched a path towards models that can perform read/write operations between large, external memory arrays and a smaller set of memory registers (think of these as our working memory) where the computation happens. Crucially, the NTM paper also featured very interesting memory addressing mechanisms that were implemented with a (soft, and fully-differentiable) attention model. The concept of soft attention has turned out to be a powerful modeling feature and was also featured in Neural Machine Translation by Jointly Learning to Align and Translate for Machine Translation and Memory Networks for (toy) Question Answering. In fact, I’d go as far as to say that\nThe concept of attention is the most interesting recent architectural innovation in neural networks.\nNow, I don’t want to dive into too many details but a soft attention scheme for memory addressing is convenient because it keeps the model fully-differentiable, but unfortunately one sacrifices efficiency because everything that can be attended to is attended to (but softly). Think of this as declaring a pointer in C that doesn’t point to a specific address but instead defines an entire distribution over all addresses in the entire memory, and dereferencing the pointer returns a weighted sum of the pointed content (that would be an expensive operation!). This has motivated multiple authors to swap soft attention models for hard attention where one samples a particular chunk of memory to attend to (e.g. a read/write action for some memory cell instead of reading/writing from all cells to some degree). This model is significantly more philosophically appealing, scalable and efficient, but unfortunately it is also non-differentiable. This then calls for use of techniques from the Reinforcement Learning literature (e.g. REINFORCE) where people are perfectly used to the concept of non-differentiable interactions. This is very much ongoing work but these hard attention models have been explored, for example, in Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets, Reinforcement Learning Neural Turing Machines, and Show Attend and Tell.\nPeople. If you’d like to read up on RNNs I recommend theses from Alex Graves, Ilya Sutskever and Tomas Mikolov. For more about REINFORCE and more generally Reinforcement Learning and policy gradient methods (which REINFORCE is a special case of) David Silver’s class, or one of Pieter Abbeel’s classes.\nCode. If you’d like to play with training RNNs I hear good things about keras or passage for Theano, the code released with this post for Torch, or this gist for raw numpy code I wrote a while ago that implements an efficient, batched LSTM forward and backward pass. You can also have a look at my numpy-based NeuralTalk which uses an RNN/LSTM to caption images, or maybe this Caffe implementation by Jeff Donahue.\nConclusion\nWe’ve learned about RNNs, how they work, why they have become a big deal, we’ve trained an RNN character-level language model on several fun datasets, and we’ve seen where RNNs are going. You can confidently expect a large amount of innovation in the space of RNNs, and I believe they will become a pervasive and critical component to intelligent systems.\nLastly, to add some meta to this post, I trained an RNN on the source file of this blog post. Unfortunately, at about 46K characters I haven’t written enough data to properly feed the RNN, but the returned sample (generated with low temperature to get a more typical sample) is:\nI've the RNN with and works, but the computed with program of the\nRNN with and the computed of the RNN with with and the code\nYes, the post was about RNN and how well it works, so clearly this works :). See you next time!\nEDIT (extra links):\nVideos:\n- I gave a talk on this work at the London Deep Learning meetup (video).\nDiscussions:\n- HN discussion\n- Reddit discussion on r/machinelearning\n- Reddit discussion on r/programming\nReplies:\n- Yoav Goldberg compared these RNN results to n-gram maximum likelihood (counting) baseline\n- @nylk trained char-rnn on cooking recipes. They look great!\n- @MrChrisJohnson trained char-rnn on Eminem lyrics and then synthesized a rap song with robotic voice reading it out. Hilarious :)\n- @samim trained char-rnn on Obama Speeches. They look fun!\n- João Felipe trained char-rnn irish folk music and sampled music\n- Bob Sturm also trained char-rnn on music in ABC notation\n- RNN Bible bot by Maximilien\n- Learning Holiness learning the Bible\n- Terminal.com snapshot that has char-rnn set up and ready to go in a browser-based virtual machine (thanks @samim)\n\n\n\nUnderstanding LSTM Networks\nPosted on August 27, 2015\nRecurrent Neural Networks\nHumans don’t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence.\nTraditional neural networks can’t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It’s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones.\nRecurrent neural networks address this issue. They are networks with loops in them, allowing information to persist.\nIn the above diagram, a chunk of neural network, \\(A\\), looks at some input \\(x_t\\) and outputs a value \\(h_t\\). A loop allows information to be passed from one step of the network to the next.\nThese loops make recurrent neural networks seem kind of mysterious. However, if you think a bit more, it turns out that they aren’t all that different than a normal neural network. A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. Consider what happens if we unroll the loop:\nThis chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. They’re the natural architecture of neural network to use for such data.\nAnd they certainly are used! In the last few years, there have been incredible success applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioning… The list goes on. I’ll leave discussion of the amazing feats one can achieve with RNNs to Andrej Karpathy’s excellent blog post, The Unreasonable Effectiveness of Recurrent Neural Networks. But they really are pretty amazing.\nEssential to these successes is the use of “LSTMs,” a very special kind of recurrent neural network which works, for many tasks, much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them. It’s these LSTMs that this essay will explore.\nThe Problem of Long-Term Dependencies\nOne of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, they’d be extremely useful. But can they? It depends.\nSometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in “the clouds are in the sky,” we don’t need any further context – it’s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it’s needed is small, RNNs can learn to use the past information.\nBut there are also cases where we need more context. Consider trying to predict the last word in the text “I grew up in France… I speak fluent French.” Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It’s entirely possible for the gap between the relevant information and the point where it is needed to become very large.\nUnfortunately, as that gap grows, RNNs become unable to learn to connect the information.\nIn theory, RNNs are absolutely capable of handling such “long-term dependencies.” A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don’t seem to be able to learn them. The problem was explored in depth by Hochreiter (1991) [German] and Bengio, et al. (1994), who found some pretty fundamental reasons why it might be difficult.\nThankfully, LSTMs don’t have this problem!\nLSTM Networks\nLong Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber (1997), and were refined and popularized by many people in following work.1 They work tremendously well on a large variety of problems, and are now widely used.\nLSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!\nAll recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.\nLSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.\nDon’t worry about the details of what’s going on. We’ll walk through the LSTM diagram step by step later. For now, let’s just try to get comfortable with the notation we’ll be using.\nIn the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations.\nThe Core Idea Behind LSTMs\nThe key to LSTMs is the cell state, the horizontal line running through the top of the diagram.\nThe cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged.\nThe LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.\nGates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.\nThe sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means “let nothing through,” while a value of one means “let everything through!”\nAn LSTM has three of these gates, to protect and control the cell state.\nStep-by-Step LSTM Walk Through\nThe first step in our LSTM is to decide what information we’re going to throw away from the cell state. This decision is made by a sigmoid layer called the “forget gate layer.” It looks at \\(h_{t-1}\\) and \\(x_t\\), and outputs a number between \\(0\\) and \\(1\\) for each number in the cell state \\(C_{t-1}\\). A \\(1\\) represents “completely keep this” while a \\(0\\) represents “completely get rid of this.”\nLet’s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.\nThe next step is to decide what new information we’re going to store in the cell state. This has two parts. First, a sigmoid layer called the “input gate layer” decides which values we’ll update. Next, a tanh layer creates a vector of new candidate values, \\(\\tilde{C}_t\\), that could be added to the state. In the next step, we’ll combine these two to create an update to the state.\nIn the example of our language model, we’d want to add the gender of the new subject to the cell state, to replace the old one we’re forgetting.\nIt’s now time to update the old cell state, \\(C_{t-1}\\), into the new cell state \\(C_t\\). The previous steps already decided what to do, we just need to actually do it.\nWe multiply the old state by \\(f_t\\), forgetting the things we decided to forget earlier. Then we add \\(i_t*\\tilde{C}_t\\). This is the new candidate values, scaled by how much we decided to update each state value.\nIn the case of the language model, this is where we’d actually drop the information about the old subject’s gender and add the new information, as we decided in the previous steps.\nFinally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through \\(\\tanh\\) (to push the values to be between \\(-1\\) and \\(1\\)) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.\nFor the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that’s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that’s what follows next.\nVariants on Long Short Term Memory\nWhat I’ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but it’s worth mentioning some of them.\nOne popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding “peephole connections.” This means that we let the gate layers look at the cell state.\nThe above diagram adds peepholes to all the gates, but many papers will give some peepholes and not others.\nAnother variation is to use coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together. We only forget when we’re going to input something in its place. We only input new values to the state when we forget something older.\nA slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by Cho, et al. (2014). It combines the forget and input gates into a single “update gate.” It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.\nThese are only a few of the most notable LSTM variants. There are lots of others, like Depth Gated RNNs by Yao, et al. (2015). There’s also some completely different approach to tackling long-term dependencies, like Clockwork RNNs by Koutnik, et al. (2014).\nWhich of these variants is best? Do the differences matter? Greff, et al. (2015) do a nice comparison of popular variants, finding that they’re all about the same. Jozefowicz, et al. (2015) tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks.\nConclusion\nEarlier, I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks!\nWritten down as a set of equations, LSTMs look pretty intimidating. Hopefully, walking through them step by step in this essay has made them a bit more approachable.\nLSTMs were a big step in what we can accomplish with RNNs. It’s natural to wonder: is there another big step? A common opinion among researchers is: “Yes! There is a next step and it’s attention!” The idea is to let every step of an RNN pick information to look at from some larger collection of information. For example, if you are using an RNN to create a caption describing an image, it might pick a part of the image to look at for every word it outputs. In fact, Xu, et al. (2015) do exactly this – it might be a fun starting point if you want to explore attention! There’s been a number of really exciting results using attention, and it seems like a lot more are around the corner…\nAttention isn’t the only exciting thread in RNN research. For example, Grid LSTMs by Kalchbrenner, et al. (2015) seem extremely promising. Work using RNNs in generative models – such as Gregor, et al. (2015), Chung, et al. (2015), or Bayer & Osendorfer (2015) – also seems very interesting. The last few years have been an exciting time for recurrent neural networks, and the coming ones promise to only be more so!\nAcknowledgments\nI’m grateful to a number of people for helping me better understand LSTMs, commenting on the visualizations, and providing feedback on this post.\nI’m very grateful to my colleagues at Google for their helpful feedback, especially Oriol Vinyals, Greg Corrado, Jon Shlens, Luke Vilnis, and Ilya Sutskever. I’m also thankful to many other friends and colleagues for taking the time to help me, including Dario Amodei, and Jacob Steinhardt. I’m especially thankful to Kyunghyun Cho for extremely thoughtful correspondence about my diagrams.\nBefore this post, I practiced explaining LSTMs during two seminar series I taught on neural networks. Thanks to everyone who participated in those for their patience with me, and for their feedback.\nIn addition to the original authors, a lot of people contributed to the modern LSTM. A non-comprehensive list is: Felix Gers, Fred Cummins, Santiago Fernandez, Justin Bayer, Daan Wierstra, Julian Togelius, Faustino Gomez, Matteo Gagliolo, and Alex Graves.↩\n\n\n\narXiv:1409.2329v5  [cs.NE]  19 Feb 2015Underreviewasaconferencepaperat ICLR2015\nRECURRENT NEURALNETWORK REGULARIZATION\nWojciechZaremba∗\nNewYorkUniversity\nwoj.zaremba@gmail.com\nIlyaSutskever,OriolVinyals\nGoogleBrain\n{ilyasu,vinyals }@google.com\nABSTRACT\nWe present a simple regularization technique for Recurrent Neural Networks\n(RNNs) with Long Short-Term Memory (LSTM) units. Dropout, t he most suc-\ncessfultechniqueforregularizingneuralnetworks,doesn otworkwellwithRNNs\nand LSTMs. In this paper, we show how to correctly apply dropo ut to LSTMs,\nandshowthatitsubstantiallyreducesoverﬁttingonavarie tyoftasks. Thesetasks\ninclude language modeling, speech recognition, image capt ion generation, and\nmachinetranslation.\n∗\n1 INTRODUCTION\nThe Recurrent Neural Network (RNN) is neural sequence model that achieves state of the art per-\nformance on important tasks that include language modeling Mikolov (2012), speech recognition\nGraveset al. (2013), and machine translation Kalchbrenner &Blunsom (2013). It is known that\nsuccessful applications of neural networks require good re gularization. Unfortunately, dropout\nSrivastava (2013), the most powerful regularization metho d for feedforwardneural networks, does\nnot work well with RNNs. As a result, practical applications of RNNs often use models that are\ntoo small because largeRNNs tend to overﬁt. Existingregula rizationmethodsgiverelativelysmall\nimprovementsfor RNNs Graves (2013). In this work, we show th at dropout, when correctly used,\ngreatlyreducesoverﬁttinginLSTMs,andevaluateit onthre edifferentproblems.\nThecodeforthisworkcanbefoundin https://github.com/wojzaremba/lstm .\n2 RELATED WORK\nDropout Srivastava (2013) is a recently introduced regular ization method that has been very suc-\ncessfulwithfeed-forwardneuralnetworks. Whilemuchwork hasextendeddropoutinvariousways\nWang& Manning (2013); Wan et al. (2013), there has been relat ively little research in applying it\nto RNNs. The only paper on this topic is by Bayeret al. (2013), who focuses on “marginalized\ndropout” Wang&Manning (2013), a noiseless deterministic a pproximation to standard dropout.\nBayeret al. (2013) claim that conventionaldropoutdoes not work well with RNNs because the re-\ncurrenceampliﬁes noise, which in turn hurts learning. In th is work, we show that this problemcan\nbe ﬁxed by applying dropoutto a certain subset of the RNNs’ co nnections. As a result, RNNs can\nnowalsobeneﬁtfromdropout.\nIndependentlyofourwork,Phamet al.(2013)developedthev erysameRNNregularizationmethod\nand applied it to handwriting recognition. We rediscovered this method and demonstrated strong\nempirical results over a wide range of problems. Other work t hat applied dropout to LSTMs is\nPachitariu& Sahani(2013).\n∗Workdone while the author was inGoogle Brain.\n1Underreviewasaconferencepaperat ICLR2015\nTherehavebeenanumberofarchitecturalvariantsoftheRNN thatperformbetteronproblemswith\nlong term dependenciesHochreiter&Schmidhuber(1997); Gr aveset al. (2009); Cho etal. (2014);\nJaegeretal. (2007); Koutn´ ıketal. (2014); Sundermeyeret al. (2012). In this work, we show how\nto correctly apply dropoutto LSTMs, the most commonly-used RNN variant; this way of applying\ndropoutislikelyto workwell withotherRNN architecturesa swell.\nIn this paper, we consider the following tasks: language mod eling, speech recognition, and ma-\nchine translation. Language modeling is the ﬁrst task where RNNs have achieved substantial suc-\ncess Mikolovet al. (2010; 2011); Pascanuetal. (2013). RNNs have also been successfully used\nfor speech recognition Robinsonet al. (1996); Graveset al. (2013) and have recently been applied\nto machine translation, where they are used for language mod eling, re-ranking, or phrase model-\ning Devlinet al. (2014); Kalchbrenner& Blunsom (2013); Cho et al. (2014); Chow etal. (1987);\nMikolovetal. (2013).\n3 REGULARIZING RNNS WITHLSTM CELLS\nIn this section we describe the deep LSTM (Section 3.1). Next , we show how to regularize them\n(Section3.2),andexplainwhyourregularizationschemewo rks.\nWe let subscriptsdenotetimestepsandsuperscriptsdenote layers. All ourstates are n-dimensional.\nLethl\nt∈Rnbe a hiddenstate in layer lin timestep t. Moreover,let Tn,m:Rn→Rmbe an afﬁne\ntransform( Wx+bforsomeWandb). Let⊙beelement-wisemultiplicationandlet h0\ntbeaninput\nword vector at timestep k. We use the activations hL\ntto predict yt, sinceLis the numberof layers\ninourdeepLSTM.\n3.1 L ONG-SHORT TERM MEMORY UNITS\nTheRNNdynamicscanbedescribedusingdeterministictrans itionsfromprevioustocurrenthidden\nstates. Thedeterministicstate transitionis afunction\nRNN:hl−1\nt,hl\nt−1→hl\nt\nForclassical RNNs, thisfunctionisgivenby\nhl\nt=f(Tn,nhl−1\nt+Tn,nhl\nt−1), wheref∈ {sigm,tanh}\nTheLSTMhascomplicateddynamicsthatallowittoeasily“me morize”informationforanextended\nnumber of timesteps. The “long term” memory is stored in a vec tor ofmemory cells cl\nt∈Rn. Al-\nthoughmanyLSTMarchitecturesthatdifferintheirconnect ivitystructureandactivationfunctions,\nall LSTM architectureshaveexplicitmemorycells forstori nginformationforlongperiodsof time.\nTheLSTMcandecidetooverwritethememorycell,retrieveit ,orkeepitforthenexttimestep. The\nLSTMarchitectureusedinourexperimentsisgivenbythefol lowingequationsGraveset al.(2013):\nLSTM:hl−1\nt,hl\nt−1,cl\nt−1→hl\nt,cl\nt\ni\nf\no\ng\n=\nsigm\nsigm\nsigm\ntanh\nT2n,4n/parenleftbigg\nhl−1\nt\nhl\nt−1/parenrightbigg\ncl\nt=f⊙cl\nt−1+i⊙g\nhl\nt=o⊙tanh(cl\nt)\nIn these equations, sigmandtanhare applied element-wise. Figure 1 illustrates the LSTM equ a-\ntions.\n3.2 R EGULARIZATION WITH DROPOUT\nThemaincontributionofthispaperisarecipeforapplyingd ropouttoLSTMsinawaythatsuccess-\nfully reduces overﬁtting. The main idea is to apply the dropo ut operator only to the non-recurrent\n2Underreviewasaconferencepaperat ICLR2015\n✍✌✎☞\nctCell\n❢×\n✍✌✎☞\nfForget gate✻✣✠\n✁✁ ✕hl\nt−1❆❆ ❑hl−1\nt✍✌✎☞\niInput\ngate❆ ❯hl\nt−1\n✁✁ ☛hl−1\nt\n✍✌✎☞\noOutput\ngate❆ ❯hl\nt−1\n✁✁ ☛hl−1\nt\n✍✌✎☞\ng\nInput\nmodulation\ngate❢×✲✲❏\n❏❏ ❫❢×✲ ✲❄hl\nthl\nt−1hl−1\nt\n✘✘ ✿❳❳ ③\nFigure 1: A graphical representation of LSTM memory cells us ed in this paper (there are minor\ndifferencesin comparisontoGraves(2013)).\n✲✲\n✲✲\n✲✲\n✲✲\n✲✲\n✲✲\n✻✻✻\n✻✻✻\n✻✻✻\n✻✻✻\n✻✻✻\nxt−2xt−1xtxt+1xt+2yt−2yt−1ytyt+1yt+2\nFigure 2: Regularized multilayer RNN. The dashed arrows ind icate connections where dropout is\napplied,andthesolidlinesindicateconnectionswheredro poutisnotapplied.\nconnections(Figure2). Thefollowingequationdescribesi t moreprecisely,where Disthe dropout\noperatorthat setsa randomsubsetofitsargumentto zero:\n\ni\nf\no\ng\n=\nsigm\nsigm\nsigm\ntanh\nT2n,4n/parenleftbigg\nD(hl−1\nt)\nhl\nt−1/parenrightbigg\ncl\nt=f⊙cl\nt−1+i⊙g\nhl\nt=o⊙tanh(cl\nt)\nOur method works as follows. The dropout operator corrupts t he information carried by the units,\nforcingthemtoperformtheirintermediatecomputationsmo rerobustly. Atthesametime,wedonot\nwant to erase all the information from the units. It is especi ally important that the units remember\nevents that occurred many timesteps in the past. Figure 3 sho ws how information could ﬂow from\naneventthatoccurredat timestep t−2to thepredictionintimestep t+2inourimplementationof\ndropout. We can see that the informationis corruptedby the d ropoutoperatorexactly L+1times,\n3Underreviewasaconferencepaperat ICLR2015\n✲✲\n✲✲\n✲✲\n✲✲\n✲✲\n✲✲\n✻✻✻\n✻✻✻\n✻✻✻\n✻✻✻\n✻✻✻\nxt−2xt−1xtxt+1xt+2yt−2yt−1ytyt+1yt+2\nFigure 3: The thick line shows a typical path of informationﬂ ow in the LSTM. The informationis\naffectedbydropout L+1times,where Lisdepthofnetwork.\nthe meaning of life is that only if an end would be of the whole supplier. widespread rules are re-\ngarded as the companies of refuses to deliver. in balance of t he nation ’s information and loan\ngrowth associated with the carrier thrifts are in the proces s of slowing the seed and commercial paper.\nthe meaning of life is nearly in the ﬁrst several months before the government was a ddressing such a move as\npresident and chief executive of the nation past from a natio nal commitment to curb grounds. meanwhile the\ngovernment invests overcapacity thatcriticism andinthe o uter reversal of small-townamerica.\nFigure 4: Some interesting samples drawn from a large regula rized model conditioned on “The\nmeaningoflifeis”. We haveremoved“unk”,“N”,“$”fromthes et ofpermissiblewords.\nand this number is independent of the number of timesteps tra versed by the information. Standard\ndropout perturbsthe recurrent connections, which makes it difﬁcult for the LSTM to learn to store\ninformationforlongperiodsoftime. Bynotusingdropouton therecurrentconnections,the LSTM\ncanbeneﬁtfromdropoutregularizationwithoutsacriﬁcing itsvaluablememorizationability.\n4 EXPERIMENTS\nWe present results in three domains: languagemodeling(Sec tion 4.1), speech recognition(Section\n4.2),machinetranslation(Section4.3), andimagecaption generation(Section4.4).\n4.1 L ANGUAGE MODELING\nWeconductedword-levelpredictionexperimentsonthePenn TreeBank(PTB)datasetMarcusetal.\n(1993),whichconsistsof 929ktrainingwords, 73kvalidationwords,and 82ktestwords. Ithas 10k\nwordsinitsvocabulary. WedownloadeditfromTomasMikolov ’swebpage†. Wetrainedregularized\nLSTMs of two sizes; these are denoted the medium LSTM and larg e LSTM. Both LSTMs have\ntwo layers and are unrolled for 35steps. We initialize the hidden states to zero. We then use th e\nﬁnal hidden states of the current minibatch as the initial hi dden state of the subsequent minibatch\n(successiveminibatchessequentiallytraversethetraini ngset). Thesizeofeachminibatchis20.\n†http://www.fit.vutbr.cz/ ˜imikolov/rnnlm/simple-examples.tgz\n4Underreviewasaconferencepaperat ICLR2015\nModel Validationset Testset\nAsingle model\nPascanuet al.(2013) 107.5\nCheng etal. 100.0\nnon-regularized LSTM 120.7 114.5\nMedium regularizedLSTM 86.2 82.7\nLargeregularized LSTM 82.2 78.4\nModel averaging\nMikolov (2012) 83.5\nCheng etal. 80.6\n2non-regularized LSTMs 100.4 96.1\n5non-regularized LSTMs 87.9 84.1\n10non-regularized LSTMs 83.5 80.0\n2medium regularizedLSTMs 80.6 77.0\n5medium regularizedLSTMs 76.7 73.3\n10medium regularized LSTMs 75.2 72.0\n2large regularizedLSTMs 76.9 73.6\n10large regularized LSTMs 72.8 69.5\n38large regularized LSTMs 71.9 68.7\nModel averagingwithdynamic RNNsand n-gram models\nMikolov & Zweig(2012) 72.9\nTable1: Word-levelperplexityonthePennTreeBankdataset .\nThe medium LSTM has 650units per layer and its parameters are initialized uniforml y in\n[−0.05,0.05]. As described earlier, we apply 50%dropout on the non-recurrent connections. We\ntraintheLSTMfor 39epochswithalearningrateof 1,andafter 6epochswedecreaseitbyafactor\nof1.2after each epoch. We clip the norm of the gradients (normaliz ed by minibatch size) at 5.\nTrainingthisnetworktakesabouthalfa dayonanNVIDIAK20G PU.\nThe large LSTM has 1500units per layer and its parameters are initialized uniforml y in\n[−0.04,0.04]. We apply 65%dropout on the non-recurrent connections. We train the mode l for\n55epochs with a learning rate of 1; after14epochs we start to reduce the learning rate by a factor\nof1.15after each epoch. We clip the norm of the gradients (normaliz ed by minibatch size) at 10\nMikolovetal. (2010). Trainingthisnetworktakesanentire dayonanNVIDIAK20GPU.\nFor comparison,we traineda non-regularizednetwork. We op timizedits parametersto get the best\nvalidation performance. The lack of regularization effect ively constrains size of the network, forc-\ning us to use small network because larger networks overﬁt. O ur best performing non-regularized\nLSTM has two hidden layers with 200units per layer, and its weights are initialized uniformly i n\n[−0.1,0.1]. We train it for 4epochswitha learningrateof 1and thenwe decreasethelearningrate\nbyafactorof 2aftereachepoch,foratotal of 13trainingepochs. Thesize ofeachminibatchis 20,\nand we unroll the network for 20steps. Training this network takes 2-3 hours on an NVIDIA K20\nGPU.\nTable1comparespreviousresultswithourLSTMs,andFigure 4showssamplesdrawnfromasingle\nlargeregularizedLSTM.\n4.2 SPEECH RECOGNITION\nDeep Neural Networks have been used for acoustic modeling fo r over half a century (see\nBourlard&Morgan (1993) for a good review). Acoustic modeli ng is a key component in map-\nping acoustic signals to sequences of words, as it models p(st|X)wherestis the phonetic state at\ntimetandXistheacousticobservation. RecentworkhasshownthatLSTM scanachieveexcellent\nperformance on acoustic modeling Saket al. (2014), yet rela tively small LSTMs (in terms of the\nnumber of their parameters) can easily overﬁt the training s et. A useful metric for measuring the\nperformance of acoustic models is frame accuracy, which is m easured at each stfor all timesteps\nt. Generally, this metric correlates with the actual metric o f interest, the Word Error Rate (WER).\n5Underreviewasaconferencepaperat ICLR2015\nModel Trainingset Validationset\nNon-regularized LSTM 71.6 68.9\nRegularizedLSTM 69.4 70.5\nTable2: Frame-levelaccuracyontheIcelandicSpeechDatas et. Thetrainingsethas93kutterances.\nModel Test perplexity TestBLEUscore\nNon-regularized LSTM 5.8 25.9\nRegularized LSTM 5.0 29.03\nLIUMsystem 33.30\nTable3: ResultsontheEnglishtoFrenchtranslationtask.\nSincecomputingtheWERinvolvesusingalanguagemodelandt uningthedecodingparametersfor\nevery change in the acoustic model, we decided to focus on fra me accuracy in these experiments.\nTable2showsthatdropoutimprovestheframeaccuracyofthe LSTM.Notsurprisingly,thetraining\nframe accuracy drops due to the noise added during training, but as is often the case with dropout,\nthisyieldsmodelsthatgeneralizebettertounseendata. No tethatthetest setiseasierthanthetrain-\ning set, as its accuracy is higher. We report the performance of an LSTM on an internal Google\nIcelandicSpeechdataset,whichisrelativelysmall(93kut terances),sooverﬁttingisagreatconcern.\n4.3 M ACHINE TRANSLATION\nWeformulateamachinetranslationproblemasalanguagemod ellingtask,whereanLSTMistrained\ntoassignhighprobabilitytoacorrecttranslationofasour cesentence. Thus,theLSTMistrainedon\nconcatenationsofsourcesentencesandtheir translations Sutskeveretal. (2014) (see alsoCho etal.\n(2014)). We compute a translation by approximatingthe most probable sequence of words using a\nsimple beam search with a beam of size 12. We ran an LSTM on the W MT’14 English to French\ndataset, on the “selected” subset from Schwenk (2014) which has 340M French words and 304M\nEnglish words. Our LSTM has 4 hidden layers, and both its laye rs and word embeddings have\n1000units. ItsEnglish vocabularyhas160,000wordsandits French vocabularyhas80,000words.\nThe optimal dropout probability was 0.2. Table 3 shows the pe rformance of an LSTM trained\nwith and without dropout. While our LSTM does not beat the phr ase-based LIUM SMT system\nSchwenket al. (2011), our results show that dropout improve s the translation performance of the\nLSTM.\n4.4 IMAGECAPTIONGENERATION\nWe applied the dropoutvariantto the image captiongenerati onmodel of Vinyalsetal. (2014). The\nimage caption generation is similar to the sequence-to-seq uence model of Sutskeveret al. (2014),\nbutwheretheinputimageismappedontoa vectorwitha highly -accuratepre-trainedconvolutional\nneural network (Szegedyet al., 2014), which is convertedin to a caption with a single-layer LSTM\n(see Vinyalsetal. (2014) forthe detailsonthe architectur e). We test ourdropoutschemeon LSTM\nastheconvolutionalneuralnetworkisnottrainedontheima gecaptiondatasetbecauseitisnotlarge\n(MSCOCO (Linetal., 2014)).\nOur results are summarized in the following Table 4. In brief , dropout helps relative to not using\ndropout, but using an ensemble eliminates the gains attaine d by dropout. Thus, in this setting, the\nmain effect of dropout is to produce a single model that is as g ood as an ensemble, which is a\nreasonableimprovementgiventhesimplicityofthetechniq ue.\n5 CONCLUSION\nWe presented a simple way of applying dropout to LSTMs that re sults in large performance in-\ncreases on several problems in different domains. Our work m akes dropout useful for RNNs, and\nourresultssuggestthatourimplementationofdropoutcoul dimproveperformanceonawidevariety\nofapplications.\n6Underreviewasaconferencepaperat ICLR2015\nModel Testperplexity TestBLEUscore\nNon-regularized model 8.47 23.5\nRegularizedmodel 7.99 24.3\n10non-regularized models 7.5 24.4\nTable4: Resultsontheimagecaptiongenerationtask.\n6 ACKNOWLEDGMENTS\nWe wish toacknowledgeTomasMikolovforusefulcommentsont heﬁrst versionofthepaper.\nREFERENCES\nBayer, Justin, Osendorfer, Christian, Chen, Nutan, Urban, Sebastian, and van der Smagt, Patrick. On fast\ndropout and itsapplicabilitytorecurrent networks. arXiv preprint arXiv:1311.0701 , 2013.\nBourlard, H. and Morgan, N. Connectionist Speech Recognition: A Hybrid Approach . Kluwer Academic\nPublishers, 1993.\nCheng, Wei-Chen, Kok, Stanley, Pham, Hoai Vu, Chieu, Hai Leo ng, and Chai, Kian Ming A. Language\nmodeling withsum-product networks.\nCho, Kyunghyun, van Merrienboer, Bart, Gulcehre, Caglar, B ougares, Fethi, Schwenk, Holger, and Bengio,\nYoshua. Learningphraserepresentationsusingrnnencoder -decoderforstatisticalmachinetranslation. arXiv\npreprint arXiv:1406.1078 , 2014.\nChow, Y, Dunham, M, Kimball, O, Krasner, M, Kubala, G, Makhou l, J, Price, P, Roucos, S, and Schwartz, R.\nByblos: Thebbncontinuous speechrecognition system. In Acoustics,Speech, andSignal Processing, IEEE\nInternational Conference on ICASSP'87. ,volume 12, pp. 89–92. IEEE,1987.\nDevlin, J., Zbib, R., Huang, Z.,Lamar, T., Schwartz, R., and Makhoul, J. Fast and robust neural network joint\nmodels forstatisticalmachine translation. In ACL,2014.\nGraves,Alex. Generating sequences withrecurrent neural n etworks. arXiv preprint arXiv:1308.0850 , 2013.\nGraves, Alex, Liwicki, Marcus, Fern´ andez, Santiago, Bert olami, Roman, Bunke, Horst, and Schmidhuber,\nJ¨ urgen. A novel connectionist system for unconstrained ha ndwriting recognition. Pattern Analysis and\nMachine Intelligence, IEEETransactions on , 31(5):855–868, 2009.\nGraves, Alex, Mohamed, Abdel-rahman, and Hinton, Geoffrey . Speech recognition withdeep recurrent neural\nnetworks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEE E International Conference on ,\npp. 6645–6649. IEEE,2013.\nHochreiter, Sepp and Schmidhuber, J¨ urgen. Long short-ter m memory. Neural computation , 9(8):1735–1780,\n1997.\nJaeger, Herbert, Lukoˇ seviˇ cius, Mantas, Popovici, Dan, a nd Siewert, Udo. Optimization and applications of\necho statenetworks withleaky-integrator neurons. Neural Networks ,20(3):335–352, 2007.\nKalchbrenner, N.and Blunsom, P. Recurrent continuous tran slationmodels. In EMNLP,2013.\nKoutn´ ık, Jan, Greff, Klaus, Gomez, Faustino, and Schmidhu ber, J¨ urgen. A clockwork rnn. arXiv preprint\narXiv:1402.3511 , 2014.\nLin, Tsung-Yi, Maire, Michael, Belongie, Serge, Hays, Jame s, Perona, Pietro, Ramanan, Deva, Doll´ ar, Piotr,\nand Zitnick, C Lawrence. Microsoft coco: Common objects in c ontext.arXiv preprint arXiv:1405.0312 ,\n2014.\nMarcus, Mitchell P, Marcinkiewicz, Mary Ann, and Santorini , Beatrice. Building a large annotated corpus of\nenglish: The penn treebank. Computational linguistics ,19(2):313–330, 1993.\nMikolov, Tom´ aˇ s. Statistical language models based on neural networks . PhD thesis, Ph. D. thesis, Brno\nUniversityof Technology, 2012.\nMikolov, Tomas and Zweig, Geoffrey. Context dependent recu rrent neural network language model. In SLT,\npp. 234–239, 2012.\n7Underreviewasaconferencepaperat ICLR2015\nMikolov, Tomas, Karaﬁ´ at, Martin, Burget, Lukas, Cernock` y, Jan, and Khudanpur, Sanjeev. Recurrent neural\nnetwork based language model. In INTERSPEECH ,pp. 1045–1048, 2010.\nMikolov,Tomas,Deoras,Anoop,Povey,Daniel,Burget,Luka s,andCernocky,Jan. Strategiesfortraininglarge\nscale neural network language models. In Automatic Speech Recognition and Understanding (ASRU),20 11\nIEEEWorkshop on , pp. 196–201. IEEE,2011.\nMikolov, Tomas, Le, Quoc V, and Sutskever, Ilya. Exploiting similarities among languages for machine trans-\nlation.arXiv preprint arXiv:1309.4168 , 2013.\nPachitariu, Marius and Sahani, Maneesh. Regularization an d nonlinearities for neural language models: when\nare theyneeded? arXiv preprint arXiv:1301.5650 , 2013.\nPascanu, Razvan, Gulcehre, Caglar, Cho, Kyunghyun, and Ben gio, Yoshua. How to construct deep recurrent\nneural networks. arXiv preprint arXiv:1312.6026 , 2013.\nPham, Vu, Kermorvant, Christopher, and Louradour, J´ erˆ om e. Dropout improves recurrent neural networks for\nhandwriting recognition. arXivpreprint arXiv:1312.4569 , 2013.\nRobinson,Tony,Hochberg,Mike,andRenals,Steve. Theuseo frecurrentneuralnetworksincontinuousspeech\nrecognition. In Automaticspeech and speaker recognition , pp. 233–258. Springer, 1996.\nSak,H.,Vinyals,O.,Heigold,G.,Senior,A.,McDermott,E. ,Monga,R.,andMao,M. Sequencediscriminative\ndistributedtrainingof longshort-term memory recurrent n eural networks. In Interspeech , 2014.\nSchwenk, Holger. Universityle mans, 2014.\nhttp://www-lium.univ-lemans.fr/ ˜schwenk/cslm_joint/paper .\nSchwenk, Holger, Lambert, Patrik, Barrault, Lo¨ ıc, Servan , Christophe, Aﬂi, Haithem, Abdul-Rauf, Sadaf, and\nShah, Kashif. Lium’ssmtmachine translationsystems for wm t2011. In Proceedings ofthe SixthWorkshop\nonStatistical Machine Translation , pp. 464–469. Associationfor Computational Linguistics, 2011.\nSrivastava, Nitish. Improving neural networks withdropout . PhD thesis,Universityof Toronto, 2013.\nSundermeyer, Martin, Schl¨ uter, Ralf, and Ney, Hermann. Ls tm neural networks for language modeling. In\nINTERSPEECH ,2012.\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV. Sequence t o sequence learning with neural networks. In\nAdvances inNeural Information ProcessingSystems , pp. 3104–3112, 2014.\nSzegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pie rre, Reed, Scott, Anguelov, Dragomir, Erhan, Du-\nmitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going d eeper with convolutions. arXiv preprint\narXiv:1409.4842 , 2014.\nVinyals,Oriol, Toshev, Alexander, Bengio, Samy, and Erhan , Dumitru. Show and tell: A neural image caption\ngenerator. arXivpreprint arXiv:1411.4555 , 2014.\nWan, Li, Zeiler, Matthew, Zhang, Sixin, Cun, Yann L, and Ferg us, Rob. Regularization of neural networks\nusing dropconnect. In Proceedings of the 30th International Conference on Machin e Learning (ICML-13) ,\npp. 1058–1066, 2013.\nWang,SidaandManning, Christopher. Fastdropout training . InProceedings ofthe30thInternational Confer-\nence onMachine Learning (ICML-13) ,pp. 118–126, 2013.\n8\n\n\n\nKeeping Neural Net w orks Simple b y Minimizingthe Description Length of the W eigh tsGeo/#0Brey E/. Hin ton and Drew v an CampDepartmen t of Computer ScienceUniv ersit yo f T oron to/1/0 King/'s College RoadT oron to M/5S /1A/4/, CanadaAbstractSup ervised neural net w orks generalize w ell ifthere is m uc h less information in the w eigh tsthan there is in the output v ectors of the train/-ing cases/. So during learning/, it is imp or/-tan tt ok eep the w eigh ts simple b y p enaliz/-ing the amoun t of information they con tain/.The amoun t of information in a w eigh t canb e con trolled b y adding Gaussian noise andthe noise lev el can b e adapted during learningto optimize the trade/-o/#0B b et w een the exp ectedsquared error of the net w ork and the amoun tof information in the w eigh ts/. W e describ ea metho d of computing the deriv ativ es of theexp ected squared error and of the amoun to finformation in the noisy w eigh ts in a net/-w ork that con tains a la y er of non/-linear hiddenunits/. Pro vided the output units are linear/, theexact deriv ativ es can b e computed e/#0Ecien tlywithout time/-consuming Mon te Carlo sim ula/-tions/. The idea of minimi zing the amoun to finformation that is required to comm unicatethe w eigh ts of a neural net w ork leads to an um be r o f i n teresting sc hemes for enco ding thew eigh ts/./1 In tro ductionIn man y practical learning tasks there is little a v ailabletraining data so an y reasonably complicated mo del willtend to o v er/#0Ct the data and giv e p o or generalization tonew data/. T oa v oid o v er/#0Ctting w e need to ensure thatthere is less information in the w eigh ts than there is inthe output v ectors of the training cases/. Researc hersha v e considered man y p ossible w a ys of limiti ng the in/-formation in the w eigh ts/:\n/#0F Limit the n um b er of connections in the net w ork/#28and hop e that eac hw eigh t do es not ha v et o o m uc hinformation in it/#29/./#0F Divide the connections in to subsets/, and force thew eigh ts within a subset to b e iden tical/. If this/#5Cw eigh t/-sharing/\" is based on an analysis of the nat/-ural symmetries of the task it can b e v ery e/#0Bectiv e/#28Lang/, W aib el and Hin ton /#28/1/9/9/0/#29/; LeCun /1/9/8/9/#29/./#0F Quan tize all the w eigh ts in the net w ork so that aprobabilit y mass/, p /, can b e assigned to eac h quan/-tized v alue/. The n um b er of bits in a w eigh t is then/, log p /, pro vided w e ignore the cost of de/#0Cning thequan tization/. Unfortunately this metho d leads toa di/#0Ecult searc h space b ecause the cost of a w eigh tdo es not ha v e a smo oth deriv ativ e/./2 Applying the Minim um DescriptionLength PrincipleWhen /#0Ctting mo dels to data/, it is alw a ys p ossible to /#0Ctthe training data b etter b y using a more complex mo del/,but this ma y mak e the mo del w orse at /#0Ctting new data/.So w e need some w a y of deciding when extra complex/-it y in the mo del is not w orth the impro v emen t in thedata/-/#0Ct/. The Minim um Description Length Principle/#28Rissanen/, /1/9/8/6/#29 asserts that the b est mo del of some\ndata is the one that minim izes the com bined cost ofdescribing the mo del and describing the mis/#0Ct b et w eenthe mo del and the data/. F or sup ervised neural net w orkswith a predetermined arc hitecture/, the mo del cost is then um b er of bits it tak es to describ e the w eigh ts/, and thedata/-mis/#0Ct cost is the n um b er of bits it tak es to describ ethe discrepancy b et w een the correct output and the out/-put of the neural net w ork on eac h training case/. W e canthink in terms of a sender who can see b oth the input\nv ector and the correct output and a receiv er who canonly see the input v ector/. The sender /#0Crst /#0Cts a neuralnet w ork/, of pre/-arranged arc hitecture/, to the completeset of training cases/, then sends the w eigh ts to the re/-ceiv er/. F or eac h training case the sender also sends thediscrepancy b et w een the net/'s output and the correctoutput/. By adding this discrepancy to the output ofthe net/, the receiv er can generate exactly the correctoutput/.t\n0vFigure /1/: This sho ws the probabilit y mass asso/-ciated with a quan tized v alue/, v /, using a quan/-tization width t /.I f t is m uc h narro w er than theGaussian distribution/, the probabilit y mass is w ellappro ximated b y the pro duct of the heigh t and thewidth/, so the log probabilit y is a sum of t w o terms/.The log t term is a constan t/, and if the distribu/-tion is a zero/-mean Gaussian/, the log of the heigh tis prop ortional to v\n/2/./3 Co ding the data mis/#0CtsT o apply the MDL principle w e need to decide on aco ding sc heme for the data mis/#0Cts and for the w eigh ts/.Clearly /, if the data mis/#0Cts are real n um b ers/, an in/#0Cniteamoun t of information is needed to con v ey them/. So w eshall assume that they are v ery /#0Cnely quan tized/, usingin terv als of /#0Cxed width t /.W e shall also assume thatthe data mis/#0Cts are enco ded separately for eac h of theoutput units/.The co ding theorem tells us that if a sender and a re/-ceiv er ha v e agreed on a probabilit y distribution that as/-signs a probabilit y mass/, p /#28/#01 y /#29/, to eac h p ossible quan/-tized data mis/#0Ct/, /#01 y /, then w e can co de the mis/#0Ct using/, log/2\np /#28/#01 y /#29 bits/. If w ew an t to minim ize the exp ectedn um b er of bits/, the b est probabilit y distribution to useis the correct one/, but an y other agreed distribution canalso b e used/. F or con v enience/, w e shall assume that foroutput unit j the data mis/#0Cts are enco ded b y assumingthat they are dra wn from a zero/-mean Gaussian distri/-bution with standard deviation/, /#1Bj\n/. Pro vided that /#1Bj\nislarge compared with the quan tization width t /, the as/-sumed probabilit y of a particular data mis/#0Ct b et w eenthe desired output/, d\ncj\non training case c and the actualoutput y\ncj\nis then w ell appro ximated b y the probabilit ymass sho wn in /#0Cgure /1/.p /#28 d\ncj\n/, y\ncj\n/#29/= t\n/1p/2 /#19/#1Bj\nexp\n/\"/, /#28 d\ncj\n/, y\ncj\n/#29\n/2/2 /#1B\n/2j\n/#23/#28/1/#29Using an optimal co de/, the description length of a datamis/#0Ct/, d\ncj\n/, y\ncj\n/, in units of log/2\n/#28 e /#29 bits /#28called /#5Cnats/\"/#29 is/:/, log p /#28 d\ncj\n/, y\ncj\n/#29/= /, log t /+ log\np/2 /#19 /+ log /#1Bj\n/+\n/#28 d\ncj\n/, y\ncj\n/#29\n/2/2 /#1B\n/2j/#28/2/#29T o minimi ze this description length summed o v er all Ntraining cases/, the optimal v alue of /#1Bj\nis the ro ot mean\nsquare deviation of the mis/#0Cts from zero/.\n/1Using thisv alue of /#1Bj\nand summing o v er all training cases the lastterm of equation /2 b ecomes a constan t and the datamis/#0Ct cost is/:Cdata/-mis/#0Ct\n/= kN /+\nN/2\nlog\n/\"/1N\nXc\n/#28 d\ncj\n/, y\ncj\n/#29\n/2\n/#23/#28/3/#29where k is a constan t that dep ends only on t /.Indep enden tly of whether w e use the optimal v alue ora predetermined v alue for /#1Bj\n/, it is apparen t that thedescription length is minim ized b y minim izing the usualsquared error function/, so the Gaussian assumptions w eha v e made ab out co ding can b e view ed as the MDLjusti/#0Ccation of this error function/./4 A simple metho d of co ding thew eigh tsW e could co de the w eigh ts in just the same w a ya s w eco de the data mis/#0Cts/. W e assume that the w eigh ts ofthe trained net w ork are /#0Cnely quan tized and come froma zero/-mean Gaussian distribution/. If the standard de/-viation/, /#1Bw\n/, of this distribution is /#0Cxed in adv ance/, thedescription length of the w eigh ts is simply prop ortionalto the sum of their squares/. So/, assuming w e use a Gaus/-sian with standard deviation /#1Bj\nfor enco ding the outputerrors/, w e can minimi ze the total description length ofthe data mis/#0Cts and the w eigh ts b y minimi zing the sumof t w o terms/:C /=\nXj\n/1/2 /#1B\n/2j\nXc\n/#28 d\ncj\n/, y\ncj\n/#29\n/2/+\n/1/2 /#1B\n/2w\nXij\nw\n/2ij\n/#28/4/#29where c is an index o v er training cases/.This is just the standard /#5Cw eigh t/-deca y/\" metho d/. Thefact that w eigh t/-deca y impro v es generalization /#28Hin ton/,/1/9/8/7/#29 can therefore b e view ed as a vindication of thiscrude MDL approac h in whic h the standard deviationsof the gaussians used for co ding the data mis/#0Cts and thew eigh ts are b oth /#0Cxed in adv ance/.\n/2An elab oration of standard w eigh t/-deca y is to assumethat the distribution of w eigh ts in the trained net w orkcan b e mo delled more accurately b y using a mixtureof sev eral Gaussians whose means/, v ariances and mix/-ing prop ortions are adapted as the net w ork is trained/1If the optimal v alue of /#1Bj\nis to b e used/, it m ust b e com/-m unicated b efore the data mis/#0Cts are sen t/, so it to o m ust b eco ded/. Ho w ev er/, since it is only one n um be r w e are probablysafe in ignoring this asp ect of the total description length/./2It is clear from equation /4 that it is only the ratio ofthe v ariances of the t w o Gaussians that matters/. Ratherthan guessing this ratio/, it is usually b etter to estimate it b yseeing whic h ratio giv es optimal p erformance on a v alidationset/./#28No wlan and Hin ton/, /1/9/9/2/#29/. F or some tasks this moreelab orate w a y of co ding the w eigh ts giv es considerablyb etter generalization/. This is esp ecially true when onlya small n um b er of di/#0Beren tw eigh tv alues are required/.Ho w ev er/, this more elab orate sc heme still su/#0Bers froma serious w eakness/: It assumes that all the w eigh ts arequan tized to the same tolerance/, and that this toleranceis small compared with the standard deviations of the\nGaussians used for mo delling the w eigh t distribution/.Th us it tak es in to accoun t the probabilit y densit yo f aw eigh t /#28the heigh t in /#0Cgure /1/#29 but it ignores the pre/-cision /#28the width/#29/. This is a terrible w aste of bits/. Anet w ork is clearly m uc h more economical to describ e ifsome of the w eigh tv alues can b e describ ed v ery impre/-cisely without signi/#0Ccan tly a/#0Becting the predictions ofthe net w ork/.MacKa y /#28/1/9/9/2/#29 has considered the e/#0Bects of smallc hanges in the w eigh ts on the outputs of the net w orkafter the net w ork has b een trained/. The next sectiondescrib es a metho d of taking the precision of the w eigh tsin to accoun t during training so that the precision of aw eigh t can b e traded against b oth its probabilit y den/-sit y and the excess data mis/#0Ct caused b y imprecision inthe w eigh t/./5 Noisy w eigh tsA standard w a y of limiting the amoun t of information inan um b er is to add zero/-mean Gaussian noise/. A t /#0Crstsigh t/, a noisy w eigh t seems to b e ev en more exp ensiv eto comm uni cate than a precise one since it app ears thatw e need to send a v ariance as w ell as a mean/, and thatw e need to decide on a precision for b oth of these/. As w eshall see/, ho w ev er/, the MDL framew ork can b e adaptedto allo wv ery noisy w eigh ts to b e comm unicated v eryc heaply /.When using bac kpropagation to train a feedforw ardneural net w ork/, it is standard practice to start at someparticular p oin ti nw eigh t space and to mo v e this p oin tin the direction that reduces the error function/. An al/-ternativ e approac h is to start with a m ultiv aria te Gaus/-sian distribution o v er w eigh tv ectors and to c hange b oththe mean and the v ariance of this cloud of w eigh tv ec/-tors so as to reduce some cost function/. W e shall restrictourselv es to distributions in whic h the w eigh ts are inde/-p enden t/, so the distribution can b e represen ted b y onemean and one v ariance p er w eigh t/.The cost function is the exp ected description length ofthe w eigh ts and of the data mis/#0Cts/. It turns out thathigh/-v ariance w eigh ts are c heap er to comm unicate butthey cause extra v ariance in the data mis/#0Cts th us mak/-ing these mis/#0Cts more exp ensiv e to comm unicate/./5/./1 The exp ected description length of thew eigh tsW e assume that the sender and the receiv er ha v ea nagreed Gaussian prior distribution/, P /, for a giv en\nw eigh t/. After learning/, the sender has a Gaussian p os/-terior distribution/, Q /, for the w eigh t/. W e describ e ametho d of comm unicating b oth the w eigh ts and thedata mis/#0Cts and sho w that using this metho d the n um/-b er of bits required to comm unicate the p osterior distri/-bution of a w eigh t is equal to the asymmetric div ergence/#28the Kullbac k/-Liebler distance/#29 from P to Q /.G /#28 P/; Q /#29/=\nZQ /#28 w /#29 log\nQ /#28 w /#29P /#28 w /#29\ndw /#28/5/#29/5/./2 The /#5Cbits bac k/\" argumen tT o comm unicate a set of noisy w eigh ts/, the sender /#0Crstcollapses the p osterior probabilit y distribution for eac hw eigh tb y using a source of random bits to pic k a precisev alue for the w eigh t /#28to within some v ery /#0Cne tolerancet /#29/. The probabilit y of pic king eac h p ossible v alue is de/-termined b y the p osterior probabilit y distribution forthe w eigh t/. The sender then comm uni cates these pre/-cise w eigh ts b y co ding them using some prior Gaussiandistribution/, P /, so that the comm uni cation cost of aprecise w eigh t/, w /, is/:C /#28 w /#29/= /, log t /, log P /#28 w /#29 /#28/6/#29t m ust b e small compared with the v ariance of P soC /#28 w /#29 is big/. Ho w ev er/, as w e shall see/, w e are due for abig refund at the end/.Ha ving sen t the precise w eigh ts/, the sender then com/-m unicates the data/-mis/#0Cts ac hiev ed using those w eigh ts/.Ha ving receiv ed the w eigh ts and the mis/#0Cts/, the receiv ercan then pro duce the correct outputs/. But he can alsodo something else/. Once he has the correct outputs he\ncan run whatev er learning algorithm w as used b y thesender and reco v er the exact same p osterior probabilit ydistribution/, Q /, that the sender collapsed in order to getthe precise w eigh ts/.\n/3No w/, since the receiv er kno ws thesender/'s p osterior distribution for eac hw eigh t and hekno ws the precise v alue that w as comm unicated/, he canreco v er all the random bits that the sender used to col/-lapse that distribution to that v alue/. So these randombits ha v e b een successfully comm unicated and w em ustsubtract them from the o v erall comm uni cation cost toget the true cost of comm unicating the mo del and themis/#0Cts/. The n um b er of random bits required to collapsethe p osterior distribution for a w eigh t/, Q /, to a particular/#0Cnely quan tized v alue/, w /, is/:R /#28 w /#29/= /, log t /, log Q /#28 w /#29 /#28/7/#29So the true exp ected description length for a noisyw eigh t is determined b y taking an exp ectation/, underthe distribution Q /:/3If the sender used random initial w eigh ts these can b ecomm unicated at a net cost of /0 bits using the metho d thatis b eing explained/.G /#28 P/; Q /#29/= h C /#28 w /#29 /, R /#28 w /#29 i /=\nZQ /#28 w /#29 log\nQ /#28 w /#29P /#28 w /#29\ndw /#28/8/#29F or Gaussians with di/#0Beren t means and v ariances/, theasymmetric div ergence isG /#28 P/; Q /#29 /= log\n/#1Bp/#1Bq\n/+\n/1/2 /#1B\n/2p\n/#02/#1B\n/2q\n/, /#1B\n/2p\n/+/#28 /#16p\n/, /#16q\n/#29\n/2\n/#03/#28/9/#29/5/./3 The exp ected description length of thedata mis/#0CtsT o compute the data/-mis/#0Ct cost giv en in equation /3 w eneed the exp ected v alue of /#28 d\ncj\n/, y\ncj\n/#29\n/2/. This squared erroris caused partly b y the systematic errors of the net w orkand partly b y the noise in the w eigh ts/. Unfortunately /,for general feedforw ard net w orks with noisy w eigh ts/, theexp ected squared errors are not easy to compute/. Linearappro ximations are p ossible if the lev el of noise in thew eigh ts is su/#0Ecien tly small compared with the smo oth/-ness of the non/-linearities/, but this defeats one of the\nmain purp oses of the idea whic h is to allo wv ery noisyw eigh ts/. F ortunately /, if there is only one hidden la y erand if the output units are linear/, it is p ossible to com/-\npute the exp ected squared error exactly /.The w eigh ts are assumed to ha v e indep enden t Gaussiannoise/, so for an y input v ector w e can compute the mean/#16xh\n/, and v ariance/, Vxh\n/, of the Gaussian/-distributed to/-tal input/, xh\n/, receiv ed b y hidden unit h /. Using a table/,w e can then compute the mean/, /#16yh\nand v ariance/, Vyh\n/,of the output of the hidden unit/, ev en though this out/-put is not Gaussian distributed/. A lot of computationis required to create this t w o/-dimensional table sinceman y di/#0Beren t pairs of /#16xh\nand Vxh\nm ust b e used/, andfor eac h pair w em ust use Mon te Carlo sampling or n u/-merical in tegration to compute /#16yh\nand Vyh\n/. Once thetable is built/, ho w ev er/, it is m uc h more e/#0Ecien t thanusing Mon te Carlo sampling at run time/.Since the noise in the outputs of the hidden units isindep enden t/, they indep enden tly con tribute v ariance toeac h linear output unit/. The noisy w eigh ts/, whj\n/, alsocon tribute v ariance to the output units/. Since the out/-put units are linear/, their outputs/, yj\n/, are equal to thetotal inputs they receiv e/, xj\n/. On a particular trainingcase/, the output/, yj\n/, of output unit j is a random v ari/-able with the follo wing mean and v ariance/:/#16yj\n/=\nXh\n/#16yh\n/#16whj\n/#28/1/0/#29Vyj\n/=\nXh\nh/#16\n/2whj\nVyh\n/+ /#16\n/2yh\nVwhj\n/+ Vyh\nVwhj\ni/#28/1/1/#29The mean and the v ariance of the activit y of outputunit j mak e indep endent con tributions to the exp ectedsquared error h Ej\ni /. If the desired output of j on a par/-ticular training case is dj\n/, h Ej\ni is giv en b y/:\nh Ej\ni /= h /#28 dj\n/, yj\n/#29\n/2i /=/#28 dj\n/, /#16yj\n/#29\n/2/+ Vyj\n/#28/1/2/#29So/, for eac h input v ector/, w e can use the table and theequations ab o v e to compute the exact v alue of h Ej\ni /.W ecan also bac kpropagate the exact deriv ativ es of E /=Pj\nh Ej\ni pro vided w e /#0Crst build another table to allo wderiv ativ es to b e bac kpropagated through the hiddenunits/. As b efore/, the table is indexed b y /#16xh\nand Vxh\nbutfor the bac kw ard pass eac h cell of the table con tains thefour partial deriv ativ es that are needed to to con v ert theoutput deriv ativ es of h in to its input deriv ativ es usingthe equations/:/@E/@/#16xh\n/=\n/@E/@/#16yh\n/@/#16yh/@/#16xh\n/+\n/@E/@Vyh\n/@Vyh/@/#16xh\n/#28/1/3/#29/@E/@Vxh\n/=\n/@E/@/#16yh\n/@/#16yh/@Vxh\n/+\n/@E/@Vyh\n/@Vyh/@Vxh\n/#28/1/4/#29/6 Letting the data determine the priorSo far/, w eh a v e assumed that the /#5Cprior/\" distributionthat is used for co ding the w eigh ts is a single Gaussian/.This co ding/-prior m ust b e kno wn to b oth the senderand the receiv er b efore the w eigh ts are comm unicated/.If w e /#0Cx its mean and v ariance in adv ance w e could pic kinappropriate v alues that mak ei tv ery exp ensiv et o c o d ethe actual w eigh ts/. W e therefore allo w the mean andv ariance of the co ding/-prior to b e determined duringthe optimization pro cess/, so the co ding/-prior dep endson the data/. This is a funn y kind of prior/! W e could tryto mak e sense of it in Ba y esian terms b y assuming thatw e start with a h yp er/-prior that sp eci/#0Ces probabilit ydistributions for the mean and v ariance of the co ding/-prior and then w e use the h yp er/-prior and the data to/#0Cnd the b est co ding/-prior/. This w ould automaticallytak ei n to accoun t the cost of comm unicati ng the co ding/-prior to a receiv er who only kno ws the h yp er/-prior/. Inpractice/, w e just ignore the cost of comm unicating thet w o parameters of the co ding/-prior so w e do not needto in v en th yp er/-priors/./6/./1 A more /#0Dexible prior distrib ut io n for thew eigh tsIf w e use a single Gaussian prior for comm uni cating thenoisy w eigh ts/, w e get a relativ ely simple p enalt y term/,the asymmetric div ergence/, for the p osterior distribu/-tion of eac h noisy w eigh t/. Unfortunately /, this co dingsc heme is not /#0Dexible enough to capture certain kindsof common structure in the w eigh ts/. Supp ose/, for ex/-ample/, that w ew an t a few of the w eigh ts to ha v ev aluesnear /1 and the rest to ha v ev alues v ery close to /0/. Ifthe p osterior distribution for eac hw eigh t has lo wv ari/-ance /#28to a v oid the extra squared error caused b y noisein the w eigh ts/#29 w e inevitiably pa y a high co de cost forw eigh ts b ecause no single Gaussian prior can pro vide ago o d mo del of a spik e around /0 and a spik e around /1/.If w e kno wi na d v ance that di/#0Beren t subsets of thew eigh ts are lik ely to ha v e di/#0Beren t distributions/, w e canuse di/#0Beren t co ding/-priors for the di/#0Beren t subsets/. AsMacKa y /#28/1/9/9/2/#29 has demonstrated/, it mak es sense to usedi/#0Beren t co ding/-priors for the input/-to/-hidden w eigh tsand the hidden/-to/-output w eigh ts since the input andoutput v alues ma yh a v e quite di/#0Beren t scales/. If w ed onot kno wi na d v ance whic hw eigh ts should b e similar/,w e can mo del the w eigh t distribution b y an adaptiv emixture of Gaussians as prop osed b yN o wlan and Hin/-ton /#28/1/9/9/2/#29/. During the optimization/, the means/, v ari/-ances and mixing prop ortions in the mixture adapt tomo del the clusters in the w eigh tv alues/. Sim ultaneously /,the w eigh ts adapt to /#0Ct the curren t mixture mo del sow eigh ts get pulled to w ards the cen ters of nearb y clus/-ters/. Supp ose/, for eaxample/, that there are t w o Gaus/-sians in the mixture/. If one gaussian has mean /1 and\nlo wv ariance and the other gaussian has mean /0 and lo wv ariance it is v ery c heap to enco de lo w/-v ariance w eigh tswith v alues near /1 or /0/.No wlan and Hin ton /#28/1/9/9/2/#29 implicitly assumed that thep osterior distribution for eac hw eigh t has a /#0Cxed andnegligible v ariance so they fo cussed on maximi zing theprobabilit y densit y of the mean of the w eigh t under theco ding/-prior mixture distribution/. W en o w sho wh o wtheir tec hnique can b e extended to tak ei n to accoun t thev ariance of the p osterior distributions for the w eigh ts/,assuming that the p osterior distributions are still con/-strained to b e single Gaussians/. As b efore/, w e ignore thecost of comm unicating the mixture distribution that isto b e used for co ding the w eigh ts/.The mixture prior has the form/:P /#28 w /#29/=\nXi\n/#19i\nPi\n/#28 w /#29 /#28/1/5/#29where /#19i\nis the mixing prop ortion of Gaussian Pi\n/. Theasymmetric div ergence b et w een the mixture prior andthe single Gaussian p osterior/, Q /, for a noisy w eigh ti sG /#28 P/; Q /#29/=\nZQ /#28 w /#29 log\nQ /#28 w /#29Pi\n/#19i\nPi\n/#28 w /#29\ndw /#28/1/6/#29The sum inside the log mak es this hard to in tegrate ana/-lytically /. This is unfortunate since the optimization pro/-cess requires that w e rep eatedly ev aluate b oth G /#28 P/; Q /#29and its deriv ativ es with resp ect to the parameters ofP and Q /.F ortunately /, there is a m uc h more tractableexpression whic h is an upp er b ound on G and can there/-fore b e used in its place/. This expression is in terms of\nthe Gi\n/#28 Pi\n/;Q /#29 the asymmetric div ergences b et w een thep osterior distribution/, Q /, and eac h of the Gaussians/, Pi\n/,in the mixture prior/./^G /#28 P/1\n/;P/2\n/:/:/: /;Q /#29/= /, log\nXi\n/#19i\ne\n/, Gi/#28/1/7/#29The w a y in whic h\n/^G dep ends on the Gi\nin equation/1/7 is precisely analogous to the w a y in whic h the free\nenergy of a system dep ends on the energies of the v ariousalternativ e con/#0Cgurations of the system/. Indeed/, onew a y to deriv e equation /1/7 is to de/#0Cne a co ding sc hemein whic h the co de cost resem bles a free energy and tothen use a lemma from statistical mec hanics/./7 A co ding sc heme that uses a mixtureof GaussiansSupp ose that a sender and a receiv er ha v e alreadyagreed on a particular mixture of Gaussians distribu/-tion/. The sender can no w send a sample from the p os/-terior Gaussian distribution of a w eigh t using the fol/-lo wing co ding sc heme/:/1/. Randomly pic k one of the Gaussians in the mixturewith probabilit y ri\ngiv en b yri\n/=\n/#19i\ne\n/, GiPj\n/#19j\ne\n/, Gj\n/#28/1/8/#29/2/. Comm unicate the c hoice of Gaussian to the re/-ceiv er/. If w e use the mixing prop ortions as a priorfor comm unicating the c hoice/, the exp ected co decost isXi\nri\nlog\n/1/#19i\n/#28/1/9/#29/3/. Comm unicate the sample v alue to the receiv er us/-ing the c hosen Gaussian/. If w e tak ei n to accoun tthe random bits that w e get bac k when the receiv erreconstructs the p osterior distribution from whic hthe sample w as c hosen/, the exp ected cost of com/-m unicating the sample isXi\nri\nGi\n/#28/2/0/#29So the exp ected cost of comm unicating b oth thec hoice of Gaussian and the sample v alue giv en thatc hoice isXi\nri\nGi\n/+\nXi\nri\nlog\n/1/#19i\n/=\nXi\nri\n/#28 /, log /#19i\ne\n/, Gi/#29/#28/2/1/#29/4/. After receiving samples from all the p osteriorw eigh t distributions and also receiving the errors onthe training cases with these sampled w eigh ts/, thereceiv er can run the learning algorithm and recon/-struct the p osterior distributions from whic h thew eigh ts are sampled/. This allo ws the receiv er toreconstruct all of the Gi\nand hence to reconstructthe random bits used to c ho ose a Gaussian from themixture/. So the n um b er of /#5Cbits bac k/\" that m ustb e subtracted from the exp ected cost in equation/2/1 isH /=\nXi\nri\nlog\n/1ri\n/#28/2/2/#29W en o w use a lemma from statistical mec hanics to get asimple expression for the exp ected co de cost min us thebits bac k/./7/./1 A lemma from statistical mec hanicsF or a ph ysical system at a temp erature of /1/, theHelmholtz free energy /, F /, is de/#0Cned as the exp ectedenergy min us the en trop yF /=\nXi\nri\nEi\n/,\nXi\nri\nlog\n/1ri\n/#28/2/3/#29where i is an index o v er the alternativ e p ossible statesof the system/, Ei\nis the energy of a state/, and ri\nis theprobabilit y of a state/. F is a function of the probabil/-it y distribution o v er states and the probabilit y distribu/-tion whic h minim izes F is the Boltzmann distribution inwhic h probabilities are exp onen tially related to energiesri\n/=\ne\n/, EiPj\ne\n/, Ej\n/#28/2/4/#29A t the minim um giv en b y the Boltzmann distribution/,the free energy is equal to min us the log of the partitionfunction/:F /= /, log\nXi\ne\n/, Ei/#28/2/5/#29If w e equate eac h Gaussian in the mixture with an al/-ternativ e state of a ph ysical system/, w e can equate e\n/, Eiwith /#19i\ne\n/, Gi/. So our metho d of pic king a Gaussian fromthe mixture uses a Boltzmann distribution b ecause it\nmak es ri\nprop ortional to e\n/, Ei/. The random bits thatare successfully comm unicated when the receiv er recon/-structs the probabilities ri\ncorresp ond exactly to the en/-trop y of the Boltzmann distribution/, so the total co decost /#28including the bits bac k/#29 is equiv alen tt o F and istherefore equal to the expression giv en in equation /1/7/./8 Impleme n tationWith an adaptiv e mixture of Gaussians co ding/-prior/,the deriv ativ es of the cost function are mo derately com/-plicated so it is easy to mak e an error in implemen tingthem/. This is w orrying b ecause gradien t descen t algo/-rithms are quite robust against minor errors/. Also/, itis hard to kno wh o w large to mak e the tables that areused for propagating Gaussian distributions through lo/-gistic functions or for bac kpropagating deriv ativ es/. T odemonstrate that the implemen tation w as correct andto decide the table sizes w e used the follo wing seman ticc hec k/. W ec hange eac h parameter b y a small step andc hec k that the cost function c hanges b y the pro duct ofthe gradien t and step size/. Using this metho d w e foundthat a /3/0/0 /#02 /3/0/0 table with linear in terp olation giv esreasonably accurate deriv ativ es/.\n/9 Preliminary ResultsW eh a v e not y et p erformed a thorough comparison b e/-t w een this algorithm and alternativ e metho ds and itma yw ell turn out that further re/#0Cnemen ts are requiredto mak e it comp etitiv e/. W eh a v e/, ho w ev er/, tried thealgorithm on one v ery high dimensional task with v eryscarce training data/. The task is to predict the e/#0Bec/-tiv eness of a class of p eptide molecules/. Eac h moleculeis describ ed b y /1/2/8 parameters /#28the input v ector/#29 andhas an e/#0Bectiv eness that is a single scalar /#28the ouputv alue/#29/. All inputs and outputs w ere normalized to ha v ezero mean and unit v ariance so that the w eigh ts couldb e exp ected to ha v e similar scales/. The training set con/-sisted of /1/0/5 cases and the test set w as the remaining/4/2/0 cases/. W e delib erately c hose a v ery small trainingset since these are the circumstances in whic h it shouldb e most helpful to con trol the amoun t of information inthe w eigh ts/. W e tried a net w ork with /4 hidden units/.This net w ork con tains /5/2/1 adaptiv ew eigh ts /#28includingthe biases of the output and hidden units/#29 so it o v er/#0Ctsthe /1/0/5 training cases v ery badly if w e do not limit theinformation in the w eigh ts/.W e used an adaptiv e mixture of /5 Gaussians as ourco ding/-prior for the w eigh ts/. The Gaussians w ere ini/-tialized with means uniformly spaced b et w een /, /0 /: /2/4and /+/0 /: /2/4 and separated b y /2 standard deviations fromtheir neigh b ors/. The initial means for the p osterior dis/-tributions of eac hw eigh tw ere c hosen from a Gaussianwith mean /0 and standard deviation /0 /: /1/5/. The stan/-dard deviations of the p osteriors for the w eigh ts w ereall initialized at /0 /: /1/.W e optimize all of the parameters sim ultaneously us/-ing a conjugate gradien t metho d/. F or the v ariances w eoptimize the log v ariance so that it cannot go negativ eand cannot collapse to zero/. T o ensure that the mix/-ing prop ortions of the Gaussians in the co ding/-prior liebe t w een /0 and /1 and add to /1 w e optimize the xi\nwhere/#19i\n/=\ne\nxiPj\ne\nxj\n/#28/2/6/#29If w e p enalize the w eigh ts b y the full cost of describingthem/, the optimization quic kly mak es all of the w eigh tsequal and negativ e and uses the bias of the output unitto /#0Cx the output at the mean of the desired v alues inthe training set/. It seems that it is initially v ery easy toreduce the com bined cost function b y using w eigh ts thatcon tain almost no information/, and it is v ery hard to es/-cap e from this p o or solution/. T oa v oid this trap/, w em ul/-tiply the cost of the w eigh ts b y a co e/#0Ecen t that starts at/: /0/5 and gradually increases to /1 according to the sc hed/-ule /: /0/5 /;/: /1 /;/: /1/5 /;/: /2 /;/: /3 /;/: /4 /;/: /5 /;/: /6 /;/: /7 /;/: /8 /;/: /9 /; /1 /: /0/. A t eac hv alueof the co e/#0Ecien tw e do /1/0/0 conjugate gradien t up datesof the w eigh ts and at the /#0Cnal v alue of /1 /: /0w e do not ter/-minate the optimization un til the cost function c hangesb y less than /1/0\n/, /6nats /#28a nat is log/2\n/#28 e /#29 bits/#29/. Figure/2 sho ws all the incoming and outgoing w eigh ts of thefour hidden units after one run of the optimization/. ItFigure /2/: The /#0Cnal w eigh ts of the net w ork/. Eac hlarge blo c k represen ts one hidden unit/. The smallblac k or white rectangles represen t negativ eo rp ositiv ew eigh ts with the area of a rectangle rep/-resen ting the magnitude of the w eigh t/. The b ot/-tom /1/2 ro ws in eac h blo c k represen t the incomingw eigh ts of the hidden unit/. The cen tral w eigh ta tthe top of eac h blo c k is the w eigh t from the hiddenunit to the linear output unit/. The w eigh t at thetop/-righ to fab l o c k is the bias of the hidden unit/.\nFigure /3/: The /#0Cnal probabilit y distribution thatis used for co ding the w eigh ts/. This distributionis implem en ted b y adapting the means/, v ariancesand mixing prop ortions of /#0Cv e gaussians/.is clear that the w eigh ts form three fairly sharp clus/-ters/. Figure /3 sho ws that the mixture of /5 Gaussianshas adapted to implem en t the appropriate co ding/-priorfor this w eigh t distribution/.The p erformance of the net w ork can b e measured b ycomparing the squared error it ac hiev es on the test datawith the error that w ould b e ac hiev ed b y simply guess/-ing the mean of the correct answ ers for the test data/:Relativ e Error /=\nPc\n/#28 dc\n/, yc\n/#29\n/2Pc\n/#28 dc\n/,\n/#16d /#29\n/2\n/#28/2/7/#29W e ran the optimization /#0Cv e times using di/#0Beren t ran/-domly c hosen v alues for the initial means of the noisyw eigh ts/. F or the net w ork that ac hiev ed the lo w est v alueof the o v erall cost function/, the relativ e error w as /0 /: /2/8/6/.This compares with a relativ e error of /0 /: /9/6/7 for the samenet w ork when w e used noise/-free w eigh ts and did notp enalize their information con ten t/. The b est relativ eerror obtained using simple w eigh t/-deca y with four non/-linear hidden units w as /: /3/1/7/. This required a carefullyc hosen p enalt y co e/#0Ecien t for the squared w eigh ts thatcorresp onds to /#1B\n/2j\n/=/#1B\n/2w\nin equation /4/. T o set this w eigh t/-deca y co e/#0Ecien t appropriately it w as necessary to tryman y di/#0Beren tv alues on a p ortion of the training setand to use the remainder of the training set to decide\nwhic h co e/#0Ecien tg a v e the b est generalization/. Once theb est co e/#0Ecien t had b een determined the whole of thetraining set w as used with this co e/#0Ecien t/. A lo w er er/-ror of /0 /: /2/9/1 can b e ac hiev ed using w eigh t/-deca yi f w egradually increase the w eigh t/-deca y co e/#0Ecien t and pic kthe v alue that giv es optimal p erformance on the testdata/. But this is c heating/. Linear regression ga v eah uge relativ e error of /3/5 /: /6 /#28gross o v er/#0Ctting/#29 but thisfell to /0 /: /2/9/1 when w e p enalized the sum of the squaresof the regression co e/#0Ecien ts b y an amoun t that w as c ho/-sen to optimize p erformance on the test data/. This isalmost iden tical to the p erformance with /4 hidden unitsand optimal w eigh t/-deca y probably b ecause/, with smallw eigh ts/, the hidden units op erate in their cen tral linearrange/, so the whole net w ork is e/#0Bectiv ely linear/.These preliminary results demonstrate that our newmetho d allo ws us to /#0Ct quite complicated non/-linearmo dels ev en when the n um b er of training cases is lessthan the n um b er of dimensions in the input v ector/.The results also sho w that the new metho d is sligh tlyb etter than simple w eigh t/-deca y on at least one task/.Muc h more exp erimen tal w ork is required to decidewhether the metho d is comp etitiv e with other statis/-tical tec hniques for handling non/-linear tasks in whic hthe amoun t of training data is v ery small compared withthe dimensionalit y of the input/. It is also w orth men/-tioning that the solution with the lo w est v alue of thetotal description length w as the solution in whic h allthe w eigh ts except the output bias are equal and nega/-tiv e/. This solution has a relativ e error of appro ximately/1 /: /0 so it is a serious em barrassmen t for the Minim umDescription Length Principle or for our metho d of de/-\nscribing the w eigh ts/./1/0 DiscussionThere is a correct/, but in tractable/, Ba y esian metho dof determining the w eigh ts in a feedforw ard neural net/-w ork/. W e start with a prior distribution o v er all p ossiblep oin ts in w eigh t space/. W e then construct the correctp osterior distribution at eac h p oin ti nw eigh t space b ym ultiplying the prior b y the probabilit y of getting theoutputs in the training set giv en those w eigh ts/.\n/4Fi/-nally w e normalize to get the full p osterior distribution/.Then w e use this distribution of w eigh tv alues to mak epredictions for new input v ectors/.In practice/, the closest w e can get to the ideal Ba y esianmetho d is to use a Mon te Carlo metho d to sample fromthe p osterior distribution/. This could b e done b y con/-sidering random mo v es in w eigh t space and accepting amo v e with a probabilit y that dep ends on ho ww ell theresulting net w ork /#0Cts the desired outputs/. Neal /#28/1/9/9/3/#29sho ws ho w the gradien t information pro vided b y bac k/-propagation can b e used to get a m uc h more e/#0Ecien tmetho d of obtaining samples from the p osterior distri/-bution/. The ma jor adv an tage of Mon te Carlo meth/-o ds is that they do not imp ose unrealistically simple\nassumptions ab out the shap e of the p osterior distribu/-tion in w eigh t space/.If w e are willing to mak e simplifyi ng assumptions ab outthe p osterior distribution/, time/-consuming Mon te Carlosim ulations can b e a v oided/. MacKa y /#28/1/9/9/2/#29 /#0Cnds a sin/-gle lo cally optimal p oin ti nw eigh t space and constructsa full co v ariance Gaussian appro ximation to the p os/-terior distribution around that p oin t/. The alternativ emetho d prop osed in this pap er is to use a simpler Gaus/-\nsian appro ximation /#28with no o/#0B/-diagonal terms in theco v ariance matrix/#29 but to tak e this distribution in to ac/-coun t during the learning/. With one la y er of non/-linear/4This assumes that the output of the neural net repre/-sen ts the mean of a Gaussian distributio n from whic h the/#0Cnal output is randomly selected/. So the /#0Cnal output couldb e exactly correct ev en though the output of the net is not/.\nhidden units/, the in tegration o v er the Gaussian distri/-bution can b e p erformed exactly and the exact w eigh tderiv ativ es can b e computed e/#0Ecien tly /.It is not clear ho wm uc h is lost b y ignoring the o/#0B/-diagonal terms in the co v ariance matrix/. Da vid Mac k a y/#28p ersonal comm unication/#29 has sho wn that if standardbac kpropagation is used to /#0Cnd a single/, lo cally optimalp oin ti n w eigh t space and a Gaussian appro ximationto the p osterior w eigh t distribution is then constructedaround this p oin t/, the co v ariances b et w een di/#0Beren tw eigh ts are signi/#0Ccan t/. Ho w ev er/, this do es not meanthat the co v ariances are signi/#0Ccan t when the learningalgorithm is explicitly manipulating the Gaussian dis/-tribution b ecause in this case the learning will try to\nforce the noise in the w eigh ts to b e indep enden t/. Thepressure for indep endence comes from the fact that thecost function will o v erestimate the information in thew eigh ts if they ha v e correlated noise/. W e are curren tlyp erforming sim ulations to see if this pressure do es in/-deed suppress the co v ariances/.When using the standard bac kpropagation algorithm/, itis essen tial that the output of a hidden unit is a smo othfunction of its input/. This is wh y the hidden units usea smo oth sigmoid function instead of a linear thresh/-old function/. With noisy w eigh ts/, ho w ev er/, it is p ossi/-ble to use a v ersion of the bac kpropagation algorithmdescrib ed ab o v e in net w orks that ha v e one la y er of lin/-ear threshold units/. The noise in the w eigh ts ensuresthat the probabilit y of a threshold unit b eing activ ei sa smo oth function of its inputs/. As a result/, it is easierto optimize a whole Gaussian distribution o v er w eigh tv ectors than it is to optimize a single w eigh tv ector/./1/1 Ac kno wledgemen tsThis researc hw as funded b y op erating and strategicgran ts from NSER C/. Geo/#0Brey Hin ton is the Norandafello w of the Canadian Institute for Adv anced Researc h/.W e thank Da vid Mac k a y /, Radford Neal/, Chris Williamsand Ric h Zemel for helpful discussions/./1/2 ReferencesHin ton/, G/. E/. /#28/1/9/8/7/#29 Learning translation in v arian trecognition in a massiv ely parallel net w ork/. In Go os/, G/.and Hartmanis/, J/./, editors/, P ARLE/: Par al lel A r chite c/-tur es and L anguages Eur op e /, pages /1/#7B/1/3/, Lecture Notesin Computer Science/, Springer/-V erlag/, Berlin/.Lang/, K/./, W aib el/, A/. and Hin ton/, G/. E/. /#28/1/9/9/0/#29 A Time/-Dela y Neural Net w ork Arc hitecture for Isolated W ordRecognition/. Neur al Networks /, /3 /, /2/3/-/4/3/.Le Cun/, Y/./, Boser/, B/./, Denk er/, J/. S/./, Henderson/,D/./, Ho w ard/, R/. E/./, Hubbard/, W/. and Jac k el/, L/. D/./#28/1/9/8/9/#29 Bac k/-Propagation Applied to Handwritten Zip/-co de Recognition/. Neur al Computation /, /1 /, /5/4/1/-/5/5/1/.Mac k a y /, D/. J/. C/. /#28/1/9/9/2/#29 A practical Ba y esian framew orkfor bac kpropagation net w orks/. Neur al Computation /, /4 /,/4/4/8/-/4/7/2/.Neal/, R/. M/. /#28/1/9/9/3/#29 Ba y esian learning via sto c hastic dy/-namics/. In Giles/, C/. L/./, Hanson/, S/. J/. and Co w an/, J/.D/. /#28Eds/#29/, A dvanc es in Neur al Information Pr o c essingSystems /5 /, Morgan Kaufmann/, San Mateo CA/.No wlan/. S/. J/. and Hin ton/, G/. E/. /#28/1/9/9/2/#29 Simplifyingneural net w orks b y soft w eigh t sharing/. Neur al Compu/-tation /, /4 /, /1/7/3/-/1/9/3/.Rissanen/, J/. /#28/1/9/8/6/#29 Sto c hastic Complexit y and Mo del/-ing/. A nnals of Statistics /, /1/4 /, /1/0/8/0/-/1/1/0/0/.\n\n\n\nPointer Networks\nOriol Vinyals\u0003\nGoogle BrainMeire Fortunato\u0003\nDepartment of Mathematics, UC BerkeleyNavdeep Jaitly\nGoogle Brain\nAbstract\nWe introduce a new neural architecture to learn the conditional probability of an\noutput sequence with elements that are discrete tokens corresponding to positions\nin an input sequence. Such problems cannot be trivially addressed by existent ap-\nproaches such as sequence-to-sequence [1] and Neural Turing Machines [2], be-\ncause the number of target classes in each step of the output depends on the length\nof the input, which is variable. Problems such as sorting variable sized sequences,\nand various combinatorial optimization problems belong to this class. Our model\nsolves the problem of variable size output dictionaries using a recently proposed\nmechanism of neural attention. It differs from the previous attention attempts in\nthat, instead of using attention to blend hidden units of an encoder to a context\nvector at each decoder step, it uses attention as a pointer to select a member of\nthe input sequence as the output. We call this architecture a Pointer Net (Ptr-Net).\nWe show Ptr-Nets can be used to learn approximate solutions to three challenging\ngeometric problems – ﬁnding planar convex hulls, computing Delaunay triangu-\nlations, and the planar Travelling Salesman Problem – using training examples\nalone. Ptr-Nets not only improve over sequence-to-sequence with input attention,\nbut also allow us to generalize to variable size output dictionaries. We show that\nthe learnt models generalize beyond the maximum lengths they were trained on.\nWe hope our results on these tasks will encourage a broader exploration of neural\nlearning for discrete problems.\n1 Introduction\nRecurrent Neural Networks (RNNs) have been used for learning functions over sequences from\nexamples for more than three decades [3]. However, their architecture limited them to settings\nwhere the inputs and outputs were available at a ﬁxed frame rate (e.g. [4]). The recently introduced\nsequence-to-sequence paradigm [1] removed these constraints by using one RNN to map an input\nsequence to an embedding and another (possibly the same) RNN to map the embedding to an output\nsequence. Bahdanau et. al. augmented the decoder by propagating extra contextual information\nfrom the input using a content-based attentional mechanism [5, 2, 6]. These developments have\nmade it possible to apply RNNs to new domains, achieving state-of-the-art results in core problems\nin natural language processing such as translation [1, 5] and parsing [7], image and video captioning\n[8, 9], and even learning to execute small programs [2, 10].\nNonetheless, these methods still require the size of the output dictionary to be ﬁxed a priori . Because\nof this constraint we cannot directly apply this framework to combinatorial problems where the size\nof the output dictionary depends on the length of the input sequence. In this paper, we address this\nlimitation by repurposing the attention mechanism of [5] to create pointers to input elements. We\nshow that the resulting architecture, which we name Pointer Networks (Ptr-Nets), can be trained to\noutput satisfactory solutions to three combinatorial optimization problems – computing planar con-\nvex hulls, Delaunay triangulations and the symmetric planar Travelling Salesman Problem (TSP).\nThe resulting models produce approximate solutions to these problems in a purely data driven fash-\n\u0003Equal contribution\n1arXiv:1506.03134v2  [stat.ML]  2 Jan 2017(a) Sequence-to-Sequence\n(b) Ptr-Net\nFigure 1: (a)Sequence-to-Sequence - An RNN (blue) processes the input sequence to create a code\nvector that is used to generate the output sequence (purple) using the probability chain rule and\nanother RNN. The output dimensionality is ﬁxed by the dimensionality of the problem and it is the\nsame during training and inference [1]. (b)Ptr-Net - An encoding RNN converts the input sequence\nto a code (blue) that is fed to the generating network (purple). At each step, the generating network\nproduces a vector that modulates a content-based attention mechanism over inputs ([5, 2]). The\noutput of the attention mechanism is a softmax distribution with dictionary size equal to the length\nof the input.\nion (i.e., when we only have examples of inputs and desired outputs). The proposed approach is\ndepicted in Figure 1.\nThe main contributions of our work are as follows:\n\u000fWe propose a new architecture, that we call Pointer Net, which is simple and effective. It\ndeals with the fundamental problem of representing variable length dictionaries by using a\nsoftmax probability distribution as a “pointer”.\n\u000fWe apply the Pointer Net model to three distinct non-trivial algorithmic problems involving\ngeometry. We show that the learned model generalizes to test problems with more points\nthan the training problems.\n\u000fOur Pointer Net model learns a competitive small scale ( n\u001450) TSP approximate solver.\nOur results demonstrate that a purely data driven approach can learn approximate solutions\nto problems that are computationally intractable.\n2 Models\nWe review the sequence-to-sequence [1] and input-attention models [5] that are the baselines for this\nwork in Sections 2.1 and 2.2. We then describe our model - Ptr-Net in Section 2.3.\n2.1 Sequence-to-Sequence Model\nGiven a training pair, (P;CP), the sequence-to-sequence model computes the conditional probabil-\nityp(CPjP;\u0012)using a parametric model (an RNN with parameters \u0012) to estimate the terms of the\nprobability chain rule (also see Figure 1), i.e.\np(CPjP;\u0012) =m(P)Y\ni=1p\u0012(CijC1;:::;Ci\u00001;P;\u0012): (1)\n2HereP=fP1;:::;Pngis a sequence of nvectors andCP=fC1;:::;Cm(P)gis a sequence of\nm(P)indices, each between 1 and n.\nThe parameters of the model are learnt by maximizing the conditional probabilities for the training\nset, i.e.\n\u0012\u0003= arg max\n\u0012X\nP;CPlogp(CPjP;\u0012); (2)\nwhere the sum is over training examples.\nAs in [1], we use an Long Short Term Memory (LSTM) [11] to model p\u0012(CijC1;:::;Ci\u00001;P;\u0012).\nThe RNN is fed Piat each time step, i, until the end of the input sequence is reached, at which time\na special symbol,)is input to the model. The model then switches to the generation mode until\nthe network encounters the special symbol (, which represents termination of the output sequence.\nNote that this model makes no statistical independence assumptions. We use two separate RNNs\n(one to encode the sequence of vectors Pj, and another one to produce or decode the output symbols\nCi). We call the former RNN the encoder and the latter the decoder or the generative RNN.\nDuring inference, given a sequence P, the learnt parameters \u0012\u0003are used to select the sequence\n^CPwith the highest probability, i.e., ^CP= arg max\nCPp(CPjP;\u0012\u0003):Finding the optimal sequence ^C\nis computationally impractical because of the combinatorial number of possible output sequences.\nInstead we use a beam search procedure to ﬁnd the best possible sequence given a beam size.\nIn this sequence-to-sequence model, the output dictionary size for all symbols Ciis ﬁxed and equal\nton, since the outputs are chosen from the input. Thus, we need to train a separate model for each\nn. This prevents us from learning solutions to problems that have an output dictionary with a size\nthat depends on the input sequence length.\nUnder the assumption that the number of outputs is O(n)this model has computational complexity\nofO(n). However, exact algorithms for the problems we are dealing with are more costly. For exam-\nple, the convex hull problem has complexity O(nlogn). The attention mechanism (see Section 2.2)\nadds more “computational capacity” to this model.\n2.2 Content Based Input Attention\nThe vanilla sequence-to-sequence model produces the entire output sequence CPusing the ﬁxed\ndimensional state of the recognition RNN at the end of the input sequence P. This constrains\nthe amount of information and computation that can ﬂow through to the generative model. The\nattention model of [5] ameliorates this problem by augmenting the encoder and decoder RNNs with\nan additional neural network that uses an attention mechanism over the entire sequence of encoder\nRNN states.\nFor notation purposes, let us deﬁne the encoder and decoder hidden states as (e1;:::;en)and\n(d1;:::;dm(P)), respectively. For the LSTM RNNs, we use the state after the output gate has\nbeen component-wise multiplied by the cell activations. We compute the attention vector at each\noutput timeias follows:\nui\nj=vTtanh(W1ej+W2di)j2(1;:::;n )\nai\nj=softmax (ui\nj) j2(1;:::;n ) (3)\nd0\ni=nX\nj=1ai\njej\nwhere softmax normalizes the vector ui(of lengthn) to be the “attention” mask over the inputs,\nandv,W1, andW2are learnable parameters of the model. In all our experiments, we use the same\nhidden dimensionality at the encoder and decoder (typically 512), so vis a vector and W1andW2\nare square matrices. Lastly, d0\nianddiare concatenated and used as the hidden states from which we\nmake predictions and which we feed to the next time step in the recurrent model.\nNote that for each output we have to perform noperations, so the computational complexity at\ninference time becomes O(n2).\n3This model performs signiﬁcantly better than the sequence-to-sequence model on the convex hull\nproblem, but it is not applicable to problems where the output dictionary size depends on the input.\nNevertheless, a very simple extension (or rather reduction) of the model allows us to do this easily.\n2.3 Ptr-Net\nWe now describe a very simple modiﬁcation of the attention model that allows us to apply the\nmethod to solve combinatorial optimization problems where the output dictionary size depends on\nthe number of elements in the input sequence.\nThe sequence-to-sequence model of Section 2.1 uses a softmax distribution over a ﬁxed sized output\ndictionary to compute p(CijC1;:::;Ci\u00001;P)in Equation 1. Thus it cannot be used for our problems\nwhere the size of the output dictionary is equal to the length of the input sequence. To solve this\nproblem we model p(CijC1;:::;Ci\u00001;P)using the attention mechanism of Equation 3 as follows:\nui\nj=vTtanh(W1ej+W2di)j2(1;:::;n )\np(CijC1;:::;Ci\u00001;P) = softmax (ui)\nwhere softmax normalizes the vector ui(of lengthn) to be an output distribution over the dictionary\nof inputs, and v,W1, andW2are learnable parameters of the output model. Here, we do not blend\nthe encoder state ejto propagate extra information to the decoder, but instead, use ui\njas pointers\nto the input elements. In a similar way, to condition on Ci\u00001as in Equation 1, we simply copy\nthe corresponding PCi\u00001as the input. Both our method and the attention model can be seen as an\napplication of content-based attention mechanisms proposed in [6, 5, 2].\nWe also note that our approach speciﬁcally targets problems whose outputs are discrete and corre-\nspond to positions in the input. Such problems may be addressed artiﬁcially – for example we could\nlearn to output the coordinates of the target point directly using an RNN. However, at inference,\nthis solution does not respect the constraint that the outputs map back to the inputs exactly. With-\nout the constraints, the predictions are bound to become blurry over longer sequences as shown in\nsequence-to-sequence models for videos [12].\n3 Motivation and Datasets Structure\nIn the following sections, we review each of the three problems we considered, as well as our data\ngeneration protocol.1\nIn the training data, the inputs are planar point sets P=fP1;:::;Pngwithnelements each, where\nPj= (xj;yj)are the cartesian coordinates of the points over which we ﬁnd the convex hull, the De-\nlaunay triangulation or the solution to the corresponding Travelling Salesman Problem. In all cases,\nwe sample from a uniform distribution in [0;1]\u0002[0;1]. The outputsCP=fC1;:::;Cm(P)gare\nsequences representing the solution associated to the point set P. In Figure 2, we ﬁnd an illustration\nof an input/output pair (P;CP)for the convex hull and the Delaunay problems.\n3.1 Convex Hull\nWe used this example as a baseline to develop our models and to understand the difﬁculty of solving\ncombinatorial problems with data driven approaches. Finding the convex hull of a ﬁnite number\nof points is a well understood task in computational geometry, and there are several exact solutions\navailable (see [13, 14, 15]). In general, ﬁnding the (generally unique) solution has complexity\nO(nlogn), wherenis the number of points considered.\nThe vectorsPjare uniformly sampled from [0;1]\u0002[0;1]. The elements Ciare indices between 1\nandncorresponding to positions in the sequence P, or special tokens representing beginning or end\nof sequence. See Figure 2 (a) for an illustration. To represent the output as a sequence, we start\nfrom the point with the lowest index, and go counter-clockwise – this is an arbitrary choice but helps\nreducing ambiguities during training.\n1We release all the datasets at http://goo.gl/NDcOIG .\n4(a) InputP=fP1;:::;P 10g, and the output se-\nquenceCP=f);2;4;3;5;6;7;2;(grepresent-\ning its convex hull.\nP1\nP2P3\nP4P5(b) InputP=fP1;:::;P 5g, and the outputCP=\nf);(1;2;4);(1;4;5);(1;3;5);(1;2;3);(g repre-\nsenting its Delaunay Triangulation.\nFigure 2: Input/output representation for ( a) convex hull and ( b) Delaunay triangulation. The tokens\n)and(represent beginning and end of sequence, respectively.\n3.2 Delaunay Triangulation\nA Delaunay triangulation for a set Pof points in a plane is a triangulation such that each circumcircle\nof every triangle is empty, that is, there is no point from Pin its interior. Exact O(nlogn)solutions\nare available [16], where nis the number of points in P.\nIn this example, the outputs CP=fC1;:::;Cm(P)gare the corresponding sequences representing\nthe triangulation of the point set P. EachCiis a triple of integers from 1 to ncorresponding to the\nposition of triangle vertices in Por the beginning/end of sequence tokens. See Figure 2 (b).\nWe note that any permutation of the sequence CPrepresents the same triangulation for P, addi-\ntionally each triangle representation Ciof three integers can also be permuted. Without loss of\ngenerality, and similarly to what we did for convex hulls at training time, we order the triangles Ci\nby their incenter coordinates (lexicographic order) and choose the increasing triangle representa-\ntion2. Without ordering, the models learned were not as good, and ﬁnding a better ordering that the\nPtr-Net could better exploit is part of future work.\n3.3 Travelling Salesman Problem (TSP)\nTSP arises in many areas of theoretical computer science and is an important algorithm used for\nmicrochip design or DNA sequencing. In our work we focused on the planar symmetric TSP: given\na list of cities, we wish to ﬁnd the shortest possible route that visits each city exactly once and\nreturns to the starting point. Additionally, we assume the distance between two cities is the same\nin each opposite direction. This is an NP-hard problem which allows us to test the capabilities and\nlimitations of our model.\nThe input/output pairs (P;CP)have a similar format as in the Convex Hull problem described in\nSection 3.1.Pwill be the cartesian coordinates representing the cities, which are chosen randomly\nin the [0;1]\u0002[0;1]square.CP=fC1;:::;Cngwill be a permutation of integers from 1 to n\nrepresenting the optimal path (or tour). For consistency, in the training dataset, we always start in\nthe ﬁrst city without loss of generality.\nTo generate exact data, we implemented the Held-Karp algorithm [17] which ﬁnds the optimal\nsolution inO(2nn2)(we used it up to n= 20 ). For larger n, producing exact solutions is extremely\ncostly, therefore we also considered algorithms that produce approximated solutions: A1 [18] and\nA2 [19], which are both O(n2), and A3 [20] which implements the O(n3)Christoﬁdes algorithm.\nThe latter algorithm is guaranteed to ﬁnd a solution within a factor of 1.5 from the optimal length.\nTable 2 shows how they performed in our test sets.\n2We choose Ci= (1;2;4)instead of (2,4,1) or any other permutation.\n54 Empirical Results\n4.1 Architecture and Hyperparameters\nNo extensive architecture or hyperparameter search of the Ptr-Net was done in the work presented\nhere, and we used virtually the same architecture throughout all the experiments and datasets. Even\nthough there are likely some gains to be obtained by tuning the model, we felt that having the same\nmodel hyperparameters operate on all the problems would make the main message of the paper\nstronger.\nAs a result, all our models used a single layer LSTM with either 256 or 512 hidden units, trained with\nstochastic gradient descent with a learning rate of 1.0, batch size of 128, random uniform weight\ninitialization from -0.08 to 0.08, and L2 gradient clipping of 2.0. We generated 1M training example\npairs, and we did observe overﬁtting in some cases where the task was simpler (i.e., for small n).\n4.2 Convex Hull\nWe used the convex hull as the guiding task which allowed us to understand the deﬁciencies of\nstandard models such as the sequence-to-sequence approach, and also setting up our expectations\non what a purely data driven model would be able to achieve with respect to an exact solution.\nWe reported two metrics: accuracy, and area covered of the true convex hull (note that any simple\npolygon will have full intersection with the true convex hull). To compute the accuracy, we con-\nsidered two output sequences C1andC2to be the same if they represent the same polygon. For\nsimplicity, we only computed the area coverage for the test examples in which the output represents\na simple polygon (i.e., without self-intersections). If an algorithm fails to produce a simple polygon\nin more than 1% of the cases, we simply reported FAIL.\nThe results are presented in Table 1. We note that the area coverage achieved with the Ptr-Net is\nclose to 100%. Looking at examples of mistakes, we see that most problems come from points that\nare aligned (see Figure 3 (d) for a mistake for n= 500 ) – this is a common source of errors in most\nalgorithms to solve the convex hull.\nIt was seen that the order in which the inputs are presented to the encoder during inference affects\nits performance. When the points on the true convex hull are seen “late” in the input sequence, the\naccuracy is lower. This is possibly the network does not have enough processing steps to “update”\nthe convex hull it computed until the latest points were seen. In order to overcome this problem,\nwe used the attention mechanism described in Section 2.2, which allows the decoder to look at\nthe whole input at any time. This modiﬁcation boosted the model performance signiﬁcantly. We\ninspected what attention was focusing on, and we observed that it was “pointing” at the correct\nanswer on the input side. This inspired us to create the Ptr-Net model described in Section 2.3.\nMore than outperforming both the LSTM and the LSTM with attention, our model has the key\nadvantage of being inherently variable length. The bottom half of Table 1 shows that, when training\nour model on a variety of lengths ranging from 5 to 50 (uniformly sampled, as we found other forms\nof curriculum learning to not be effective), a single model is able to perform quite well on all lengths\nit has been trained on (but some degradation for n= 50 can be observed w.r.t. the model trained only\non length 50 instances). More impressive is the fact that the model does extrapolate to lengths that it\nhas never seen during training. Even for n= 500 , our results are satisfactory and indirectly indicate\nthat the model has learned more than a simple lookup. Neither LSTM or LSTM with attention can\nbe used for any given n06=nwithout training a new model on n0.\n4.3 Delaunay Triangulation\nThe Delaunay Triangulation test case is connected to our ﬁrst problem of ﬁnding the convex hull. In\nfact, the Delaunay Triangulation for a given set of points triangulates the convex hull of these points.\nWe reported two metrics: accuracy and triangle coverage in percentage (the percentage of triangles\nthe model predicted correctly). Note that, in this case, for an input point set P, the output sequence\nC(P)is, in fact, a set. As a consequence, any permutation of its elements will represent the same\ntriangulation.\n6Table 1: Comparison between LSTM, LSTM with attention, and our Ptr-Net model on the convex\nhull problem. Note that the baselines must be trained on the same nthat they are tested on.\nMETHOD TRAINED n n ACCURACY AREA\nLSTM [1] 50 50 1.9% FAIL\n+ATTENTION [5] 50 50 38.9% 99.7%\nPTR-NET 50 50 72.6% 99.9%\nLSTM [1] 5 5 87.7% 99.6%\nPTR-NET 5-50 5 92.0% 99.6%\nLSTM [1] 10 10 29.9% FAIL\nPTR-NET 5-50 10 87.0% 99.8%\nPTR-NET 5-50 50 69.6% 99.9%\nPTR-NET 5-50 100 50.3% 99.9%\nPTR-NET 5-50 200 22.1% 99.9%\nPTR-NET 5-50 500 1.3% 99.2%\nGround Truth Predictions\n(a) LSTM, m=50,n=50\nGround Truth (b) Truth, n=50\nGround Truth: tour length is 3.518 (c) Truth, n=20\nGround Truth Predictions\n(d) Ptr-Net, m=5-50, n=500\nPredictions (e) Ptr-Net , m=50,n=50\nPredictions: tour length is 3.523 (f) Ptr-Net , m=5-20, n=20\nFigure 3: Examples of our model on Convex hulls (left), Delaunay (center) and TSP (right), trained\nonmpoints, and tested on npoints. A failure of the LSTM sequence-to-sequence model for Convex\nhulls is shown in (a). Note that the baselines cannot be applied to a different length from training.\nUsing the Ptr-Net model for n= 5, we obtained an accuracy of 80.7% and triangle coverage of\n93.0%. For n= 10 , the accuracy was 22.6% and the triangle coverage 81.3%. For n= 50 , we\ndid not produce any precisely correct triangulation, but obtained 52.8% triangle coverage. See the\nmiddle column of Figure 3 for an example for n= 50 .\n4.4 Travelling Salesman Problem\nWe considered the planar symmetric travelling salesman problem (TSP), which is NP-hard as the\nthird problem. Similarly to ﬁnding convex hulls, it also has sequential outputs. Given that the Ptr-\nNet implements an O(n2)algorithm, it was unclear if it would have enough capacity to learn a\nuseful algorithm solely from data.\nAs discussed in Section 3.3, it is feasible to generate exact solutions for relatively small values\nofnto be used as training data. For larger n, due to the importance of TSP, good and efﬁcient\nalgorithms providing reasonable approximate solutions exist. We used three different algorithms in\nour experiments – A1, A2, and A3 (see Section 3.3 for references).\nTable 2 shows all of our results on TSP. The number reported is the length of the proposed tour.\nUnlike the convex hull and Delaunay triangulation cases, where the decoder was unconstrained, in\n7Table 2: Tour length of the Ptr-Net and a collection of algorithms on a small scale TSP problem.\nn OPTIMAL A1 A2 A3 P TR-NET\n5 2.12 2.18 2.12 2.12 2.12\n10 2.87 3.07 2.87 2.87 2.88\n50 (A1 TRAINED ) N/A 6.46 5.84 5.79 6.42\n50 (A3 TRAINED ) N/A 6.46 5.84 5.79 6.09\n5 (5-20 TRAINED ) 2.12 2.18 2.12 2.12 2.12\n10 (5-20 TRAINED ) 2.87 3.07 2.87 2.87 2.87\n20 (5-20 TRAINED ) 3.83 4.24 3.86 3.85 3.88\n25 (5-20 TRAINED ) N/A 4.71 4.27 4.24 4.30\n30 (5-20 TRAINED ) N/A 5.11 4.63 4.60 4.72\n40 (5-20 TRAINED ) N/A 5.82 5.27 5.23 5.91\n50 (5-20 TRAINED ) N/A 6.46 5.84 5.79 7.66\nthis example we set the beam search procedure to only consider valid tours. Otherwise, the Ptr-Net\nmodel would sometimes output an invalid tour – for instance, it would repeat two cities or decided\nto ignore a destination. This procedure was relevant for n > 20, where at least 10% of instances\nwould not produce any valid tour.\nThe ﬁrst group of rows in the table show the Ptr-Net trained on optimal data, except for n= 50 ,\nsince that is not feasible computationally (we trained a separate model for each n). Interestingly,\nwhen using the worst algorithm (A1) data to train the Ptr-Net, our model outperforms the algorithm\nthat is trying to imitate.\nThe second group of rows in the table show how the Ptr-Net trained on optimal data with 5 to\n20 cities can generalize beyond that. The results are virtually perfect for n= 25 , and good for\nn= 30 , but it seems to break for 40 and beyond (still, the results are far better than chance). This\ncontrasts with the convex hull case, where we were able to generalize by a factor of 10. However,\nthe underlying algorithms are of far greater complexity than O(nlogn), which could explain this\nphenomenon.\n5 Conclusions\nIn this paper we described Ptr-Net, a new architecture that allows us to learn a conditional prob-\nability of one sequence CPgiven another sequence P, whereCPis a sequence of discrete tokens\ncorresponding to positions in P. We show that Ptr-Nets can be used to learn solutions to three dif-\nferent combinatorial optimization problems. Our method works on variable sized inputs (yielding\nvariable sized output dictionaries), something the baseline models (sequence-to-sequence with or\nwithout attention) cannot do directly. Even more impressively, they outperform the baselines on\nﬁxed input size problems - to which both the models can be applied.\nPrevious methods such as RNNSearch, Memory Networks and Neural Turing Machines [5, 6, 2]\nhave used attention mechanisms to process inputs. However these methods do not directly address\nproblems that arise with variable output dictionaries. We have shown that an attention mechanism\ncan be applied to the output to solve such problems. In so doing, we have opened up a new class\nof problems to which neural networks can be applied without artiﬁcial assumptions. In this paper,\nwe have applied this extension to RNNSearch, but the methods are equally applicable to Memory\nNetworks and Neural Turing Machines.\nFuture work will try and show its applicability to other problems such as sorting where the outputs\nare chosen from the inputs. We are also excited about the possibility of using this approach to other\ncombinatorial optimization problems.\nAcknowledgments\nWe would like to thank Rafal Jozefowicz, Ilya Sutskever, Quoc Le and Samy Bengio for useful\ndiscussions on this topic. We would also like to thank Daniel Gillick for his help with the ﬁnal\nmanuscript.\n8References\n[1] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\n[2] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint\narXiv:1410.5401 , 2014.\n[3] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representa-\ntions by error propagation. Technical report, DTIC Document, 1985.\n[4] Anthony J Robinson. An application of recurrent nets to phone probability estimation. Neural\nNetworks, IEEE Transactions on , 5(2):298–305, 1994.\n[5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\njointly learning to align and translate. In ICLR 2015, arXiv preprint arXiv:1409.0473 , 2014.\n[6] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In ICLEAR 2015, arXiv\npreprint arXiv:1410.3916 , 2014.\n[7] Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton.\nGrammar as a foreign language. arXiv preprint arXiv:1412.7449 , 2014.\n[8] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural\nimage caption generator. In CVPR 2015, arXiv preprint arXiv:1411.4555 , 2014.\n[9] Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venu-\ngopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for\nvisual recognition and description. In CVPR 2015, arXiv preprint arXiv:1411.4389 , 2014.\n[10] Wojciech Zaremba and Ilya Sutskever. Learning to execute. arXiv preprint arXiv:1410.4615 ,\n2014.\n[11] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation ,\n9(8):1735–1780, 1997.\n[12] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhutdinov. Unsupervised learning of\nvideo representations using lstms. In ICML 2015, arXiv preprint arXiv:1502.04681 , 2015.\n[13] Ray A Jarvis. On the identiﬁcation of the convex hull of a ﬁnite set of points in the plane.\nInformation Processing Letters , 2(1):18–21, 1973.\n[14] Ronald L. Graham. An efﬁcient algorith for determining the convex hull of a ﬁnite planar set.\nInformation processing letters , 1(4):132–133, 1972.\n[15] Franco P. Preparata and Se June Hong. Convex hulls of ﬁnite sets of points in two and three\ndimensions. Communications of the ACM , 20(2):87–93, 1977.\n[16] S1 Rebay. Efﬁcient unstructured mesh generation by means of delaunay triangulation and\nbowyer-watson algorithm. Journal of computational physics , 106(1):125–138, 1993.\n[17] Richard Bellman. Dynamic programming treatment of the travelling salesman problem. Jour-\nnal of the ACM (JACM) , 9(1):61–63, 1962.\n[18] Suboptimal travelling salesman problem (tsp) solver, 2015. Available at\nhttps://github.com/dmishin/tsp-solver.\n[19] Traveling salesman problem c++ implementation, 2015. Available at\nhttps://github.com/samlbest/traveling-salesman.\n[20] C++ implementation of traveling salesman problem using christoﬁdes and 2-opt, 2015. Avail-\nable at https://github.com/beckysag/traveling-salesman.\n9\n\n\n\nImageNet Classiﬁcation with Deep Convolutional\nNeural Networks\nAlex Krizhevsky\nUniversity of Toronto\nkriz@cs.utoronto.caIlya Sutskever\nUniversity of Toronto\nilya@cs.utoronto.caGeoffrey E. Hinton\nUniversity of Toronto\nhinton@cs.utoronto.ca\nAbstract\nWe trained a large, deep convolutional neural network to classify the 1.2 million\nhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-\nferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%\nand 17.0% which is considerably better than the previous state-of-the-art. The\nneural network, which has 60 million parameters and 650,000 neurons, consists\nof ﬁve convolutional layers, some of which are followed by max-pooling layers,\nand three fully-connected layers with a ﬁnal 1000-way softmax. To make train-\ning faster, we used non-saturating neurons and a very efﬁcient GPU implemen-\ntation of the convolution operation. To reduce overﬁtting in the fully-connected\nlayers we employed a recently-developed regularization method called “dropout”\nthat proved to be very effective. We also entered a variant of this model in the\nILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%,\ncompared to 26.2% achieved by the second-best entry.\n1 Introduction\nCurrent approaches to object recognition make essential use of machine learning methods. To im-\nprove their performance, we can collect larger datasets, learn more powerful models, and use bet-\nter techniques for preventing overﬁtting. Until recently, datasets of labeled images were relatively\nsmall — on the order of tens of thousands of images (e.g., NORB [16], Caltech-101/256 [8, 9], and\nCIFAR-10/100 [12]). Simple recognition tasks can be solved quite well with datasets of this size,\nespecially if they are augmented with label-preserving transformations. For example, the current-\nbest error rate on the MNIST digit-recognition task (<0.3%) approaches human performance [4].\nBut objects in realistic settings exhibit considerable variability, so to learn to recognize them it is\nnecessary to use much larger training sets. And indeed, the shortcomings of small image datasets\nhave been widely recognized (e.g., Pinto et al. [21]), but it has only recently become possible to col-\nlect labeled datasets with millions of images. The new larger datasets include LabelMe [23], which\nconsists of hundreds of thousands of fully-segmented images, and ImageNet [6], which consists of\nover 15 million labeled high-resolution images in over 22,000 categories.\nTo learn about thousands of objects from millions of images, we need a model with a large learning\ncapacity. However, the immense complexity of the object recognition task means that this prob-\nlem cannot be speciﬁed even by a dataset as large as ImageNet, so our model should also have lots\nof prior knowledge to compensate for all the data we don’t have. Convolutional neural networks\n(CNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26]. Their capacity can be con-\ntrolled by varying their depth and breadth, and they also make strong and mostly correct assumptions\nabout the nature of images (namely, stationarity of statistics and locality of pixel dependencies).\nThus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have\nmuch fewer connections and parameters and so they are easier to train, while their theoretically-best\nperformance is likely to be only slightly worse.\n1Despite the attractive qualities of CNNs, and despite the relative efﬁciency of their local architecture,\nthey have still been prohibitively expensive to apply in large scale to high-resolution images. Luck-\nily, current GPUs, paired with a highly-optimized implementation of 2D convolution, are powerful\nenough to facilitate the training of interestingly-large CNNs, and recent datasets such as ImageNet\ncontain enough labeled examples to train such models without severe overﬁtting.\nThe speciﬁc contributions of this paper are as follows: we trained one of the largest convolutional\nneural networks to date on the subsets of ImageNet used in the ILSVRC-2010 and ILSVRC-2012\ncompetitions [2] and achieved by far the best results ever reported on these datasets. We wrote a\nhighly-optimized GPU implementation of 2D convolution and all the other operations inherent in\ntraining convolutional neural networks, which we make available publicly1. Our network contains\na number of new and unusual features which improve its performance and reduce its training time,\nwhich are detailed in Section 3. The size of our network made overﬁtting a signiﬁcant problem, even\nwith 1.2 million labeled training examples, so we used several effective techniques for preventing\noverﬁtting, which are described in Section 4. Our ﬁnal network contains ﬁve convolutional and\nthree fully-connected layers, and this depth seems to be important: we found that removing any\nconvolutional layer (each of which contains no more than 1% of the model’s parameters) resulted in\ninferior performance.\nIn the end, the network’s size is limited mainly by the amount of memory available on current GPUs\nand by the amount of training time that we are willing to tolerate. Our network takes between ﬁve\nand six days to train on two GTX 580 3GB GPUs. All of our experiments suggest that our results\ncan be improved simply by waiting for faster GPUs and bigger datasets to become available.\n2 The Dataset\nImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000\ncategories. The images were collected from the web and labeled by human labelers using Ama-\nzon’s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object\nChallenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge\n(ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of\n1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and\n150,000 testing images.\nILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so this is\nthe version on which we performed most of our experiments. Since we also entered our model in\nthe ILSVRC-2012 competition, in Section 6 we report our results on this version of the dataset as\nwell, for which test set labels are unavailable. On ImageNet, it is customary to report two error rates:\ntop-1 and top-5, where the top-5 error rate is the fraction of test images for which the correct label\nis not among the ﬁve labels considered most probable by the model.\nImageNet consists of variable-resolution images, while our system requires a constant input dimen-\nsionality. Therefore, we down-sampled the images to a ﬁxed resolution of 256\u0002256. Given a\nrectangular image, we ﬁrst rescaled the image such that the shorter side was of length 256, and then\ncropped out the central 256\u0002256patch from the resulting image. We did not pre-process the images\nin any other way, except for subtracting the mean activity over the training set from each pixel. So\nwe trained our network on the (centered) raw RGB values of the pixels.\n3 The Architecture\nThe architecture of our network is summarized in Figure 2. It contains eight learned layers —\nﬁve convolutional and three fully-connected. Below, we describe some of the novel or unusual\nfeatures of our network’s architecture. Sections 3.1-3.4 are sorted according to our estimation of\ntheir importance, with the most important ﬁrst.\n1http://code.google.com/p/cuda-convnet/\n23.1 ReLU Nonlinearity\nFigure 1: A four-layer convolutional neural\nnetwork with ReLUs (solid line) reaches a 25%\ntraining error rate on CIFAR-10 six times faster\nthan an equivalent network with tanh neurons\n(dashed line) . The learning rates for each net-\nwork were chosen independently to make train-\ning as fast as possible. No regularization of\nany kind was employed. The magnitude of the\neffect demonstrated here varies with network\narchitecture, but networks with ReLUs consis-\ntently learn several times faster than equivalents\nwith saturating neurons.The standard way to model a neuron’s output fas\na function of its input xis withf(x) = tanh(x)\norf(x) = (1 +e\u0000x)\u00001. In terms of training time\nwith gradient descent, these saturating nonlinearities\nare much slower than the non-saturating nonlinearity\nf(x) = max(0;x). Following Nair and Hinton [20],\nwe refer to neurons with this nonlinearity as Rectiﬁed\nLinear Units (ReLUs). Deep convolutional neural net-\nworks with ReLUs train several times faster than their\nequivalents with tanh units. This is demonstrated in\nFigure 1, which shows the number of iterations re-\nquired to reach 25% training error on the CIFAR-10\ndataset for a particular four-layer convolutional net-\nwork. This plot shows that we would not have been\nable to experiment with such large neural networks for\nthis work if we had used traditional saturating neuron\nmodels.\nWe are not the ﬁrst to consider alternatives to tradi-\ntional neuron models in CNNs. For example, Jarrett\net al. [11] claim that the nonlinearity f(x) =jtanh(x)j\nworks particularly well with their type of contrast nor-\nmalization followed by local average pooling on the\nCaltech-101 dataset. However, on this dataset the pri-\nmary concern is preventing overﬁtting, so the effect\nthey are observing is different from the accelerated\nability to ﬁt the training set which we report when us-\ning ReLUs. Faster learning has a great inﬂuence on the\nperformance of large models trained on large datasets.\n3.2 Training on Multiple GPUs\nA single GTX 580 GPU has only 3GB of memory, which limits the maximum size of the networks\nthat can be trained on it. It turns out that 1.2 million training examples are enough to train networks\nwhich are too big to ﬁt on one GPU. Therefore we spread the net across two GPUs. Current GPUs\nare particularly well-suited to cross-GPU parallelization, as they are able to read from and write to\none another’s memory directly, without going through host machine memory. The parallelization\nscheme that we employ essentially puts half of the kernels (or neurons) on each GPU, with one\nadditional trick: the GPUs communicate only in certain layers. This means that, for example, the\nkernels of layer 3 take input from all kernel maps in layer 2. However, kernels in layer 4 take input\nonly from those kernel maps in layer 3 which reside on the same GPU. Choosing the pattern of\nconnectivity is a problem for cross-validation, but this allows us to precisely tune the amount of\ncommunication until it is an acceptable fraction of the amount of computation.\nThe resultant architecture is somewhat similar to that of the “columnar” CNN employed by Cire¸ san\net al. [5], except that our columns are not independent (see Figure 2). This scheme reduces our top-1\nand top-5 error rates by 1.7% and 1.2%, respectively, as compared with a net with half as many\nkernels in each convolutional layer trained on one GPU. The two-GPU net takes slightly less time\nto train than the one-GPU net2.\n2The one-GPU net actually has the same number of kernels as the two-GPU net in the ﬁnal convolutional\nlayer. This is because most of the net’s parameters are in the ﬁrst fully-connected layer, which takes the last\nconvolutional layer as input. So to make the two nets have approximately the same number of parameters, we\ndid not halve the size of the ﬁnal convolutional layer (nor the fully-conneced layers which follow). Therefore\nthis comparison is biased in favor of the one-GPU net, since it is bigger than “half the size” of the two-GPU\nnet.\n33.3 Local Response Normalization\nReLUs have the desirable property that they do not require input normalization to prevent them\nfrom saturating. If at least some training examples produce a positive input to a ReLU, learning will\nhappen in that neuron. However, we still ﬁnd that the following local normalization scheme aids\ngeneralization. Denoting by ai\nx;ythe activity of a neuron computed by applying kernel iat position\n(x;y)and then applying the ReLU nonlinearity, the response-normalized activity bi\nx;yis given by\nthe expression\nbi\nx;y=ai\nx;y=0\n@k+\u000bmin(N\u00001;i+n=2)X\nj=max(0;i\u0000n=2)(aj\nx;y)21\nA\f\nwhere the sum runs over n“adjacent” kernel maps at the same spatial position, and Nis the total\nnumber of kernels in the layer. The ordering of the kernel maps is of course arbitrary and determined\nbefore training begins. This sort of response normalization implements a form of lateral inhibition\ninspired by the type found in real neurons, creating competition for big activities amongst neuron\noutputs computed using different kernels. The constants k;n;\u000b , and\fare hyper-parameters whose\nvalues are determined using a validation set; we used k= 2,n= 5,\u000b= 10\u00004, and\f= 0:75. We\napplied this normalization after applying the ReLU nonlinearity in certain layers (see Section 3.5).\nThis scheme bears some resemblance to the local contrast normalization scheme of Jarrett et al. [11],\nbut ours would be more correctly termed “brightness normalization”, since we do not subtract the\nmean activity. Response normalization reduces our top-1 and top-5 error rates by 1.4% and 1.2%,\nrespectively. We also veriﬁed the effectiveness of this scheme on the CIFAR-10 dataset: a four-layer\nCNN achieved a 13% test error rate without normalization and 11% with normalization3.\n3.4 Overlapping Pooling\nPooling layers in CNNs summarize the outputs of neighboring groups of neurons in the same kernel\nmap. Traditionally, the neighborhoods summarized by adjacent pooling units do not overlap (e.g.,\n[17, 11, 4]). To be more precise, a pooling layer can be thought of as consisting of a grid of pooling\nunits spaced spixels apart, each summarizing a neighborhood of size z\u0002zcentered at the location\nof the pooling unit. If we set s=z, we obtain traditional local pooling as commonly employed\nin CNNs. If we set s < z , we obtain overlapping pooling. This is what we use throughout our\nnetwork, with s= 2 andz= 3. This scheme reduces the top-1 and top-5 error rates by 0.4% and\n0.3%, respectively, as compared with the non-overlapping scheme s= 2;z= 2, which produces\noutput of equivalent dimensions. We generally observe during training that models with overlapping\npooling ﬁnd it slightly more difﬁcult to overﬁt.\n3.5 Overall Architecture\nNow we are ready to describe the overall architecture of our CNN. As depicted in Figure 2, the net\ncontains eight layers with weights; the ﬁrst ﬁve are convolutional and the remaining three are fully-\nconnected. The output of the last fully-connected layer is fed to a 1000-way softmax which produces\na distribution over the 1000 class labels. Our network maximizes the multinomial logistic regression\nobjective, which is equivalent to maximizing the average across training cases of the log-probability\nof the correct label under the prediction distribution.\nThe kernels of the second, fourth, and ﬁfth convolutional layers are connected only to those kernel\nmaps in the previous layer which reside on the same GPU (see Figure 2). The kernels of the third\nconvolutional layer are connected to all kernel maps in the second layer. The neurons in the fully-\nconnected layers are connected to all neurons in the previous layer. Response-normalization layers\nfollow the ﬁrst and second convolutional layers. Max-pooling layers, of the kind described in Section\n3.4, follow both response-normalization layers as well as the ﬁfth convolutional layer. The ReLU\nnon-linearity is applied to the output of every convolutional and fully-connected layer.\nThe ﬁrst convolutional layer ﬁlters the 224\u0002224\u00023input image with 96 kernels of size 11\u000211\u00023\nwith a stride of 4 pixels (this is the distance between the receptive ﬁeld centers of neighboring\n3We cannot describe this network in detail due to space constraints, but it is speciﬁed precisely by the code\nand parameter ﬁles provided here: http://code.google.com/p/cuda-convnet/.\n4Figure 2: An illustration of the architecture of our CNN, explicitly showing the delineation of responsibilities\nbetween the two GPUs. One GPU runs the layer-parts at the top of the ﬁgure while the other runs the layer-parts\nat the bottom. The GPUs communicate only at certain layers. The network’s input is 150,528-dimensional, and\nthe number of neurons in the network’s remaining layers is given by 253,440–186,624–64,896–64,896–43,264–\n4096–4096–1000.\nneurons in a kernel map). The second convolutional layer takes as input the (response-normalized\nand pooled) output of the ﬁrst convolutional layer and ﬁlters it with 256 kernels of size 5\u00025\u000248.\nThe third, fourth, and ﬁfth convolutional layers are connected to one another without any intervening\npooling or normalization layers. The third convolutional layer has 384 kernels of size 3\u00023\u0002\n256connected to the (normalized, pooled) outputs of the second convolutional layer. The fourth\nconvolutional layer has 384 kernels of size 3\u00023\u0002192, and the ﬁfth convolutional layer has 256\nkernels of size 3\u00023\u0002192. The fully-connected layers have 4096 neurons each.\n4 Reducing Overﬁtting\nOur neural network architecture has 60 million parameters. Although the 1000 classes of ILSVRC\nmake each training example impose 10 bits of constraint on the mapping from image to label, this\nturns out to be insufﬁcient to learn so many parameters without considerable overﬁtting. Below, we\ndescribe the two primary ways in which we combat overﬁtting.\n4.1 Data Augmentation\nThe easiest and most common method to reduce overﬁtting on image data is to artiﬁcially enlarge\nthe dataset using label-preserving transformations (e.g., [25, 4, 5]). We employ two distinct forms\nof data augmentation, both of which allow transformed images to be produced from the original\nimages with very little computation, so the transformed images do not need to be stored on disk.\nIn our implementation, the transformed images are generated in Python code on the CPU while the\nGPU is training on the previous batch of images. So these data augmentation schemes are, in effect,\ncomputationally free.\nThe ﬁrst form of data augmentation consists of generating image translations and horizontal reﬂec-\ntions. We do this by extracting random 224\u0002224patches (and their horizontal reﬂections) from the\n256\u0002256images and training our network on these extracted patches4. This increases the size of our\ntraining set by a factor of 2048, though the resulting training examples are, of course, highly inter-\ndependent. Without this scheme, our network suffers from substantial overﬁtting, which would have\nforced us to use much smaller networks. At test time, the network makes a prediction by extracting\nﬁve224\u0002224patches (the four corner patches and the center patch) as well as their horizontal\nreﬂections (hence ten patches in all), and averaging the predictions made by the network’s softmax\nlayer on the ten patches.\nThe second form of data augmentation consists of altering the intensities of the RGB channels in\ntraining images. Speciﬁcally, we perform PCA on the set of RGB pixel values throughout the\nImageNet training set. To each training image, we add multiples of the found principal components,\n4This is the reason why the input images in Figure 2 are 224\u0002224\u00023-dimensional.\n5with magnitudes proportional to the corresponding eigenvalues times a random variable drawn from\na Gaussian with mean zero and standard deviation 0.1. Therefore to each RGB image pixel Ixy=\n[IR\nxy;IG\nxy;IB\nxy]Twe add the following quantity:\n[p1;p2;p3][\u000b1\u00151;\u000b2\u00152;\u000b3\u00153]T\nwhere piand\u0015iareith eigenvector and eigenvalue of the 3\u00023covariance matrix of RGB pixel\nvalues, respectively, and \u000biis the aforementioned random variable. Each \u000biis drawn only once\nfor all the pixels of a particular training image until that image is used for training again, at which\npoint it is re-drawn. This scheme approximately captures an important property of natural images,\nnamely, that object identity is invariant to changes in the intensity and color of the illumination. This\nscheme reduces the top-1 error rate by over 1%.\n4.2 Dropout\nCombining the predictions of many different models is a very successful way to reduce test errors\n[1, 3], but it appears to be too expensive for big neural networks that already take several days\nto train. There is, however, a very efﬁcient version of model combination that only costs about a\nfactor of two during training. The recently-introduced technique, called “dropout” [10], consists\nof setting to zero the output of each hidden neuron with probability 0.5. The neurons which are\n“dropped out” in this way do not contribute to the forward pass and do not participate in back-\npropagation. So every time an input is presented, the neural network samples a different architecture,\nbut all these architectures share weights. This technique reduces complex co-adaptations of neurons,\nsince a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to\nlearn more robust features that are useful in conjunction with many different random subsets of the\nother neurons. At test time, we use all the neurons but multiply their outputs by 0.5, which is a\nreasonable approximation to taking the geometric mean of the predictive distributions produced by\nthe exponentially-many dropout networks.\nWe use dropout in the ﬁrst two fully-connected layers of Figure 2. Without dropout, our network ex-\nhibits substantial overﬁtting. Dropout roughly doubles the number of iterations required to converge.\nFigure 3: 96 convolutional kernels of size\n11\u000211\u00023learned by the ﬁrst convolutional\nlayer on the 224\u0002224\u00023input images. The\ntop 48 kernels were learned on GPU 1 while\nthe bottom 48 kernels were learned on GPU\n2. See Section 6.1 for details.5 Details of learning\nWe trained our models using stochastic gradient descent\nwith a batch size of 128 examples, momentum of 0.9, and\nweight decay of 0.0005. We found that this small amount\nof weight decay was important for the model to learn. In\nother words, weight decay here is not merely a regularizer:\nit reduces the model’s training error. The update rule for\nweightwwas\nvi+1 := 0:9\u0001vi\u00000:0005\u0001\u000f\u0001wi\u0000\u000f\u0001\u001c@L\n@w\f\f\nwi\u001d\nDi\nwi+1 :=wi+vi+1\nwhereiis the iteration index, vis the momentum variable, \u000fis the learning rate, andD\n@L\n@w\f\f\nwiE\nDiis\nthe average over the ith batchDiof the derivative of the objective with respect to w, evaluated at\nwi.\nWe initialized the weights in each layer from a zero-mean Gaussian distribution with standard de-\nviation 0.01. We initialized the neuron biases in the second, fourth, and ﬁfth convolutional layers,\nas well as in the fully-connected hidden layers, with the constant 1. This initialization accelerates\nthe early stages of learning by providing the ReLUs with positive inputs. We initialized the neuron\nbiases in the remaining layers with the constant 0.\nWe used an equal learning rate for all layers, which we adjusted manually throughout training.\nThe heuristic which we followed was to divide the learning rate by 10 when the validation error\nrate stopped improving with the current learning rate. The learning rate was initialized at 0.01 and\n6reduced three times prior to termination. We trained the network for roughly 90 cycles through the\ntraining set of 1.2 million images, which took ﬁve to six days on two NVIDIA GTX 580 3GB GPUs.\n6 Results\nOur results on ILSVRC-2010 are summarized in Table 1. Our network achieves top-1 and top-5\ntest set error rates of 37.5% and17.0%5. The best performance achieved during the ILSVRC-\n2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced\nfrom six sparse-coding models trained on different features [2], and since then the best pub-\nlished results are 45.7% and 25.7% with an approach that averages the predictions of two classi-\nﬁers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features [24].\nModel Top-1 Top-5\nSparse coding [2] 47.1% 28.2%\nSIFT + FVs [24] 45.7% 25.7%\nCNN 37.5% 17.0%\nTable 1: Comparison of results on ILSVRC-\n2010 test set. In italics are best results\nachieved by others.We also entered our model in the ILSVRC-2012 com-\npetition and report our results in Table 2. Since the\nILSVRC-2012 test set labels are not publicly available,\nwe cannot report test error rates for all the models that\nwe tried. In the remainder of this paragraph, we use\nvalidation and test error rates interchangeably because\nin our experience they do not differ by more than 0.1%\n(see Table 2). The CNN described in this paper achieves\na top-5 error rate of 18.2%. Averaging the predictions\nof ﬁve similar CNNs gives an error rate of 16.4%. Training one CNN, with an extra sixth con-\nvolutional layer over the last pooling layer, to classify the entire ImageNet Fall 2011 release\n(15M images, 22K categories), and then “ﬁne-tuning” it on ILSVRC-2012 gives an error rate of\n16.6%. Averaging the predictions of two CNNs that were pre-trained on the entire Fall 2011 re-\nlease with the aforementioned ﬁve CNNs gives an error rate of 15.3% . The second-best con-\ntest entry achieved an error rate of 26.2% with an approach that averages the predictions of sev-\neral classiﬁers trained on FVs computed from different types of densely-sampled features [7].\nModel Top-1 (val) Top-5 (val) Top-5 (test)\nSIFT + FVs [7] — — 26.2%\n1 CNN 40.7% 18.2% —\n5 CNNs 38.1% 16.4% 16.4%\n1 CNN* 39.0% 16.6% —\n7 CNNs* 36.7% 15.4% 15.3%\nTable 2: Comparison of error rates on ILSVRC-2012 validation and\ntest sets. In italics are best results achieved by others. Models with an\nasterisk* were “pre-trained” to classify the entire ImageNet 2011 Fall\nrelease. See Section 6 for details.Finally, we also report our error\nrates on the Fall 2009 version of\nImageNet with 10,184 categories\nand 8.9 million images. On this\ndataset we follow the convention\nin the literature of using half of\nthe images for training and half\nfor testing. Since there is no es-\ntablished test set, our split neces-\nsarily differs from the splits used\nby previous authors, but this does\nnot affect the results appreciably.\nOur top-1 and top-5 error rates\non this dataset are 67.4% and\n40.9% , attained by the net described above but with an additional, sixth convolutional layer over the\nlast pooling layer. The best published results on this dataset are 78.1% and 60.9% [19].\n6.1 Qualitative Evaluations\nFigure 3 shows the convolutional kernels learned by the network’s two data-connected layers. The\nnetwork has learned a variety of frequency- and orientation-selective kernels, as well as various col-\nored blobs. Notice the specialization exhibited by the two GPUs, a result of the restricted connec-\ntivity described in Section 3.5. The kernels on GPU 1 are largely color-agnostic, while the kernels\non on GPU 2 are largely color-speciﬁc. This kind of specialization occurs during every run and is\nindependent of any particular random weight initialization (modulo a renumbering of the GPUs).\n5The error rates without averaging predictions over ten patches as described in Section 4.1 are 39.0% and\n18.3%.\n7Figure 4: (Left) Eight ILSVRC-2010 test images and the ﬁve labels considered most probable by our model.\nThe correct label is written under each image, and the probability assigned to the correct label is also shown\nwith a red bar (if it happens to be in the top 5). (Right) Five ILSVRC-2010 test images in the ﬁrst column. The\nremaining columns show the six training images that produce feature vectors in the last hidden layer with the\nsmallest Euclidean distance from the feature vector for the test image.\nIn the left panel of Figure 4 we qualitatively assess what the network has learned by computing its\ntop-5 predictions on eight test images. Notice that even off-center objects, such as the mite in the\ntop-left, can be recognized by the net. Most of the top-5 labels appear reasonable. For example,\nonly other types of cat are considered plausible labels for the leopard. In some cases (grille, cherry)\nthere is genuine ambiguity about the intended focus of the photograph.\nAnother way to probe the network’s visual knowledge is to consider the feature activations induced\nby an image at the last, 4096-dimensional hidden layer. If two images produce feature activation\nvectors with a small Euclidean separation, we can say that the higher levels of the neural network\nconsider them to be similar. Figure 4 shows ﬁve images from the test set and the six images from\nthe training set that are most similar to each of them according to this measure. Notice that at the\npixel level, the retrieved training images are generally not close in L2 to the query images in the ﬁrst\ncolumn. For example, the retrieved dogs and elephants appear in a variety of poses. We present the\nresults for many more test images in the supplementary material.\nComputing similarity by using Euclidean distance between two 4096-dimensional, real-valued vec-\ntors is inefﬁcient, but it could be made efﬁcient by training an auto-encoder to compress these vectors\nto short binary codes. This should produce a much better image retrieval method than applying auto-\nencoders to the raw pixels [14], which does not make use of image labels and hence has a tendency\nto retrieve images with similar patterns of edges, whether or not they are semantically similar.\n7 Discussion\nOur results show that a large, deep convolutional neural network is capable of achieving record-\nbreaking results on a highly challenging dataset using purely supervised learning. It is notable\nthat our network’s performance degrades if a single convolutional layer is removed. For example,\nremoving any of the middle layers results in a loss of about 2% for the top-1 performance of the\nnetwork. So the depth really is important for achieving our results.\nTo simplify our experiments, we did not use any unsupervised pre-training even though we expect\nthat it will help, especially if we obtain enough computational power to signiﬁcantly increase the\nsize of the network without obtaining a corresponding increase in the amount of labeled data. Thus\nfar, our results have improved as we have made our network larger and trained it longer but we still\nhave many orders of magnitude to go in order to match the infero-temporal pathway of the human\nvisual system. Ultimately we would like to use very large and deep convolutional nets on video\nsequences where the temporal structure provides very helpful information that is missing or far less\nobvious in static images.\n8References\n[1] R.M. Bell and Y . Koren. Lessons from the netﬂix prize challenge. ACM SIGKDD Explorations Newsletter ,\n9(2):75–79, 2007.\n[2] A. Berg, J. Deng, and L. Fei-Fei. Large scale visual recognition challenge 2010. www.image-\nnet.org/challenges. 2010.\n[3] L. Breiman. Random forests. Machine learning , 45(1):5–32, 2001.\n[4] D. Cire¸ san, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classiﬁcation.\nArxiv preprint arXiv:1202.2745 , 2012.\n[5] D.C. Cire¸ san, U. Meier, J. Masci, L.M. Gambardella, and J. Schmidhuber. High-performance neural\nnetworks for visual object classiﬁcation. Arxiv preprint arXiv:1102.0183 , 2011.\n[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical\nImage Database. In CVPR09 , 2009.\n[7] J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-Fei. ILSVRC-2012 , 2012. URL\nhttp://www.image-net.org/challenges/LSVRC/2012/ .\n[8] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An\nincremental bayesian approach tested on 101 object categories. Computer Vision and Image Understand-\ning, 106(1):59–70, 2007.\n[9] G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 object category dataset. Technical Report 7694, Cali-\nfornia Institute of Technology, 2007. URL http://authors.library.caltech.edu/7694 .\n[10] G.E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R.R. Salakhutdinov. Improving neural net-\nworks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 , 2012.\n[11] K. Jarrett, K. Kavukcuoglu, M. A. Ranzato, and Y . LeCun. What is the best multi-stage architecture for\nobject recognition? In International Conference on Computer Vision , pages 2146–2153. IEEE, 2009.\n[12] A. Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, Department of\nComputer Science, University of Toronto, 2009.\n[13] A. Krizhevsky. Convolutional deep belief networks on cifar-10. Unpublished manuscript , 2010.\n[14] A. Krizhevsky and G.E. Hinton. Using very deep autoencoders for content-based image retrieval. In\nESANN , 2011.\n[15] Y . Le Cun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, L.D. Jackel, et al. Hand-\nwritten digit recognition with a back-propagation network. In Advances in neural information processing\nsystems , 1990.\n[16] Y . LeCun, F.J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to\npose and lighting. In Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the\n2004 IEEE Computer Society Conference on , volume 2, pages II–97. IEEE, 2004.\n[17] Y . LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional networks and applications in vision. In\nCircuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on , pages 253–256.\nIEEE, 2010.\n[18] H. Lee, R. Grosse, R. Ranganath, and A.Y . Ng. Convolutional deep belief networks for scalable unsuper-\nvised learning of hierarchical representations. In Proceedings of the 26th Annual International Conference\non Machine Learning , pages 609–616. ACM, 2009.\n[19] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Metric Learning for Large Scale Image Classiﬁ-\ncation: Generalizing to New Classes at Near-Zero Cost. In ECCV - European Conference on Computer\nVision , Florence, Italy, October 2012.\n[20] V . Nair and G. E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In Proc. 27th\nInternational Conference on Machine Learning , 2010.\n[21] N. Pinto, D.D. Cox, and J.J. DiCarlo. Why is real-world visual object recognition hard? PLoS computa-\ntional biology , 4(1):e27, 2008.\n[22] N. Pinto, D. Doukhan, J.J. DiCarlo, and D.D. Cox. A high-throughput screening approach to discovering\ngood forms of biologically inspired visual representation. PLoS computational biology , 5(11):e1000579,\n2009.\n[23] B.C. Russell, A. Torralba, K.P. Murphy, and W.T. Freeman. Labelme: a database and web-based tool for\nimage annotation. International journal of computer vision , 77(1):157–173, 2008.\n[24] J. Sánchez and F. Perronnin. High-dimensional signature compression for large-scale image classiﬁcation.\nInComputer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on , pages 1665–1672. IEEE,\n2011.\n[25] P.Y . Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied to\nvisual document analysis. In Proceedings of the Seventh International Conference on Document Analysis\nand Recognition , volume 2, pages 958–962, 2003.\n[26] S.C. Turaga, J.F. Murray, V . Jain, F. Roth, M. Helmstaedter, K. Briggman, W. Denk, and H.S. Seung. Con-\nvolutional networks can learn to generate afﬁnity graphs for image segmentation. Neural Computation ,\n22(2):511–538, 2010.\n9\n\n\n\nPublished as a conference paper at ICLR 2016\nORDER MATTERS : SEQUENCE TO SEQUENCE FOR SETS\nOriol Vinyals, Samy Bengio, Manjunath Kudlur\nGoogle Brain\nfvinyals, bengio, keveman g@google.com\nABSTRACT\nSequences have become ﬁrst class citizens in supervised learning thanks to the\nresurgence of recurrent neural networks. Many complex tasks that require map-\nping from or to a sequence of observations can now be formulated with the\nsequence-to-sequence (seq2seq) framework which employs the chain rule to ef-\nﬁciently represent the joint probability of sequences. In many cases, however,\nvariable sized inputs and/or outputs might not be naturally expressed as sequences.\nFor instance, it is not clear how to input a set of numbers into a model where the\ntask is to sort them; similarly, we do not know how to organize outputs when\nthey correspond to random variables and the task is to model their unknown joint\nprobability. In this paper, we ﬁrst show using various examples that the order in\nwhich we organize input and/or output data matters signiﬁcantly when learning an\nunderlying model. We then discuss an extension of the seq2seq framework that\ngoes beyond sequences and handles input sets in a principled way. In addition,\nwe propose a loss which, by searching over possible orders during training, deals\nwith the lack of structure of output sets. We show empirical evidence of our claims\nregarding ordering, and on the modiﬁcations to the seq2seq framework on bench-\nmark language modeling and parsing tasks, as well as two artiﬁcial tasks – sorting\nnumbers and estimating the joint probability of unknown graphical models.\n1 I NTRODUCTION\nDeep architectures have shown in the last few years that they often yield state-of-the-art perfor-\nmance on several tasks, ranging from image classiﬁcation (Ioffe & Szegedy, 2015) to speech recog-\nnition (Hinton et al., 2012). More recently, recurrent neural networks (RNNs) and variants such as\nthe Long Short Term Memory network (LSTMs) proposed by Hochreiter & Schmidhuber (1997)\nhave shown similar impressive performance on several inherently sequential tasks. Such examples\nrange from machine translation (Sutskever et al., 2014; Bahdanau et al., 2015a), to image caption-\ning (Vinyals et al., 2015c; Mao et al., 2015; Donahue et al., 2015), speech recognition (Chan et al.,\n2015; Bahdanau et al., 2015b), constituency parsing (Vinyals et al., 2015b) and learning to com-\npute (Zaremba & Sutskever, 2014; Vinyals et al., 2015a). These approaches all follow a simple\narchitecture, dubbed sequence-to-sequence (seq2seq), where the input is read completely using an\nencoder, which is either an LSTM when the input is a sequence, or a convolutional network for\nimages. The ﬁnal state of the encoder is then fed to a decoder LSTM whose purpose is to produce\nthe target sequence, one token at a time.\nWhen the data is naturally organized as a sequence, the sequence-to-sequence framework is well\nsuited. For example, the chain rule is used to decompose the joint probability of sequences of words,\nand can be implemented by an LSTM without making any conditional independence assumption.\nBut how should we represent data, either inputs or outputs, for problems where an obvious order\ncannot be determined? For instance, how should we encode a set of numbers when the task is to sort\nthem? Alternatively, how should we output a set of detected objects in an image when there is no\nspeciﬁc known order among them? Does the a priori choice of ordering of the data to be presented\nto the model matter?\nThe purpose of this paper is two-fold. First, we show that even when no natural order is known\namong input or output objects, there might still be one that yields better performance, hence, order\nmatters . Second, we propose two approaches to consider sets either as inputs and/or outputs in our\nmodels and evaluate how they perform on various artiﬁcial and real datasets.\n1arXiv:1511.06391v4  [stat.ML]  23 Feb 2016Published as a conference paper at ICLR 2016\n2 R ELATED WORK\nSince sequence-to-sequence models were proposed for machine translation (Sutskever et al., 2014;\nCho et al., 2014; Kalchbrenner & Blunsom, 2013), the research community has proposed several\napplications in which these models can perform mappings from and/or to sequences. For example,\nimage captioning maps from an image to a sentence (Vinyals et al., 2015c; Mao et al., 2015; Donahue\net al., 2015), parsing maps from a sentence to a (linearized) parse tree (Vinyals et al., 2015b), and\nmodels for computation map from problem statements (e.g. a python program or a set of points\non the plane) to their solutions (the answer to the program (Zaremba & Sutskever, 2014), or the\ntraveling salesman problem tour for the set of points (Vinyals et al., 2015a)). It is out of the scope of\nthis paper to review all successful applications of seq2seq, but the list above already includes some\nnon-trivial examples of mapping to/from objects that are not necessarily sequences.\nMore recently, many related models and key contributions have been proposed, that utilize the con-\ncept of external memories, including RNNSearch (Bahdanau et al., 2015a), Memory Networks (We-\nston et al., 2015) and Neural Turing Machines (Graves et al., 2014). The key element that these\nmodels utilize is a reading (or attention) mechanism to read these external memories in a fully dif-\nferentiable way (though there has also been work with discrete reading mechanism, most notably\nRL-NTM (Zaremba & Sutskever, 2015)).\nUnlike traditional structured prediction algorithms (Bakir et al., 2007), our approach relies on the\nchain rule to serialize output random variables through the strong capabilities of LSTM networks\nto model long-term correlation. Similarly, we do not want to assume a known structured input, as\nis done for instance with recursive neural networks (Socher et al., 2010) which encode sentences\nrecursively as (given) trees.\n3 N EURAL NETWORKS FOR SEQUENCES AND SETS\nLet us consider a generic supervised task with a given training set of npairs (Xi;Yi)n\ni=1where\n(Xi;Yi)is theithpair of an input and its corresponding target. The sequence-to-sequence paradigm\ncorresponds to tasks where both XiandYiare represented by sequences, of possibly different\nlengths:Xi=fxi\n1;xi\n2;:::;xi\nsigandYi=fyi\n1;yi\n2;:::;yi\ntig. In this case, it is reasonable to model\neach example using the conditional probability P(YjX)and to use the chain rule to decompose it\nas follows (we drop the example index iin the rest of this section for readability):\nP(YjX) =TY\nt=1P(ytjy1;y2;:::;yt\u00001;X)\nand implement it as an encoder recurrent neural network (RNN) to read sequentially each xs2X\nas follows:\nhs=fenc(hs\u00001;xs) (1)\nwherehsis the state of the encoder at time s, followed by a decoder RNN to produce each yt2Y\none at a time, given the current state gtand the previous yt\u00001symbol:\ng1=hs\ngt=fdec(gt\u00001;yt\u00001)\nP(ytjy1;y2;:::;yt\u00001;X) =softmax (afﬁne (gt)): (2)\nThe use of the chain rule makes this approach assumption free, so when the input Xcorresponds\nto a sequence (like a sentence), it is reasonable to read it sequentially into an RNN, as in eq. (1).\nHowever, how should we encode Xif it does not correspond naturally to a sequence? For instance,\nwhat if it corresponds to an unordered set of elements?\nSimilarly, when the target Ycorresponds to a sequence, it is reasonable to produce it sequentially\nwith an RNN, as in eq. (2), but how should we produce Yif it does not correspond naturally to a\nsequence?\nNote that sequences can be encoded as sets. Indeed, if we associate to each element of a sequence the\nindex it occupies in it, forming a tuple, we effectively convert this sequence to a set. For example, the\n2Published as a conference paper at ICLR 2016\nsequence “I like cats” becomes the set f(I,1), (like,2), (cats,3) g(note that we can permute elements\nin the set but still recover the original sequence). Although this may be unnecessary in some cases,\nwe argue that, even for sequences, inputting and/or outputting them in a different order could be\nbeneﬁcial. For example, in sorting we may want to employ a divide-and-conquer strategy which\nﬁnds the median element ﬁrst (i.e., we may output the solution in neither increasing nor decreasing\nsequential order).\nIn the following two sections we discuss how to extend seq2seq to handle input sets (Section 4)\nand output sets (Section 5). We also show the importance of ordering in a variety of tasks in which\nseq2seq has successfully been applied, and include experimental results to support our claims and\nextensions to the existing models.\n4 I NPUT SETS\nWe ﬁrst study extensions to encoding (reading) sets. As we discussed in the previous section, se-\nquences can be read with a recurrent neural network which can compress its contents into a vector.\nAn important invariance property that must be satisﬁed when the input is a set (i.e., the order does\nnot matter) is that swapping two elements xiandxjin the setXshould not alter its encoding.\nA simple approach which satisﬁes this, and which in fact has been commonly used for encoding\nsentences, is the bag-of-words approach. In this case, the representation is simply a reduction (e.g.,\naddition) of counts, word embeddings, or similar embedding functions, and is naturally permutation\ninvariant. For language and other domains which are naturally sequential, this is replaced with more\ncomplex encoders such as recurrent neural networks that take order into account and model higher\norder statistics of the data.\nAn unsatisfying property of using a reduction operation (such as addition) is that it makes the rep-\nresentation quite inefﬁcient: the model operates over a ﬁxed dimensional embedding regardless of\nthe length of the set. It is unlikely that such representation will succeed, as the amount of memory\nrequired to encode a length Tset (or sequence, for that matter) should increase as a function of T.\nThus, we argue that even deep convolutional architectures will suffer from this limitation – though\nsome modiﬁcations exist (Maas et al., 2012).\nIn our work, we largely rely on attention mechanisms to integrate information from a variable length\nstructure, which we describe in Section 4.2.\n4.1 I NPUT ORDER MATTERS\nIn this section, we highlight prior work where we observed that the order of inputs impacted the per-\nformance of seq2seq models taking sequences as input. In principle, order should not matter when\nusing a complex encoder such as a recurrent neural network, as these are universal approximators\nthat can encode complex features from the input sequence (e.g., n-grams of any order). We believe\nthat the reason order seems to matter is due to the underlying non-convex optimization and more\nsuitable prior.\nThe ﬁrst example which we experimented with was altering the order of sequences in the context of\nmachine translation. In machine translation, the mapping function encodes a sentence in a source\nlanguage (e.g., English), and decodes it to its translation in a target language (e.g., French). By\nreversing the order of the input English sentence, Sutskever et al. (2014) got a 5.0 BLEU score\nimprovement which allowed them to close the gap between their model – a fully end-to-end model\nfor machine translation – and state-of-the-art models which were highly engineered. Similarly, for\nconstituency parsing, in which the mapping is from an English sentence to a ﬂattened version of\nits constituency parse tree, a 0.5% absolute increase in F1 score was observed when reversing the\nEnglish sentence (Vinyals et al., 2015b).\nFurthermore, if we preprocess the data for, e.g., convex hull computation that was presented in\nVinyals et al. (2015a) by sorting the points by angle, the task becomes simpler (from O(nlogn)to\nO(n)), and as a result the models obtained are much faster to train and better (increasing accuracy\nby up to 10% absolute in the most challenging cases).\n3Published as a conference paper at ICLR 2016\nAll these empirical ﬁndings point to the same story: often for optimization purposes, the order in\nwhich input data is shown to the model has an impact on the learning performance.\nNote that we can deﬁne an ordering which is independent of the input sequence or set X(e.g., always\nreversing the words in a translation task), but also an ordering which is input dependent (e.g., sorting\nthe input points in the convex hull case). This distinction also applies in the discussion about output\nsequences and sets in Section 5.1.\nRecent approaches which pushed the seq2seq paradigm further by adding memory and computation\nto these models allowed us to deﬁne a model which makes no assumptions about input ordering,\nwhilst preserving the right properties which we just discussed: a memory that increases with the\nsize of the set, and which is order invariant. In the next sections, we explain such a modiﬁcation,\nwhich could also be seen as a special case of a Memory Network (Weston et al., 2015) or Neural\nTuring Machine (Graves et al., 2014) – with a computation ﬂow as depicted in Figure 1.\n4.2 A TTENTION MECHANISMS\nNeural models with memories coupled to differentiable addressing mechanism have been success-\nfully applied to handwriting generation and recognition (Graves, 2012), machine translation (Bah-\ndanau et al., 2015a), and more general computation machines (Graves et al., 2014; Weston et al.,\n2015). Since we are interested in associative memories we employed a “content” based attention.\nThis has the property that the vector retrieved from our memory would not change if we randomly\nshufﬂed the memory. This is crucial for proper treatment of the input set Xas such. In particular,\nour process block based on an attention mechanism uses the following:\nqt=LSTM (q\u0003\nt\u00001) (3)\nei;t=f(mi;qt) (4)\nai;t=exp(ei;t)P\njexp(ej;t)(5)\nrt=X\niai;tmi (6)\nq\u0003\nt= [qtrt] (7)\nRead\nProcess\nWrite\nFigure 1: The Read-Process-and-Write model.\nwhereiindexes through each memory vector mi(typically equal to the cardinality of X),qtis\na query vector which allows us to read rtfrom the memories, fis a function that computes a\nsingle scalar from miandqt(e.g., a dot product), and LSTM is an LSTM which computes a\nrecurrent state but which takes no inputs. q\u0003\ntis the state which this LSTM evolves, and is formed\nby concatenating the query qtwith the resulting attention readout rt.tis the index which indicates\nhow many “processing steps” are being carried to compute the state to be fed to the decoder. Note\nthat permuting miandmi0has no effect on the read vector rt.\n4.3 R EAD, PROCESS , W RITE\nOur model, which naturally handles input sets, has three components (the exact equations and im-\nplementation will be released in an appendix prior to publication):\n• Areading block, which simply embeds each element xi2Xusing a small neural network\nonto a memory vector mi(the same neural network is used for all i).\n• Aprocess block, which is an LSTM without inputs or outputs performing Tsteps of com-\nputation over the memories mi. This LSTM keeps updating its state by reading mirepeat-\nedly using the attention mechanism described in the previous section. At the end of this\nblock, its hidden state q\u0003\nTis an embedding which is permutation invariant to the inputs. See\neqs. (3)-(7) for more details.\n4Published as a conference paper at ICLR 2016\n• Awrite block, which is an LSTM pointer network (Vinyals et al., 2015a) that takes in q\u0003\nT(as\nthe context it needs from which to produce the output from the input set), and points at ele-\nments ofmi(implicitly,xi), one step at a time. The original work in Vinyals et al. (2015a)\nused a pointer mechanism which, instead of issuing a readout of memory by a weighted\nsum with a soft pointer (see eq. 6), it uses the pointer as part of the loss. We extended this\nby adding an extra attention step before the pointer (we called this glimpse). This is related\nto the process block described above, but with the difference that the attention reads happen\ninterleaved between each pointer output. As described later in the results, we found these\ntwo mechanisms to complement each other.\nThe architecture is depicted in Figure 1 and can be seen as a special case of a Neural Turing Machine\nor Memory Network. It satisﬁes the key property of being invariant to the order of the elements in X,\nthus effectively processing the inputs as a set. Also note that the write component could simply be an\nLSTM if the outputs were from a ﬁxed dictionary. For this model, though, we study combinatorial\nproblems where the outputs are pointers to the inputs, so we use a pointer network.\n4.4 S ORTING EXPERIMENT\nIn order to verify if our model handles sets more efﬁciently than the vanilla seq2seq approach, we\nran the following experiment on artiﬁcial data for the task of sorting numbers: given Nunordered\nrandom ﬂoating point numbers between 0 and 1, we return them in a sorted order. Note that this\nproblem is an instance of set2seq. We used the architecture deﬁned in Figure 1, where the Read\nmodule is a small multilayer perceptron for each number, the Process module is an attention mech-\nanism over the read numbers, implemented as Tsteps over an LSTM with no input nor output, but\nattending the input embeddings, followed by an LSTM to produce indices in the input numbers with\na pointer network (Vinyals et al., 2015a), in the proper sorted order. We also compared this archi-\ntecture with a vanilla seq2seq architecture made of an input LSTM connected to an output LSTM\nwhich produces indices in the input numbers with a pointer network (Ptr-Net). Note that the only\ndifference between these two models is the encoding of the set using either an LSTM (as in previ-\nous work), or with the architecture proposed in the previous section. We ran multiple experiments,\nvarying the number Nof numbers to sort, as well as the number Tof processing steps of the Read,\nProcess, Write model.\nThe out-of-sample accuracies (whether we succeeded in sorting all numbers or not) of these experi-\nments are summarized in Table 1. We can see that the baseline pointer network LSTM input model is\nbetter than the Read-Process-and-Write model when no processing steps ( P= 0step) are used, but\nas soon as at least one processing step is allowed, the performance of the Read-Process-and-Write\nmodel gets better, increasing with the number of processing steps. We can also see that, as the size\nof the task (expressed in the number of elements to sort N) grows, the performance gets worse, as\nexpected. Also note that with 0 processing steps and 0 glimpses, the writing module is effectively\nunconditioned on Xand has to “blindly” point at the elements of X. Thus, it is unsurprising to see it\nperforming worse than any other model considered in Table 1. Lastly, equipping the writing module\nwith glimpses (i.e., adding an attention mechanism prior to “pointing”) improves both the baseline\nmodel (Ptr-Net), and our proposed modiﬁcation quite signiﬁcantly (in the most challenging cases, it\nmore than doubles accuracy).\nTable 1: The sorting experiment: out-of-sample sorting accuracy for various problem sizes and\nprocessing steps, with or without glimpses. All the reported accuracies are shown after reaching\n10000 training iterations, at which point all models had converged but none overﬁtted. Higher is\nbetter.\nLengthN Ptr-Net P= 0stepP= 1stepP= 5stepsP= 10 steps\nglimpses 0 1 0 1 0 1 0 1 0 1\nN= 5 81% 90% 65% 84% 84% 92% 88% 94% 90% 94%\nN= 10 8% 28% 7% 30% 14% 44% 17% 57% 19% 50%\nN= 15 0% 4% 1% 2% 0% 5% 2% 4% 0% 10%\n5Published as a conference paper at ICLR 2016\n5 O UTPUT SETS\nSo far, we have considered the problem of encoding input sets; let us now turn our attention to out-\nput representations. The chain rule which describes joint probabilities over sets of random variables\nYis, perhaps, the simplest decomposition of the joint probability which does not incur arbitrary\nrestrictions (such as conditional independence). Thus, as long as a powerful model that is trainable\nexists (which can cope with long range correlations), any order should work without any prior order\ninformation of the underlying problem that generated Y. Despite this, and even when a very pow-\nerful model (in terms of modeling power, and resilience to vanishing long term gradients) like the\nLSTM is employed, output ordering still plays a key role in successfully training models.\nIn the next subsection, we describe how the order in which we apply the chain rule affects the\nperformance on various tasks.\n5.1 O UTPUT ORDER MATTERS\nLetYbe a set (or a sequence). In this section, we will study the effect that ordering has on the per-\nformance of seq2seq models on several tasks. Namely, we will consider arbitrary (and non-arbitrary)\norders over the variables in Y, and model the conditional probability distribution P(YjX)following\nthat order for all training examples. As we will see, order matters (even when considering that the\nformulation through the chain rule works regardless of the ordering of Y, at least in principle).\n5.1.1 L ANGUAGE MODELING\nFor this experiment, we use the PennTree Bank, which is a standard language modeling bench-\nmark. This dataset is quite small for language modeling standards, so most models are data\nstarved. We trained medium sized LSTMs with large amounts of regularization (see medium model\nfrom Zaremba et al. (2014)) to estimate probabilities over sequences of words. We consider three\nversion of the dataset with three orderings: natural, reverse, and a ﬁxed, 3-word reversal:\nNatural: “This is a sentence .”\nReverse: “. sentence a is This”\n3-word: “a is This <pad>. sentence”\nNote that the 3-word reversal destroys the underlying structure of the sentence, and makes modeling\nthe joint probability much more difﬁcult since many higher order n-grams are scrambled. For each\nordering we trained a different model. The results for both natural and reverse matched each other\nat 86 perplexity on the development set (using the same setup as Zaremba et al. (2014)). Surpris-\ningly, the 3-word reversal degraded only 10 perplexity points, still achieving an impressive result in\nthis corpus at 96 perplexity. We note, however, that training perplexities were also 10 points higher,\nwhich indicates that the model had trouble handling the awkward ordering. Thus, even when consid-\nering that the chain rule still properly models the joint probability, some degradation was observed\nwhen a confounding ordering was chosen.\n5.1.2 P ARSING\nThe task of constituency parsing consists in producing a parse tree given a sentence. The model\nproposed by Vinyals et al. (2015b) is a sentence encoder LSTM followed by a decoder LSTM\ntrained to generate a depth ﬁrst traversal encoding of the parse tree, using an attention mechanism.\nThis approach matched state-of-the-art results on this task.\nEven though it seemed more sensible, depth ﬁrst traversal is only one of the many ways one can\nuniquely encode a tree onto a sequence. We thus tried to train a small model using depth ﬁrst\ntraversal (which matches the baseline of Vinyals et al. (2015b)) and another one using breadth ﬁrst\ntraversal (note that these orderings are input dependent). See Figure 2 for an example on how the tree\nlinearizes under both traversal schemes. The model trained to produce depth ﬁrst traversal linearized\ntrees obtained 89.5% F1 score (as reported by Vinyals et al. (2015b)), whereas the one producing\nbreadth ﬁrst traversal trees had a much lower F1 score at 81.5%,1showing again the importance of\npicking the right output ordering.\n1In fact, in many cases the decoder failed to produce a valid tree, so the real F1 score is likely lower.\n6Published as a conference paper at ICLR 2016\nNP\nVBZ\nis\n.\nVP\nS\nDT\nThis\nNP\nDT\na\nNN\nsentence\nDepth \nFirst \nTraversal: \nS \nNP \nDT \n!DT \n!NP \nVP \nVBZ \n!VBZ \nNP \nDT \n!DT \nNN \n!NN \n!NP \n!VP \n. \n!. \n!S\nBreadth \nFirst \nTraversal: \nS \nLEV \nNP \nVP \n. \nLEV \nDT \nPAR \nVBZ \nNP \nLEV \nPAR \nPAR \nDT \nNN \nDONE\nFigure 2: Depth ﬁrst and breadth ﬁrst linearizations of a parse tree which shows our different setups\nfor output ordering in the parsing task.\n5.1.3 C OMBINATORIAL PROBLEMS\nUnlike in the previous two examples, a problem that more commonly comes up as we try to represent\nnon-sequential data (like tours, triangulations, etc., discussed by (Vinyals et al., 2015a)), is the fact\nthat there may exist a large equivalence class of solutions.\nTake, as an example, outputting the indices for the sorted inputs of a set of random numbers, X.\nIndeed, this is a deterministic function. We can choose to output these indices in some order (e.g.,\nincreasing, decreasing, or using any arbitrary ﬁxed permutation), or treat them as a set (a tuple of\nargsort indices with corresponding ranking). As a result, there are n!possible outputs for a given X,\nall of which are perfectly valid. If our training set is generated with any of these permutations picked\nuniformly at random, our mapping (when perfectly trained) will have to place equal probability on\nn!output conﬁgurations for the same input X. Thus, this formulation is much less statistically\nefﬁcient.\nIn previous work (Vinyals et al., 2015a), it was found that restricting as much as possible the equiv-\nalence class for the outputs was always better. For instance, to output a tour (i.e. a sequence of cities\none has to visit for the traveling salesman problem), we started from the lower indexed city (i.e.,\nthe ﬁrst city that we input), and followed a counter-clockwise ordering. Similarly, to output a set\nof triangles (which triangulate the set of input points), we sorted them in lexicographical order and\nmoved left to right. In all cases, improvements of 5% absolute accuracy or more were observed.\nFailing to restrict the output equivalence class generally implies much slower convergence (and,\nthus, requires much more training data). For instance, for sorting, if considering the outputs as sets\nwhich we output in any of the possible n!orderings, convergence for nas small as 5 never reached\nthe same performance.\n5.1.4 G RAPHICAL MODELS\nLet us consider the joint probability of a set of Trandom variables P(y1;y2;:::;yT). Having no\nprior on how these random variables interact with each other, one way to model their joint probability\nis to use the chain rule as follows:\nP(y1;y2;:::;yT) =TY\nt=1P(ytjy1;y2;:::;yt\u00001) (8)\nand model this using an RNN, similar to RNN language models.\nWhile for sentences the natural order of words gives a good clue of how to order the random variables\nin the model, for other kind of data it might be harder to decide on it. Furthermore, in theory, the\norder should not matter, because of Bayes rule which lets us reorder all the conditional probabilities\nas needed. In practice however, it might be that one order is easier to model than another, as we have\nshown in this paper.\nThe purpose of this experiment is to demonstrate this using a controlled toy experiment. We gen-\nerated star-like graphical models over random variables where one variable (the head) follows an\nunconditional distribution, while the others follow a conditional distribution based on the value of\n7Published as a conference paper at ICLR 2016\nthe head variable. We expect that it should be easier to model the joint distribution by choosing any\nordering which starts with the head variable. We created several artiﬁcial datasets by varying the\nnumber of random variables to model (between 10 and 50, each of which was a multinomial over 10\nsymbols), the training set size (between 200 and 20000 training samples), and the randomness of the\nmarginal distributions, or how deterministic, or peaky, they were. For each problem, we trained two\nLSTMs for 10,000 mini-batch iterations to model the joint probability, one where the head random\nvariable was shown ﬁrst, and one where it was shown last.\nThe results were as follows:\n• when the training set size is large enough (20000), the LSTM is able to learn the joint\nprobability in whichever order;\n• when the marginal distributions are very peaky (and thus almost deterministic), the LSTM\nis also able to learn the joint probability independently of the order;\n• in all other cases (small training set size, small or large number of random variables, and\nsome amount of randomness in the marginal distributions), it was always easier to learn an\nLSTM with the optimal order of random variables than any other order.\n5.2 F INDING OPTIMAL ORDERINGS WHILE TRAINING\nRecall the model we proposed for dealing with input sets: given an embedding for each of the inputs,\nwe have a generic module that is able to process its inputs in any order. This yields an embedding\nsatisfying the key property of being invariant to reorderings, whilst being generic in the kinds of\ncomputations to do over the input set.\nUnfortunately, placing a joint probability over a set of random variables y1;:::ynwhen the structure\nof the joint probability function is unknown is a hard problem. Fortunately, and thanks to recurrent\nneural networks, we can apply the chain rule which decomposes this joint probability sequentially\n(see eq. 8) without independence assumptions. In this work, we focus on using the chain rule,\ndiscarding more naive decompositions that have strong and unrealistic assumptions (e.g., conditional\nindependence).\nAn obvious drawback of the chain rule which violates the argument of treating y1;:::ynas a set is\nthat we condition these random variables in a particular order. Even though, in principle, the order\nshould not matter, in the previous section we have shown that this is indeed not the case, and that\ncertain orderings are better than others in a variety of tasks – most likely due to the parameterization\nof the joint probability (using an LSTM), and the non-convex nature of the optimization problem.\nOur proposed solution to deal with the aforementioned drawback is extremely simple: as we train,\nwe let the model decide which is the best ordering in which it will apply the chain rule. More\nformally, assume there exists an ordering which maximally simpliﬁes the task, \u0019(X)(whereXis\nthe input sequence or set, which can be empty). We would like to train the model as p(Y\u0019(X)jX).\nThe number of possible orderings is large – n!wherenis the length of the output, and the best order\nis unknown a priori.\nSincen!can be very large, we could attempt to do (inexact) search as we train the model. Instead of\nmaximizing the log probability of p(YjX)for each training example pair, we also maximize over\norderings as follows:\n\u0012?= arg max\n\u0012X\nimax\n\u0019(Xi)logp(Y\u0019(Xi)jXi;\u0012) (9)\nwhere max\u0019(Xi)is computed either naively, or with an inexact search. Note that Equation (9) may\nnot strictly improve the regular maximum likelihood framework due to non-convexity, but we found\nthis to not be an issue in practice.\nBesides not being scalable, we found that, if done naively and picking the max over ordering as we\ntrain, the model would pick a random ordering (as a function of the initial parameters), and would\nget stuck on it permanently (since it would reinforce it through learning). We added two ways to\nexplore the space of all orderings as follows:\n8Published as a conference paper at ICLR 2016\n• We pretrain the model with a uniform prior over \u0019(X)for 1000 steps, which amounts to\nreplacing the max\u0019(Xi)in eq. (9) by aP\n\u0019(Xi).\n• We then pick an ordering by sampling \u0019(X)according to a distribution proportional to\np(Y\u0019(X)jX). This costsO(1)model evaluations (vs. naive search which would be O(n!)).\nCrucially, sampling p(Y\u0019(X)jX)can be done very efﬁciently as we can use ancestral sampling (left-\nto-right) which requires to evaluate p(:)only once instead of n!.\n5.2.1 5- GRAM MODELING\nIn our initial attempt to solve (9), we considered a simpliﬁed version of the language modeling\ntask described in Section 5.1.1. The simpliﬁed task consists of modeling the joint probability of 5-\ngrams without any further context (i.e., there is no input X). This choice allowed us to have a small\nenoughnas initially we were trying to exactly ﬁnd the best ordering out of the n!possible ones.\nThus, we disregarded possible effects of inexact search, and focused on the essential of the training\ndynamics where the model being optimized picks the best ordering \u0019which maximizes p(Y\u0019)under\nits current parameters, and reinforces that ordering by applying updates on the gradient of logp(Y\u0019)\nw.r.t. the parameters. Eventually, as noted in Section 5.2, we found sampling to be superior in terms\nof convergence, whilst simplifying the complexity from O(n!)down toO(1), and is the preferred\nsolution which we used in the rest of this section.\nTo test this framework, we converted 5-grams (i.e., sequences of words) to a set in the following\nway:\n5-gram (sequence): y1=This,y2=is,y3=a,y4=ﬁve,y5=gram\n5-gram (set): y1=(This,1),y2=(is,2),y3=(a,3),y4=(ﬁve,4),y5=(gram,5)\nNote that adding the original position alongside the words makes Ya set. Thus, we can shufﬂe\nYin arbitrarily without losing the original structure of the sequence. The ﬁrst experiment, which\nreinforces our result in Section 5.1.1, tests the hypothesis once again that order matters. Training\na model which follows the natural order (i.e., produces (This,1), followed by (is,2) conditioned on\n(This,1), etc.), achieves a validation perplexity of 225.2If, instead of picking (1;2;3;4;5), we use\n(5;1;3;4;2), perplexity drops to 280.\nWe then test optimization of eq. (9) in two setups:\nEasy: The training set contains examples from (1;2;3;4;5)and(5;1;3;4;2), uniformly sampled.\nHard: The training set contains examples from the 5!possible orderings, uniformly sampled.\nOur results are shown in Table 2. Note that, in the easy case, we restrict the search space over\norderings to only 2, where one order is clearly better than the other. We note that, after the pretraining\nphase, we decide which of the two orderings is better to represent the data under the model being\ntrained. Very quickly, the model settles on the natural (1;2;3;4;5)ordering, yielding a perplexity\nof 225. In the most difﬁcult case, where any order is possible, the model settles to orders such as\n(1;2;3;4;5),(5;4;3;2;1), and small variations of them. In all cases, the ﬁnal perplexity is 225.\nThus, the framework we propose is able to ﬁnd good orderings without any prior knowledge. We\nplan to not only recover optimal orderings, but ﬁnd ones that were unknown to us when applying\nthe seq2seq framework naively.\nTable 2: Experiments in which the model ﬁnds the optimal ordering of a set for the 5-gram language\nmodeling task. Perplexities are reported on the validation set (lower is better).\nTask Orders considered Perplexity\n(1;2;3;4;5) 1 225\n(5;1;3;4;2) 1 280\nEasy 2 225\nHard 5! 225\n2This is much worse than the results reported in Section 5.1.1 since modeling 5-grams without context is\nmuch harder than standard language modeling.\n9Published as a conference paper at ICLR 2016\n6 C ONCLUSION\nLSTMs have shown to be powerful models to represent variable length sequential data thanks to\ntheir ability to handle reasonably long term dependencies and the use of the chain rule to efﬁciently\ndecompose joint distributions. On the other hand, some problems are expressed in terms of an\nunordered set of elements, either as input or as outputs; in some other cases, the data is represented\nby some structure that needs to be linearized to be fed to the LSTM, and there might be more\nthan one way to do so. The ﬁrst goal of this paper was to shed some light on these problems:\nindeed, we show that order matters to obtain the best performance. We then considered the case\nof unordered input data, for which we proposed the Read-Process-and-Write architecture, and the\ncase of unordered output data, for which we proposed an efﬁcient training algorithm that includes a\nsearch over possible orders during training and inference. We illustrated our proposed approaches\nfor input and output sets through various experiments such as sorting, graphical models, language\nmodeling, and parsing.\nACKNOWLEDGMENTS\nWe would like to thank Ilya Sutskever, Navdeep Jaitly, Rafal Jozefowicz, Quoc Le, Lukasz Kaiser,\nGeoffrey Hinton, Jeff Dean, Shane Gu and the Google Brain Team for useful discussions on this\ntopic. We also thank the anonymous reviewers which helped improving our paper.\nREFERENCES\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine translation by jointly learning to align and translate. In\nProc. ICLR , 2015a.\nBahdanau, D., Chorowski, J., Serdyuk, D., Brakel, P., and Bengio, Y . End-to-end attention-based large vocab-\nulary speech recognition. arXiv preprint arXiv:1508.04395 , 2015b.\nBakir, G., Hofmann, T., Scholkopf, B., Smola, A. J., Taskar, B., and Vishwanathan, S.V .N. (eds.). Predicting\nStructured Data . MIT Press, 2007.\nChan, W., Jaitly, N., Le, Q. V ., and Vinyals, O. Listen, attend and spell. arXiv , abs/1508.01211, 2015. URL\nhttp://arxiv.org/abs/1508.01211 .\nCho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y . Learning\nphrase representations using RNN encoder-decoder for statistical machine translation. In Proc. EMNLP ,\n2014.\nDonahue, J., Hendricks, L. A., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., and Darrell, T.\nLong-term recurrent convolutional networks for visual recognition and description. In Proc. CVPR , 2015.\nGraves, A. Supervised Sequence Labelling with Recurrent Neural Networks . Springer, 2012.\nGraves, A., Wayne, G., and Danihelka, I. Neural turing machines. In arXiv preprint arXiv:1410.5401 , 2014.\nHinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V ., Nguyen, P., Sainath,\nT. N., and Kingsbury, B. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal\nProcessing Magazine , 29:82–97, 2012.\nHochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Computation , 9(8), 1997.\nIoffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covari-\nate shift. In Proceedings of the 32nd International Conference on Machine Learning, ICML , 2015.\nKalchbrenner, N. and Blunsom, P. Recurrent continuous translation models. In Proc. EMNLP , 2013.\nMaas, A. L., Miller, S. D., O’Neil, T. M., and Ng, A. Y . Word-level acoustic modeling with convolutional\nvector regression. In ICML 2012 Workshop on Representation Learning , 2012.\nMao, J., Xu, W., Yang, Y ., Wang, J., Huang, Z., and Yuille, A. L. Deep captioning with multimodal recurrent\nneural networks (m-RNN). In International Conference on Learning Representations , 2015.\nSocher, R., Manning, C. D., and Ng, A. Y . Learning continuous phrase representations and syntactic parsing\nwith recursive neural networks. In Advances in Neural Information Processing Systems , 2010.\n10Published as a conference paper at ICLR 2016\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc V . Sequence to sequence learning with neural networks. In Proc.\nNIPS , 2014.\nVinyals, O., Fortunato, M., and Jaitly, N. Pointer networks. In Advances in Neural Information Processing\nSystems, NIPS , 2015a.\nVinyals, O., Kaiser, L., Koo, T., Petrov, S., Sutskever, I., and Hinton, G. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems , 2015b.\nVinyals, O., Toshev, A., Bengio, S., and Erhan, D. Show and tell: A neural image caption generator. In Proc.\nCVPR , 2015c.\nWeston, J., Chopra, S., and Bordes, A. Memory networks. In International Conference on Learning Represen-\ntations, ICLR , 2015.\nZaremba, W. and Sutskever, I. Learning to execute. arXiv , abs/1410.4615, 2014.\nZaremba, W. and Sutskever, I. Reinforcement learning neural turing machines. arXiv , abs/1505.00521, 2015.\nZaremba, W., Sutskever, I., and Vinyals, O. Recurrent neural network regularization. arXiv , abs/1409.2329,\n2014.\n11\n\n\n\nGPipe: Easy Scaling with Micro-Batch Pipeline\nParallelism\nYanping Huang\nhuangyp@google.comYoulong Cheng\nylc@google.comAnkur Bapna\nankurbpn@google.com\nOrhan Firat\norhanf@google.comMia Xu Chen\nmiachen@google.comDehao Chen\ndehao@google.com\nHyoukJoong Lee\nhyouklee@google.comJiquan Ngiam\njngiam@google.comQuoc V . Le\nqvl@google.com\nYonghui Wu\nyonghui@google.comZhifeng Chen\nzhifengc@google.com\nAbstract\nScaling up deep neural network capacity has been known as an effective approach\nto improving model quality for several different machine learning tasks. In many\ncases, increasing model capacity beyond the memory limit of a single accelera-\ntor has required developing special algorithms or infrastructure. These solutions\nare often architecture-speciﬁc and do not transfer to other tasks. To address the\nneed for efﬁcient and task-independent model parallelism, we introduce GPipe, a\npipeline parallelism library that allows scaling any network that can be expressed\nas a sequence of layers. By pipelining different sub-sequences of layers on sep-\narate accelerators, GPipe provides the ﬂexibility of scaling a variety of different\nnetworks to gigantic sizes efﬁciently. Moreover, GPipe utilizes a novel batch-\nsplitting pipelining algorithm, resulting in almost linear speedup when a model\nis partitioned across multiple accelerators. We demonstrate the advantages of\nGPipe by training large-scale neural networks on two different tasks with distinct\nnetwork architectures: (i) Image Classiﬁcation : We train a 557-million-parameter\nAmoebaNet model and attain a top-1 accuracy of 84.4% on ImageNet-2012, (ii)\nMultilingual Neural Machine Translation : We train a single 6-billion-parameter,\n128-layer Transformer model on a corpus spanning over 100 languages and achieve\nbetter quality than all bilingual models.\n1 Introduction\nDeep learning has seen great progress over the last decade, partially thanks to the development of\nmethods that have facilitated scaling the effective capacity of neural networks. This trend has been\nmost visible for image classiﬁcation, as demonstrated by the accuracy improvements on ImageNet\nwith the increase in model capacity (Figure 1a). A similar phenomenon can also be observed in\nthe context of natural language processing (Figure 1b) where simple shallow models of sentence\nrepresentations [1, 2] are outperformed by their deeper and larger counterparts [3, 4].\nWhile larger models have brought remarkable quality improvements to several ﬁelds, scaling neural\nnetworks introduces signiﬁcant practical challenges. Hardware constraints, including memory\nlimitations and communication bandwidths on accelerators (GPU or TPU), force users to divide larger\nPreprint. Under review.arXiv:1811.06965v5  [cs.CV]  25 Jul 2019Figure 1: (a) Strong correlation between top-1 accuracy on ImageNet 2012 validation dataset [ 5]\nand model size for representative state-of-the-art image classiﬁcation models in recent years [ 6,7,8,\n9,10,11,12]. There has been a 36\u0002increase in the model capacity. Red dot depicts 84:4%top-1\naccuracy for the 550M parameter AmoebaNet model. (b) Average improvement in translation quality\n(BLEU) compared against bilingual baselines on our massively multilingual in-house corpus, with\nincreasing model size. Each point, T(L; H; A ), depicts the performance of a Transformer with L\nencoder and Ldecoder layers, a feed-forward hidden dimension of HandAattention heads. Red dot\ndepicts the performance of a 128-layer 6B parameter Transformer.\nmodels into partitions and to assign different partitions to different accelerators. However, efﬁcient\nmodel parallelism algorithms are extremely hard to design and implement, which often requires the\npractitioner to make difﬁcult choices among scaling capacity, ﬂexibility (or speciﬁcity to particular\ntasks and architectures) and training efﬁciency. As a result, most efﬁcient model-parallel algorithms\nare architecture and task-speciﬁc. With the growing number of applications of deep learning, there is\nan ever-increasing demand for reliable and ﬂexible infrastructure that allows researchers to easily\nscale neural networks for a large variety of machine learning tasks.\nTo address these challenges, we introduce GPipe, a ﬂexible library that enables efﬁcient training of\nlarge neural networks. GPipe allows scaling arbitrary deep neural network architectures beyond the\nmemory limitations of a single accelerator by partitioning the model across different accelerators and\nsupporting re-materialization on every accelerator [ 13,14]. With GPipe, each model can be speciﬁed\nas a sequence of layers, and consecutive groups of layers can be partitioned into cells. Each cell is\nthen placed on a separate accelerator. Based on this partitioned setup, we propose a novel pipeline\nparallelism algorithm with batch splitting. We ﬁrst split a mini-batch of training examples into\nsmaller micro-batches , then pipeline the execution of each set of micro-batches over cells. We apply\nsynchronous mini-batch gradient descent for training, where gradients are accumulated across all\nmicro-batches in a mini-batch and applied at the end of a mini-batch. Consequently, gradient updates\nusing GPipe are consistent regardless of the number of partitions, allowing researchers to easily train\nincreasingly large models by deploying more accelerators. GPipe can also be complemented with\ndata parallelism to further scale training.\nWe demonstrate the ﬂexibility and efﬁciency of GPipe on image classiﬁcation and machine translation.\nFor image classiﬁcation, we train the AmoebaNet model on 480\u0002480input from the ImageNet 2012\ndataset. By increasing the model width, we scale up the number of parameters to 557million and\nachieve a top-1 validation accuracy of 84.4%. On machine translation, we train a single 128-layer\n6-billion-parameter multilingual Transformer model on 103 languages (102 languages to English).\nWe show that this model is capable of outperforming the individually trained 350-million-parameter\nbilingual Transformer Big [15] models on 100 language pairs.\n2 The GPipe Library\nWe now describe the interface and the main design features of GPipe. This open-source library is\nimplemented under the Lingvo [ 16] framework. The core design features of GPipe are generally\napplicable and can be implemented for other frameworks [17, 18, 19].\n2Figure 2: (a) An example neural network with sequential layers is partitioned across four accelerators.\nFkis the composite forward computation function of the k-th cell. Bkis the back-propagation\nfunction, which depends on both Bk+1from the upper layer and Fk. (b) The naive model parallelism\nstrategy leads to severe under-utilization due to the sequential dependency of the network. (c) Pipeline\nparallelism divides the input mini-batch into smaller micro-batches, enabling different accelerators to\nwork on different micro-batches simultaneously. Gradients are applied synchronously at the end.\n(b)\n(a)\n (c)\n2.1 Interface\nAny deep neural network can be deﬁned as a sequence of Llayers. Each layer Liis composed of\na forward computation function fi, and a corresponding set of parameters wi. GPipe additionally\nallows the user to specify an optional computation cost estimation function, ci. With a given number\nof partitions K, the sequence of Llayers can be partitioned into Kcomposite layers, or cells. Let pk\nconsist of consecutive layers between layers iandj. The set of parameters corresponding to pkis\nequivalent to the union of wi,wi+1, . . . ,wj, and its forward function would be Fk=fj\u000e: : :\u000efi+1\u000efi.\nThe corresponding back-propagation function Bkcan be computed from Fkusing automatic symbolic\ndifferentiation. The cost estimator, Ck, is set to \u0006j\nl=icl.\nThe GPipe interface is extremely simple and intuitive, requiring the user to specify: (i) the number of\nmodel partitions K, (ii) the number of micro-batches M, and (iii) the sequence and deﬁnitions of L\nlayers that deﬁne the model. Please refer to supplementary material for examples.\n2.2 Algorithm\nOnce the user deﬁnes the sequence of layers in their network in terms of model parameters wi, forward\ncomputation function fi, and the cost estimation function ci, GPipe partitions the network into K\ncells and places the k-th cell on the k-th accelerator. Communication primitives are automatically\ninserted at partition boundaries to allow data transfer between neighboring partitions. The partitioning\nalgorithm minimizes the variance in the estimated costs of all cells in order to maximize the efﬁciency\nof the pipeline by syncing the computation time across all partitions.\nDuring the forward pass, GPipe ﬁrst divides every mini-batch of size NintoMequal micro-batches,\nwhich are pipelined through the Kaccelerators. During the backward pass, gradients for each\nmicro-batch are computed based on the same model parameters used for the forward pass. At the end\nof each mini-batch, gradients from all Mmicro-batches are accumulated and applied to update the\nmodel parameters across all accelerators. This sequence of operations is illustrated in Figure 2c.\nIf batch normalization [ 20] is used in the network, the sufﬁcient statistics of inputs during training\nare computed over each micro-batch and over replicas if necessary [ 21]. We also track the moving\naverage of the sufﬁcient statistics over the entire mini-batch to be used during evaluation.\n3Table 1: Maximum model size of AmoebaNet supported by GPipe under different scenarios. Naive-1\nrefers to the sequential version without GPipe. Pipeline- kmeans kpartitions with GPipe on k\naccelerators. AmoebaNet-D (L, D): AmoebaNet model with Lnormal cell layers and ﬁlter size D.\nTransformer-L: Transformer model with Llayers, 2048 model and 8192 hidden dimensions. Each\nmodel parameter needs 12bytes since we applied RMSProp during training.\nNVIDIA GPUs (8GB each) Naive-1 Pipeline-1 Pipeline-2 Pipeline-4 Pipeline-8\nAmoebaNet-D (L, D) (18, 208) (18, 416) (18, 544) (36, 544) (72, 512)\n# of Model Parameters 82M 318M 542M 1.05B 1.8B\nTotal Model Parameter Memory 1.05GB 3.8GB 6.45GB 12.53GB 24.62GB\nPeak Activation Memory 6.26GB 3.46GB 8.11GB 15.21GB 26.24GB\nCloud TPUv3 (16GB each) Naive-1 Pipeline-1 Pipeline-8 Pipeline-32 Pipeline-128\nTransformer-L 3 13 103 415 1663\n# of Model Parameters 282.2M 785.8M 5.3B 21.0B 83.9B\nTotal Model Parameter Memory 11.7G 8.8G 59.5G 235.1G 937.9G\nPeak Activation Memory 3.15G 6.4G 50.9G 199.9G 796.1G\n2.3 Performance Optimization\nIn order to reduce activation memory requirements, GPipe supports re-materialization [ 14]. During\nforward computation, each accelerator only stores output activations at the partition boundaries.\nDuring the backward pass, the k-th accelerator recomputes the composite forward function Fk. As a\nconsequence, peak activation memory requirement is reduced to O(N+L\nK\u0002N\nM), whereN\nMis the\nmicro-batch size andL\nKis the number of layers per partition. In comparison, memory requirement\nwithout re-materialization and partitioning would be O(N\u0002L), since computing the gradients bi\nrequires both the upper layer gradients bi+1and the cached activations fi(x).\nAs illustrated in Figure 2c, partitioning introduces some idle time per accelerator, which we refer to\nas the bubble overhead. This bubble time is O(K\u00001\nM+K\u00001)amortized over the number of micro-steps\nM. In our experiments, we found the bubble overhead to be negligible when M\u00154\u0002K. This\nis also partly because re-computation during the backward pass can be scheduled earlier, without\nwaiting for the gradients from earlier layers.\nGPipe also introduces low communication overhead, given that we only need to pass activation\ntensors at the partition boundaries between accelerators. Therefore, we can achieve efﬁcient scaling\nperformance even on accelerators without high-speed interconnects.\nFigure 2c assumes partitions are evenly balanced. However, memory requirements and computa-\ntion ﬂops at different layers are often quite imbalanced. In such scenarios, imperfect partitioning\nalgorithms might lead to load imbalance. Better partitioning algorithms can potentially improve the\nperformance over our heuristic approach.\n3 Performance Analyses\nWe evaluate GPipe performance with two very different types of model architectures: an Amoe-\nbaNet [ 12] convolutional model and a Transformer [ 15] sequence-to-sequence model. We ran\nexperiments to study their scalability, efﬁciency and communication cost.\nWe expect both re-materialization and pipeline parallelism to beneﬁt memory utilization and thus\nmake ﬁtting giant models feasible. We report the biggest model size GPipe can support under\nreasonably large input size in Table 1. For AmoebaNet, we ran the experiments on Cloud TPUv2s\nwith 8GB memory per accelerator. We used a ﬁxed input image size of 224\u0002224and mini-batch\nsize of 128. Without GPipe, a single accelerator can train up to an 82M-parameter AmoebaNet,\nconstrained by device memory limits. Owing to re-materialization in back-propagation and batch\nsplitting, GPipe reduces the intermediate activation memory requirements from 6.26GB to 3.46GB,\nenabling a 318M-parameter model on a single accelerator. With model parallelism, we were able to\nscale AmoebaNet to 1.8 billion parameters on 8 accelerators, 25x more than what is possible without\n4GPipe. In this case, the maximum model size did not scale perfectly linearly due to the imbalanced\ndistribution of model parameters over different layers in AmoebaNet.\nWe next trained Transformer models using Cloud TPUv3s with 16GB memory per accelerator core.\nWe used a ﬁxed vocabulary size of 32k, sequence length 1024 and batch size 32. Each Transformer\nlayer has 2048 for model dimension, 8192 for feed-forward hidden dimension and 32 attention heads.\nWe scaled the model by varying the number of layers. Re-materialization allows training a 2:7\u0002\nlarger model on a single accelerator. With 128 partitions, GPipe allows scaling Transformer up to\n83.9B parameters, a 298\u0002increase than what is possible on a single accelerator. Different from\nAmoebaNet, the maximum model size scales linearly with the number of accelerators for Transformer,\nsince each layer has the same number of parameters and input sizes.\nTable 2: Normalized training throughput using\nGPipe with different # of partitions Kand differ-\nent # of micro-batches Mon TPUs. Performance\nincreases with more micro-batches. There is an\nalmost linear speedup with the number of accelera-\ntors for Transformer model when M\u001dK. Batch\nsize was adjusted to ﬁt memory if necessary.\nTPU AmoebaNet Transformer\nK= 2 4 8 2 4 8\nM= 1 1 1.13 1.38 1 1.07 1.3\nM= 4 1.07 1.26 1.72 1.7 3.2 4.8\nM= 32 1.21 1.84 3.48 1.8 3.4 6.3To evaluate efﬁciency, we report the normalized\ntraining throughput of AmoebaNet-D (18, 256)\nand Transformer-48 using GPipe with different\nnumbers of partitions and different numbers of\nmicro-batches in Table 2. Each partition is as-\nsigned to a separate accelerator. We observe\nthat when the number of micro-batches Mis\nat least 4\u0002the number of partitions, the bub-\nble overhead is almost negligible. For Trans-\nformer model, there is a 3:5\u0002speedup when it is\npartitioned across four times more accelerators.\nFurthermore, training throughput scales almost\nlinearly with the number of devices, thanks to\nthe computation being evenly distributed across\nTransformer layers. In contrast, the AmoebaNet\nmodel achieves sub-linear speedup due to its im-\nbalanced computation distribution. When Mis\nrelatively small, the bubble overhead can no longer be negligible. When Mis1, there is effectively\nno pipeline parallelism. We observe relatively constant throughput regardless of the number of\naccelerators used, indicating only one device is actively computing at any given time.\nTo measure the effect of communication overhead with GPipe, we ran our experiments on a single\nhost with multiple NVIDIA P100 GPUs but without NVLinks. Data transfer across GPUs then has to\ninvolve the relatively slow device-to-host and host-to-device transfers through PCI-E. The number of\nmicro-batches was ﬁxed at 32. As shown in Table 3, we observe 2:7\u0002speedup for AmoebaNet-D\n(18, 128) when we increase the number of partitions from 2to8. For the 24-layer Transformer,\nTable 3: Normalized training throughput using\nGPipe on GPUs without high-speed interconnect.\nGPU AmoebaNet Transformer\nK= 2 4 8 2 4 8\nM= 32 1 1.7 2.7 1 1.8 3.3the speedup is 3:3\u0002. There is similar linear\nspeedup to what we observe on TPUs where\nhigh-speed interconnects are equipped. The\ncommunication bandwidth between devices is\nno longer a bottleneck for model parallelism\nsince GPipe only transfers activation tensors at\nthe boundaries of partitions.\n4 Image Classiﬁcation\nAs a proof of concept, we ﬁrst used GPipe to\nscale AmoebaNet. We increased the number of\nchannels in an AmoebaNet and scaled the input image size to 480\u0002480. We trained this 557-million-\nparameter AmoebaNet-B(18, 512) on the ImageNet 2012 dataset, using the same hyper-parameters\nas described in [ 12]. The network was divided into 4 partitions. This single model achieves 84:4%\ntop-1 and 97% top-5 validation accuracy with single-crop.\nWe further demonstrate the effectiveness of giant convolution networks on other image datasets\nthrough transfer learning [ 22,23]. Speciﬁcally, we used the pre-trained ImageNet model to ﬁne-tune\non a variety of target datasets ranging from general to ﬁne-grained classiﬁcation. We changed the\nnumber of output units in the last softmax classiﬁcation layer to the number of classes in the target\ndataset and initialized the new softmax layer randomly. All the other layers were initialized from\n5Table 4: Image classiﬁcation accuracy using AmoebaNet-B (18, 512) ﬁrst trained on ImageNet 2012\nthen ﬁne-tuned on others. Please refer to the supplementary material for a detailed description of our\ntraining setup. Our ﬁne-tuned results were averaged across 5 ﬁne-tuning runs. Baseline results from\nReal et al. [12] and Cubuk et al. [26] were directly trained from scratch. *Mahajan et al.’s model [ 27]\nachieved 85:4%top-1 accuracy but it was pretrained on non-public Instagram data. Ngiam et al. [28]\nachieved better results by pre-training with data from a private dataset (JFT-300M).\nDataset # Train # Test # Classes Accuracy ( %) Previous Best ( %)\nImageNet-2012 1,281,167 50,000 1000 84:4 83:9[12] ( 85:4\u0003[27])\nCIFAR-10 50,000 10,000 10 99:0 98:5[26]\nCIFAR-100 50,000 10,000 100 91:3 89:3[26]\nStanford Cars 8,144 8,041 196 94:6 94:8\u0003[26]\nOxford Pets 3,680 3,369 37 95:9 93:8\u0003[29]\nFood-101 75,750 25,250 101 93:0 90:4\u0003[30]\nFGVC Aircraft 6,667 3,333 100 92:7 92:9\u0003[31]\nBirdsnap 47,386 2,443 500 83:6 80:2\u0003[32]\nImageNet pre-training. Input images to the network during training were resized to 480\u0002480,\nhorizontally ﬂipped randomly and augmented using cutout [ 24]. Training hyper-parameters were\nthe same as those used for ImageNet (a detailed description of our training setup is provided in\nsupplementary material). In Table 4, we report the average single-crop test accuracy over 5 ﬁne-tuning\nruns for each dataset. Our giant models obtain competitive results on all target datasets. For example,\nCIFAR-10 error rate is reduced to 1%and CIFAR-100 error rate to 8:7%. These results corroborate\nthe ﬁndings by Kornblith et al. [25], i.e., better ImageNet models transfer better.\n5 Massive Massively Multilingual Machine Translation\nNext, we demonstrate the ﬂexibility of GPipe by scaling up models used for Natural Language\nProcessing (NLP). Due to an abundance of available parallel corpora, neural machine translation\n(NMT) has become a benchmark task for any architecture used for NLP [ 33,15,34,35,36]. For\nthis reason, we continue our GPipe experiments on a large-scale multilingual NMT task. We use a\ncorpus of parallel documents over 102 languages and English, containing a total of 25 billion training\nexamples, ranging from 104to109per language [ 37]. This dataset creates a realistic test bed for\nexperiments on scalability by spanning a diverse set of languages from data-scarce (low-resource) to\ndata-rich (high-resource). For the ﬁrst time in machine translation, we show that a large enough NMT\nmodel can learn the mapping between more than 100 language pairs simultaneously, while achieving\nbetter than bilingual model performance for all languages. This further brings out the importance of\nhaving efﬁcient and ﬂexible model-parallelism tools.\nOur comparison is based on the performance of a single Transformer [ 15] trained on all language\npairs in this corpus. We scale the architecture along two dimensions to stress the ﬂexibility of GPipe:\n(i) along the depth by increasing the number of layers in the model and (ii) along the width by\nincreasing the hidden dimension in the feed-forward layers and the number of attention heads (as well\nas # attention channels) in multi-head attention layers similar to Shazeer et al. [34]. Please refer to\nthe supplementary material for a detailed description of our dataset, baselines, training conﬁguration\nand optimization hyper-parameters.\nWe start with a standard 400M-parameter Transformer Big model, T(6;8192;16)1, as described in\nChen et al. [35], with a vocabulary size of 64k. In Figure 3, we compare its performance against a\n1.3B-parameter deep model, T(24;8192;16), a 1.3B-parameter wide model, T(12;16384 ;32), a 3B-\nparameter model, T(32;16384 ;32)and a 6B-parameter model, T(64;16384 ;32). All of the models\nare trained on all language pairs simultaneously, using temperature-based sampling as employed for\nmultilingual BERT2[3].T(12;16384 ;32),T(24;8192;32),T(32;16384 ;32)andT(64;16384 ;32)\nare partitioned over 2,4,8and16accelerators respectively.\n1T(L;H;A )is a Transformer model with Lencoder layers and Ldecoder layers, a feed-forward hidden\ndimension of HandAattention heads. The model dimension is ﬁxed to 1024 .\n2https://github.com/google-research/bert/blob/master/multilingual.md\n6From Figure 3, we can observe that increasing the model capacity from 400M to 1.3B parameters\nsigniﬁcantly improves performance across all languages. Scaling up the model from 1.3B parameters\nto 6B parameters shows further improvement, especially for high-resource languages, although\ndiminishing returns can be observed when scaling the model from 1.3B to 3B and 6B parameters.\nBelow we discuss some of our empirical ﬁndings based on these large-scale experiments.\nFigure 3: Translation quality across all languages with increasing multilingual model capacity.\nLanguages are arranged in the order of decreasing training dataset size from left to right. T(L; H; A ),\ndepicts the performance of a Transformer with Lencoder and Ldecoder layers, a feed-forward hidden\ndimension of HandAattention heads. We notice that increasing the model capacity, from 400M\nparams ( T(6;8192;16)) to 1.3B ( T(24;8192;16)), and further, to 6B ( T(64;16384 ;32)), leads to\nsigniﬁcant quality improvements across all languages. We also notice huge quality improvements\nfor low-resource languages (right side of the plot), when compared against bilingual baselines,\nhighlighting the signiﬁcant transfer gains resulting from training a multilingual model.\nDepth-Width Trade-off: We study the trade-off between depth and width in our multilingual\nsetup and compare the performance of 1.3B wide model T(12;16384 ;32)and 1.3B deep model\nT(24;8192;16). While the quality of these two models on high-resource languages (left of Figure 3)\nis very similar, the deeper model outperforms by huge margins on low-resource languages, suggesting\nthat increasing model depth might be better for generalization. Further, the quality improvements for\nlow-resource languages (right side of Figure 3), when comparing the 1.3B deep model against the\n400M model, are almost as large as the improvements for high-resource languages, indicating that\nincreasing depth might potentially increase the extent of transfer to low-resource tasks.\nTrainability Challenges with Deep Models: Although depth increases the representational ca-\npacity of neural networks, it also complicates the optimization problem. In our large-scale ex-\nperiments, we encountered severe trainability issues arising from a combination of sharp activa-\ntions (positive kurtosis) and dataset noise. We observed that after training for a few thousand\nsteps, the model predictions would become extremely peaky and vulnerable to noise, which fre-\nquently resulted in non-ﬁnite or large gradients that eventually destroyed the learning progress.\nTo counter these problems, we apply two methods: (i) Following Zhang et al . [38], we scale\ndown the initialization of all transformer feed-forward layers by the number of layers. (ii) We clip\nthe logit predictions (softmax pre-activations) whenever their magnitude exceeds a certain value.\nTable 5: The Effect of Batch Size\nBatch Size 260K 1M 4M\nBLEU 30.92 31.86 32.71\nLoss (NLL) 2.58 2.51 2.46A combination of these two approaches allows us to miti-\ngate the training instability posed by scaling model depth.\nLarge Batches: Due to its simplicity, data parallelism is\nthe dominant approach to scale neural network training[ 39,\n40]. We test the limits of large-batch training by signiﬁ-\ncantly increasing the batch size used for standard Trans-\nformer Big training. Starting from 260K tokens per batch,\n7we increase the effective batch size to 4M and observe\nthe validation loss and BLEU scores on the high-resource\nlanguage pair, German-English (similar trend can also be observed for other language pairs). Opti-\nmization parameters used here are identical to those for previous experiments. To our knowledge,\n4M tokens per batch is the largest batch size that has ever been used in literature to date for training\nNMT models [ 41]. Table 5 shows that both metrics improve signiﬁcantly as we increase the batch\nsize. We believe further increasing batch size can potentially yield more improvement.\n6 Design Features and Trade-Offs\nSeveral approaches have been proposed to enable efﬁcient large-scale model parallelism. However,\neach approach chooses its own set of trade-offs, making it suitable for scaling speciﬁc architectures\nunder particular hardware constraints. Here we highlight the various design choices and trade-offs\ninvolved with several model-parallelism approaches, and how they compare with GPipe in terms of\nﬂexibility, scalability and efﬁciency under various hardware constraints and architecture variants.\nThe core idea of model parallelism involves partitioning a network into different computational units,\nwhich are then placed on different devices [ 42,43,44,45]. Conceptually this supports scaling a\nlarge spectrum of models to huge capacities. However these approaches typically suffer from low\nhardware utilization and device communication bottlenecks. Single Program Multiple Data (SPMD)\nand pipeline parallelism have been proposed as solutions to counter these challenges.\nMesh-Tensorﬂow [ 34] follows the SPMD paradigm, which extends the Single Instruction Multiple\nData (SIMD) approach used for data parallelism to other tensor dimensions. SPMD allows splitting\nevery computation across multiple devices, allowing the user to scale the size of individual matrix\nmultiplications (and thus, the model parameters of individual layers) linearly with the number of\naccelerators. However, this also introduces high communication overhead between the accelerators\ndue to an abundance of AllReduce-like operations used to combine the outputs of each parallelized\nmatrix multiplication. This limits the applicability of the approach to scenarios where accelerators\nare connected with high speed interconnects. Further, SPMD limits the type of operations that can be\nefﬁciently scaled, restricting its use to a speciﬁc set of network architectures and machine learning\ntasks. For example, splitting along the channel dimension of convolution layers under this paradigm\nis not efﬁcient given that channels are effectively fully connected, whereas splitting along the spatial\ndimension requires sophisticated techniques for the halo regions. While SPMD allows scaling the\nmodel depth by making each operation smaller, it requires splitting each layer over a larger number\nof accelerators, which in turn further increases the communication overhead across devices.\nOther approaches have attempted to utilize pipeline-parallelism-based approaches to scale neural\nnetworks [ 46,47]. The most recent iteration of pipeline parallelism applied to neural network\ntraining is PipeDream [ 48], which targets reducing the communication overhead for parameter\nservers [ 49]. PipeDream pipelines the execution of forward passes and intersperses them with\nbackward passes in an attempt to maximize hardware utilization. This design suffers from weight\nstaleness introduced by asynchronous backward updates. To avoid optimization issues stemming\nfrom the weight staleness, PipeDream requires maintaining multiple versioned copies of the model\nparameters on each accelerator in order to compute the gradient updates accurately, preventing users\nfrom scaling to bigger models.\nGPipe introduces a new brand of pipeline parallelism that pipelines the execution of micro-batches\nbefore applying a single synchronous gradient update for the entire mini-batch . Our novel batch-\nsplitting pipeline parallelism algorithm, when combined with re-materialization, allows scaling\nto a large number of micro-batches. This minimizes the bubble overhead without the need for\nasynchronous gradient updates. GPipe enables the user to scale model size linearly with the number\nof accelerators used. Unlike SPMD, pipeline parallelism introduces little additional communication\noverhead when scaling the model. Inter-device communication only takes place at partition boundaries\nfor every micro-batch and the introduced communication overhead is marginal, extending the utility\nof GPipe to situations where high-speed device interconnects are not available. However, GPipe\ncurrently assumes that a single layer ﬁts within the memory requirements of a single accelerator3.\nAdditionally, micro-batch splitting requires complicated strategies to support layers that require\n3One possible way around this limitation is splitting a single matrix-multiplication into smaller ones and\nspreading them sequentially across multiple layers.\n8computations across the batch (for example, BatchNorm uses statistics over the micro-batch during\ntraining, but accumulates mini-batch statistics for evaluation).\n7 Conclusion\nIn this work, we introduce GPipe, a scalable model-parallelism library for training giant neural\nnetworks. We propose and implement a novel batch-splitting pipeline-parallelism algorithm that uses\nsynchronous gradient updates, allowing model parallelism with high hardware utilization and training\nstability. We leverage GPipe to train large-scale convolutional and transformer-based models and\ndemonstrate strong empirical results on both image classiﬁcation and multilingual machine translation.\nWe highlight three key attributes of the GPipe library: 1) Efﬁciency: Using a novel batch-splitting\npipelining algorithm, GPipe achieves almost linear speedup with the number of devices. 2) Flexibility:\nGPipe supports any deep network that can be represented as a sequence of layers. 3) Reliability:\nGPipe utilizes synchronous gradient descent and guarantees consistent training regardless of the\nnumber of partitions.\nReferences\n[1] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation:\nContextualized word vectors. CoRR , abs/1708.00107, 2017.\n[2]Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,\nand Luke Zettlemoyer. Deep contextualized word representations. In ACL, 2018.\n[3]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 ,\n2018.\n[4]Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. 2019.\n[5]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In CVPR . IEEE, 2009.\n[6]Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, and et al. Going deeper with\nconvolutions. In CVPR , pages 1–9, 2015.\n[7]Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re-\nthinking the inception architecture for computer vision. In CVPR , pages 2818–2826, 2016.\n[8]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\nnetworks. In European conference on computer vision , pages 630–645. Springer, 2016.\n[9]Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual\ntransformations for deep neural networks. In CVPR , 2017.\n[10] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. CVPR , 2018.\n[11] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable\narchitectures for scalable image recognition. CVPR , 2018.\n[12] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for\nimage classiﬁer architecture search. arXiv preprint arXiv:1802.01548 , 2018.\n[13] Andreas Griewank and Andrea Walther. Algorithm 799: revolve: an implementation of check-\npointing for the reverse or adjoint mode of computational differentiation. ACM Transactions on\nMathematical Software (TOMS) , 26(1):19–45, 2000.\n[14] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear\nmemory cost. arXiv preprint arXiv:1604.06174 , 2016.\n[15] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neurips , pages 5998–6008,\n2017.\n[16] Jonathan Shen, Patrick Nguyen, Yonghui Wu, Zhifeng Chen, Mia Xu Chen, Ye Jia, Anjuli\nKannan, Tara Sainath, Yuan Cao, Chung-Cheng Chiu, et al. Lingvo: a modular and scalable\nframework for sequence-to-sequence modeling. arXiv preprint arXiv:1902.08295 , 2019.\n9[17] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick,\nSergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature\nembedding. In Proceedings of the 22nd ACM international conference on Multimedia , pages\n675–678. ACM, 2014.\n[18] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu,\nChiyuan Zhang, and Zheng Zhang. Mxnet: A ﬂexible and efﬁcient machine learning library for\nheterogeneous distributed systems. arXiv preprint arXiv:1512.01274 , 2015.\n[19] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\npytorch. 2017.\n[20] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training\nby reducing internal covariate shift. ICML , 2015.\n[21] Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu Zhang, Kai Jia, Gang Yu, and Jian\nSun. Megdet: A large mini-batch object detector. CVPR , 7, 2017.\n[22] Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn features\noff-the-shelf: An astounding baseline for recognition. In CVPR Workshops , pages 512–519,\n2014.\n[23] Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully convolutional networks for semantic\nsegmentation. IEEE Trans. Pattern Anal. Mach. Intell. , 39(4):640–651, 2017.\n[24] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural\nnetworks with cutout. arXiv preprint arXiv:1708.04552 , 2017.\n[25] Simon Kornblith, Jonathon Shlens, and Quoc V . Le. Do better imagenet models transfer better?\nCoRR , abs/1805.08974, 2018.\n[26] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:\nLearning augmentation policies from data. arXiv preprint arXiv:1805.09501 , 2018.\n[27] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\nAshwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised\npretraining. ECCV , 2018.\n[28] Jiquan Ngiam, Daiyi Peng, Vijay Vasudevan, Simon Kornblith, Quoc Le, and Ruoming Pang.\nDomain adaptive transfer learning. 2018.\n[29] Yuxin Peng, Xiangteng He, and Junjie Zhao. Object-part attention model for ﬁne-grained image\nclassiﬁcation. IEEE Transactions on Image Processing , 27(3):1487–1500, 2018.\n[30] Yin Cui, Yang Song, Chen Sun, Andrew Howard, and Serge Belongie. Large scale ﬁne-grained\ncategorization and domain-speciﬁc transfer learning. In CVPR , 2018.\n[31] Fisher Yu, Dequan Wang, and Trevor Darrell. Deep layer aggregation. In CVPR , 2018.\n[32] Xiu-Shen Wei, Chen-Wei Xie, Jianxin Wu, and Chunhua Shen. Mask-cnn: Localizing parts\nand selecting descriptors for ﬁne-grained bird species categorization. Pattern Recognition ,\n76:704–714, 2018.\n[33] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. CoRR , abs/1705.03122, 2017.\n[34] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanan-\ntakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorﬂow:\nDeep learning for supercomputers. In Neurips , pages 10414–10423, 2018.\n[35] Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster,\nLlion Jones, Niki Parmar, Mike Schuster, Zhifeng Chen, Yonghui Wu, and Macduff Hughes.\nThe best of both worlds: Combining recent advances in neural machine translation. CoRR ,\nabs/1804.09849, 2018.\n[36] Felix Wu, Angela Fan, Alexei Baevski, Yann N. Dauphin, and Michael Auli. Pay less attention\nwith lightweight and dynamic convolutions. CoRR , abs/1901.10430, 2019.\n[37] Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim\nKrikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. Massively multilingual neu-\nral machine translation in the wild: Findings and challenges. arXiv preprint arXiv:1907.05019 ,\n2019.\n10[38] Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning\nwithout normalization. arXiv preprint arXiv:1901.09321 , 2019.\n[39] Nitish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping T.P. Tang.\nOn large-batch training for deep learning: Generalization gap and sharp minima. CoRR ,\nabs/1609.04836, 2016.\n[40] Samuel L. Smith, Pieter-Jan Kindermans, and Quoc V . Le. Don’t decay the learning rate,\nincrease the batch size. CoRR , abs/1711.00489, 2017.\n[41] Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation.\nInProceedings of the Third Conference on Machine Translation: Research Papers , pages 1–9,\nBelgium, Brussels, October 2018. Association for Computational Linguistics.\n[42] Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv\npreprint arXiv:1404.5997 , 2014.\n[43] Seunghak Lee, Jin Kyu Kim, Xun Zheng, Qirong Ho, Garth A Gibson, and Eric P Xing. On\nmodel parallelization and scheduling strategies for distributed machine learning. In Neurips ,\npages 2834–2842, 2014.\n[44] Azalia Mirhoseini, Hieu Pham, Quoc V Le, Benoit Steiner, Rasmus Larsen, Yuefeng Zhou,\nNaveen Kumar, Mohammad Norouzi, Samy Bengio, and Jeff Dean. Device placement opti-\nmization with reinforcement learning. arXiv preprint arXiv:1706.04972 , 2017.\n[45] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc aurelio\nRanzato, Andrew Senior, Paul Tucker, Ke Yang, Quoc V . Le, and Andrew Y . Ng. Large scale\ndistributed deep networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger,\neditors, Neurips 25 , pages 1223–1231. Curran Associates, Inc., 2012.\n[46] A. Petrowski, G. Dreyfus, and C. Girault. Performance analysis of a pipelined backpropagation\nparallel algorithm. IEEE Transactions on Neural Networks , 4(6):970–981, Nov 1993.\n[47] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\ntranslation system: Bridging the gap between human and machine translation. Transactions of\nthe Association for Computational Linguistics, , 2017.\n[48] Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg\nGanger, and Phil Gibbons. Pipedream: Fast and efﬁcient pipeline parallel dnn training. arXiv\npreprint arXiv:1806.03377 , 2018.\n[49] Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja Josifovski,\nJames Long, Eugene J Shekita, and Bor-Yiing Su. Scaling distributed machine learning with\nthe parameter server. In OSDI , volume 14, pages 583–598, 2014.\n11\n\n\n\nDeep Residual Learning for Image Recognition\nKaiming He Xiangyu Zhang Shaoqing Ren Jian Sun\nMicrosoft Research\nfkahe, v-xiangz, v-shren, jiansun g@microsoft.com\nAbstract\nDeeper neural networks are more difﬁcult to train. We\npresent a residual learning framework to ease the training\nof networks that are substantially deeper than those used\npreviously. We explicitly reformulate the layers as learn-\ning residual functions with reference to the layer inputs, in-\nstead of learning unreferenced functions. We provide com-\nprehensive empirical evidence showing that these residual\nnetworks are easier to optimize, and can gain accuracy from\nconsiderably increased depth. On the ImageNet dataset we\nevaluate residual nets with a depth of up to 152 layers—8 \u0002\ndeeper than VGG nets [41] but still having lower complex-\nity. An ensemble of these residual nets achieves 3.57% error\non the ImageNet testset. This result won the 1st place on the\nILSVRC 2015 classiﬁcation task. We also present analysis\non CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance\nfor many visual recognition tasks. Solely due to our ex-\ntremely deep representations, we obtain a 28% relative im-\nprovement on the COCO object detection dataset. Deep\nresidual nets are foundations of our submissions to ILSVRC\n& COCO 2015 competitions1, where we also won the 1st\nplaces on the tasks of ImageNet detection, ImageNet local-\nization, COCO detection, and COCO segmentation.\n1. Introduction\nDeep convolutional neural networks [22, 21] have led\nto a series of breakthroughs for image classiﬁcation [21,\n50, 40]. Deep networks naturally integrate low/mid/high-\nlevel features [50] and classiﬁers in an end-to-end multi-\nlayer fashion, and the “levels” of features can be enriched\nby the number of stacked layers (depth). Recent evidence\n[41, 44] reveals that network depth is of crucial importance,\nand the leading results [41, 44, 13, 16] on the challenging\nImageNet dataset [36] all exploit “very deep” [41] models,\nwith a depth of sixteen [41] to thirty [16]. Many other non-\ntrivial visual recognition tasks [8, 12, 7, 32, 27] have also\n1http://image-net.org/challenges/LSVRC/2015/ and\nhttp://mscoco.org/dataset/#detections-challenge2015 .\n0 1 2 3 4 5 60 1020\niter. (1e4)training error (%)\n  \n0 1 2 3 4 5 601020\niter. (1e4)test error (%)\n  \n56-layer\n20-layer56-layer\n20-layerFigure 1. Training error (left) and test error (right) on CIFAR-10\nwith 20-layer and 56-layer “plain” networks. The deeper network\nhas higher training error, and thus test error. Similar phenomena\non ImageNet is presented in Fig. 4.\ngreatly beneﬁted from very deep models.\nDriven by the signiﬁcance of depth, a question arises: Is\nlearning better networks as easy as stacking more layers?\nAn obstacle to answering this question was the notorious\nproblem of vanishing/exploding gradients [1, 9], which\nhamper convergence from the beginning. This problem,\nhowever, has been largely addressed by normalized initial-\nization [23, 9, 37, 13] and intermediate normalization layers\n[16], which enable networks with tens of layers to start con-\nverging for stochastic gradient descent (SGD) with back-\npropagation [22].\nWhen deeper networks are able to start converging, a\ndegradation problem has been exposed: with the network\ndepth increasing, accuracy gets saturated (which might be\nunsurprising) and then degrades rapidly. Unexpectedly,\nsuch degradation is not caused by overﬁtting , and adding\nmore layers to a suitably deep model leads to higher train-\ning error , as reported in [11, 42] and thoroughly veriﬁed by\nour experiments. Fig. 1 shows a typical example.\nThe degradation (of training accuracy) indicates that not\nall systems are similarly easy to optimize. Let us consider a\nshallower architecture and its deeper counterpart that adds\nmore layers onto it. There exists a solution by construction\nto the deeper model: the added layers are identity mapping,\nand the other layers are copied from the learned shallower\nmodel. The existence of this constructed solution indicates\nthat a deeper model should produce no higher training error\nthan its shallower counterpart. But experiments show that\nour current solvers on hand are unable to ﬁnd solutions that\n1arXiv:1512.03385v1  [cs.CV]  10 Dec 2015identityweight layer\nweight layerrelu\nreluF(x)\u0001+\u0001xx\nF(x)xFigure 2. Residual learning: a building block.\nare comparably good or better than the constructed solution\n(or unable to do so in feasible time).\nIn this paper, we address the degradation problem by\nintroducing a deep residual learning framework. In-\nstead of hoping each few stacked layers directly ﬁt a\ndesired underlying mapping, we explicitly let these lay-\ners ﬁt a residual mapping. Formally, denoting the desired\nunderlying mapping as H(x), we let the stacked nonlinear\nlayers ﬁt another mapping of F(x) :=H(x)\u0000x. The orig-\ninal mapping is recast into F(x)+x. We hypothesize that it\nis easier to optimize the residual mapping than to optimize\nthe original, unreferenced mapping. To the extreme, if an\nidentity mapping were optimal, it would be easier to push\nthe residual to zero than to ﬁt an identity mapping by a stack\nof nonlinear layers.\nThe formulation of F(x)+xcan be realized by feedfor-\nward neural networks with “shortcut connections” (Fig. 2).\nShortcut connections [2, 34, 49] are those skipping one or\nmore layers. In our case, the shortcut connections simply\nperform identity mapping, and their outputs are added to\nthe outputs of the stacked layers (Fig. 2). Identity short-\ncut connections add neither extra parameter nor computa-\ntional complexity. The entire network can still be trained\nend-to-end by SGD with backpropagation, and can be eas-\nily implemented using common libraries ( e.g., Caffe [19])\nwithout modifying the solvers.\nWe present comprehensive experiments on ImageNet\n[36] to show the degradation problem and evaluate our\nmethod. We show that: 1) Our extremely deep residual nets\nare easy to optimize, but the counterpart “plain” nets (that\nsimply stack layers) exhibit higher training error when the\ndepth increases; 2) Our deep residual nets can easily enjoy\naccuracy gains from greatly increased depth, producing re-\nsults substantially better than previous networks.\nSimilar phenomena are also shown on the CIFAR-10 set\n[20], suggesting that the optimization difﬁculties and the\neffects of our method are not just akin to a particular dataset.\nWe present successfully trained models on this dataset with\nover 100 layers, and explore models with over 1000 layers.\nOn the ImageNet classiﬁcation dataset [36], we obtain\nexcellent results by extremely deep residual nets. Our 152-\nlayer residual net is the deepest network ever presented on\nImageNet, while still having lower complexity than VGG\nnets [41]. Our ensemble has 3.57% top-5 error on theImageNet testset, and won the 1st place in the ILSVRC\n2015 classiﬁcation competition . The extremely deep rep-\nresentations also have excellent generalization performance\non other recognition tasks, and lead us to further win the\n1st places on: ImageNet detection, ImageNet localization,\nCOCO detection, and COCO segmentation in ILSVRC &\nCOCO 2015 competitions. This strong evidence shows that\nthe residual learning principle is generic, and we expect that\nit is applicable in other vision and non-vision problems.\n2. Related Work\nResidual Representations. In image recognition, VLAD\n[18] is a representation that encodes by the residual vectors\nwith respect to a dictionary, and Fisher Vector [30] can be\nformulated as a probabilistic version [18] of VLAD. Both\nof them are powerful shallow representations for image re-\ntrieval and classiﬁcation [4, 48]. For vector quantization,\nencoding residual vectors [17] is shown to be more effec-\ntive than encoding original vectors.\nIn low-level vision and computer graphics, for solv-\ning Partial Differential Equations (PDEs), the widely used\nMultigrid method [3] reformulates the system as subprob-\nlems at multiple scales, where each subproblem is respon-\nsible for the residual solution between a coarser and a ﬁner\nscale. An alternative to Multigrid is hierarchical basis pre-\nconditioning [45, 46], which relies on variables that repre-\nsent residual vectors between two scales. It has been shown\n[3, 45, 46] that these solvers converge much faster than stan-\ndard solvers that are unaware of the residual nature of the\nsolutions. These methods suggest that a good reformulation\nor preconditioning can simplify the optimization.\nShortcut Connections. Practices and theories that lead to\nshortcut connections [2, 34, 49] have been studied for a long\ntime. An early practice of training multi-layer perceptrons\n(MLPs) is to add a linear layer connected from the network\ninput to the output [34, 49]. In [44, 24], a few interme-\ndiate layers are directly connected to auxiliary classiﬁers\nfor addressing vanishing/exploding gradients. The papers\nof [39, 38, 31, 47] propose methods for centering layer re-\nsponses, gradients, and propagated errors, implemented by\nshortcut connections. In [44], an “inception” layer is com-\nposed of a shortcut branch and a few deeper branches.\nConcurrent with our work, “highway networks” [42, 43]\npresent shortcut connections with gating functions [15].\nThese gates are data-dependent and have parameters, in\ncontrast to our identity shortcuts that are parameter-free.\nWhen a gated shortcut is “closed” (approaching zero), the\nlayers in highway networks represent non-residual func-\ntions. On the contrary, our formulation always learns\nresidual functions; our identity shortcuts are never closed,\nand all information is always passed through, with addi-\ntional residual functions to be learned. In addition, high-\n2way networks have not demonstrated accuracy gains with\nextremely increased depth ( e.g., over 100 layers).\n3. Deep Residual Learning\n3.1. Residual Learning\nLet us considerH(x)as an underlying mapping to be\nﬁt by a few stacked layers (not necessarily the entire net),\nwithxdenoting the inputs to the ﬁrst of these layers. If one\nhypothesizes that multiple nonlinear layers can asymptoti-\ncally approximate complicated functions2, then it is equiv-\nalent to hypothesize that they can asymptotically approxi-\nmate the residual functions, i.e.,H(x)\u0000x(assuming that\nthe input and output are of the same dimensions). So\nrather than expect stacked layers to approximate H(x), we\nexplicitly let these layers approximate a residual function\nF(x) :=H(x)\u0000x. The original function thus becomes\nF(x)+x. Although both forms should be able to asymptot-\nically approximate the desired functions (as hypothesized),\nthe ease of learning might be different.\nThis reformulation is motivated by the counterintuitive\nphenomena about the degradation problem (Fig. 1, left). As\nwe discussed in the introduction, if the added layers can\nbe constructed as identity mappings, a deeper model should\nhave training error no greater than its shallower counter-\npart. The degradation problem suggests that the solvers\nmight have difﬁculties in approximating identity mappings\nby multiple nonlinear layers. With the residual learning re-\nformulation, if identity mappings are optimal, the solvers\nmay simply drive the weights of the multiple nonlinear lay-\ners toward zero to approach identity mappings.\nIn real cases, it is unlikely that identity mappings are op-\ntimal, but our reformulation may help to precondition the\nproblem. If the optimal function is closer to an identity\nmapping than to a zero mapping, it should be easier for the\nsolver to ﬁnd the perturbations with reference to an identity\nmapping, than to learn the function as a new one. We show\nby experiments (Fig. 7) that the learned residual functions in\ngeneral have small responses, suggesting that identity map-\npings provide reasonable preconditioning.\n3.2. Identity Mapping by Shortcuts\nWe adopt residual learning to every few stacked layers.\nA building block is shown in Fig. 2. Formally, in this paper\nwe consider a building block deﬁned as:\ny=F(x;fWig) +x: (1)\nHere xandyare the input and output vectors of the lay-\ners considered. The function F(x;fWig)represents the\nresidual mapping to be learned. For the example in Fig. 2\nthat has two layers, F=W2\u001b(W1x)in which\u001bdenotes\n2This hypothesis, however, is still an open question. See [28].ReLU [29] and the biases are omitted for simplifying no-\ntations. The operation F+xis performed by a shortcut\nconnection and element-wise addition. We adopt the sec-\nond nonlinearity after the addition ( i.e.,\u001b(y), see Fig. 2).\nThe shortcut connections in Eqn.(1) introduce neither ex-\ntra parameter nor computation complexity. This is not only\nattractive in practice but also important in our comparisons\nbetween plain and residual networks. We can fairly com-\npare plain/residual networks that simultaneously have the\nsame number of parameters, depth, width, and computa-\ntional cost (except for the negligible element-wise addition).\nThe dimensions of xandFmust be equal in Eqn.(1).\nIf this is not the case ( e.g., when changing the input/output\nchannels), we can perform a linear projection Wsby the\nshortcut connections to match the dimensions:\ny=F(x;fWig) +Wsx: (2)\nWe can also use a square matrix Wsin Eqn.(1). But we will\nshow by experiments that the identity mapping is sufﬁcient\nfor addressing the degradation problem and is economical,\nand thusWsis only used when matching dimensions.\nThe form of the residual function Fis ﬂexible. Exper-\niments in this paper involve a function Fthat has two or\nthree layers (Fig. 5), while more layers are possible. But if\nFhas only a single layer, Eqn.(1) is similar to a linear layer:\ny=W1x+x, for which we have not observed advantages.\nWe also note that although the above notations are about\nfully-connected layers for simplicity, they are applicable to\nconvolutional layers. The function F(x;fWig)can repre-\nsent multiple convolutional layers. The element-wise addi-\ntion is performed on two feature maps, channel by channel.\n3.3. Network Architectures\nWe have tested various plain/residual nets, and have ob-\nserved consistent phenomena. To provide instances for dis-\ncussion, we describe two models for ImageNet as follows.\nPlain Network. Our plain baselines (Fig. 3, middle) are\nmainly inspired by the philosophy of VGG nets [41] (Fig. 3,\nleft). The convolutional layers mostly have 3 \u00023 ﬁlters and\nfollow two simple design rules: (i) for the same output\nfeature map size, the layers have the same number of ﬁl-\nters; and (ii) if the feature map size is halved, the num-\nber of ﬁlters is doubled so as to preserve the time com-\nplexity per layer. We perform downsampling directly by\nconvolutional layers that have a stride of 2. The network\nends with a global average pooling layer and a 1000-way\nfully-connected layer with softmax. The total number of\nweighted layers is 34 in Fig. 3 (middle).\nIt is worth noticing that our model has fewer ﬁlters and\nlower complexity than VGG nets [41] (Fig. 3, left). Our 34-\nlayer baseline has 3.6 billion FLOPs (multiply-adds), which\nis only 18% of VGG-19 (19.6 billion FLOPs).\n37x7 conv, 64, /2\npool, /2\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 128, /2\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 256, /2\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 512, /2\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\navg pool\nfc 1000image\n3x3 conv, 5123x3 conv, 64\n3x3 conv, 64\npool, /2\n3x3 conv, 128\n3x3 conv, 128\npool, /2\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\npool, /2\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\npool, /2\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\npool, /2\nfc 4096\nfc 4096\nfc 1000image\noutput \nsize: 112output \nsize: 224\noutput \nsize: 56\noutput \nsize: 28\noutput \nsize: 14\noutput \nsize: 7\noutput \nsize: 1VGG-19 34-layer plain\n7x7 conv, 64, /2\npool, /2\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 128, /2\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 256, /2\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 512, /2\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\navg pool\nfc 1000image34-layer residualFigure 3. Example network architectures for ImageNet. Left: the\nVGG-19 model [41] (19.6 billion FLOPs) as a reference. Mid-\ndle: a plain network with 34 parameter layers (3.6 billion FLOPs).\nRight : a residual network with 34 parameter layers (3.6 billion\nFLOPs). The dotted shortcuts increase dimensions. Table 1 shows\nmore details and other variants.Residual Network. Based on the above plain network, we\ninsert shortcut connections (Fig. 3, right) which turn the\nnetwork into its counterpart residual version. The identity\nshortcuts (Eqn.(1)) can be directly used when the input and\noutput are of the same dimensions (solid line shortcuts in\nFig. 3). When the dimensions increase (dotted line shortcuts\nin Fig. 3), we consider two options: (A) The shortcut still\nperforms identity mapping, with extra zero entries padded\nfor increasing dimensions. This option introduces no extra\nparameter; (B) The projection shortcut in Eqn.(2) is used to\nmatch dimensions (done by 1 \u00021 convolutions). For both\noptions, when the shortcuts go across feature maps of two\nsizes, they are performed with a stride of 2.\n3.4. Implementation\nOur implementation for ImageNet follows the practice\nin [21, 41]. The image is resized with its shorter side ran-\ndomly sampled in [256;480] for scale augmentation [41].\nA 224\u0002224 crop is randomly sampled from an image or its\nhorizontal ﬂip, with the per-pixel mean subtracted [21]. The\nstandard color augmentation in [21] is used. We adopt batch\nnormalization (BN) [16] right after each convolution and\nbefore activation, following [16]. We initialize the weights\nas in [13] and train all plain/residual nets from scratch. We\nuse SGD with a mini-batch size of 256. The learning rate\nstarts from 0.1 and is divided by 10 when the error plateaus,\nand the models are trained for up to 60\u0002104iterations. We\nuse a weight decay of 0.0001 and a momentum of 0.9. We\ndo not use dropout [14], following the practice in [16].\nIn testing, for comparison studies we adopt the standard\n10-crop testing [21]. For best results, we adopt the fully-\nconvolutional form as in [41, 13], and average the scores\nat multiple scales (images are resized such that the shorter\nside is inf224;256;384;480;640g).\n4. Experiments\n4.1. ImageNet Classiﬁcation\nWe evaluate our method on the ImageNet 2012 classiﬁ-\ncation dataset [36] that consists of 1000 classes. The models\nare trained on the 1.28 million training images, and evalu-\nated on the 50k validation images. We also obtain a ﬁnal\nresult on the 100k test images, reported by the test server.\nWe evaluate both top-1 and top-5 error rates.\nPlain Networks. We ﬁrst evaluate 18-layer and 34-layer\nplain nets. The 34-layer plain net is in Fig. 3 (middle). The\n18-layer plain net is of a similar form. See Table 1 for de-\ntailed architectures.\nThe results in Table 2 show that the deeper 34-layer plain\nnet has higher validation error than the shallower 18-layer\nplain net. To reveal the reasons, in Fig. 4 (left) we com-\npare their training/validation errors during the training pro-\ncedure. We have observed the degradation problem - the\n4layer name output size 18-layer 34-layer 50-layer 101-layer 152-layer\nconv1 112\u0002112 7\u00027, 64, stride 2\nconv2 x 56\u0002563\u00023 max pool, stride 2\n\u0014\n3\u00023, 64\n3\u00023, 64\u0015\n\u00022\u0014\n3\u00023, 64\n3\u00023, 64\u0015\n\u000232\n41\u00021, 64\n3\u00023, 64\n1\u00021, 2563\n5\u000232\n41\u00021, 64\n3\u00023, 64\n1\u00021, 2563\n5\u000232\n41\u00021, 64\n3\u00023, 64\n1\u00021, 2563\n5\u00023\nconv3 x 28\u000228\u0014\n3\u00023, 128\n3\u00023, 128\u0015\n\u00022\u0014\n3\u00023, 128\n3\u00023, 128\u0015\n\u000242\n41\u00021, 128\n3\u00023, 128\n1\u00021, 5123\n5\u000242\n41\u00021, 128\n3\u00023, 128\n1\u00021, 5123\n5\u000242\n41\u00021, 128\n3\u00023, 128\n1\u00021, 5123\n5\u00028\nconv4 x 14\u000214\u0014\n3\u00023, 256\n3\u00023, 256\u0015\n\u00022\u0014\n3\u00023, 256\n3\u00023, 256\u0015\n\u000262\n41\u00021, 256\n3\u00023, 256\n1\u00021, 10243\n5\u000262\n41\u00021, 256\n3\u00023, 256\n1\u00021, 10243\n5\u0002232\n41\u00021, 256\n3\u00023, 256\n1\u00021, 10243\n5\u000236\nconv5 x 7\u00027\u0014\n3\u00023, 512\n3\u00023, 512\u0015\n\u00022\u0014\n3\u00023, 512\n3\u00023, 512\u0015\n\u000232\n41\u00021, 512\n3\u00023, 512\n1\u00021, 20483\n5\u000232\n41\u00021, 512\n3\u00023, 512\n1\u00021, 20483\n5\u000232\n41\u00021, 512\n3\u00023, 512\n1\u00021, 20483\n5\u00023\n1\u00021 average pool, 1000-d fc, softmax\nFLOPs 1.8\u00021093.6\u00021093.8\u00021097.6\u000210911.3\u0002109\nTable 1. Architectures for ImageNet. Building blocks are shown in brackets (see also Fig. 5), with the numbers of blocks stacked. Down-\nsampling is performed by conv3 1, conv4 1, and conv5 1 with a stride of 2.\n0 10 20 30 40 502030405060\niter. (1e4)error (%)\n  \nplain-18\nplain-34\n0 10 20 30 40 502030405060\niter. (1e4)error (%)\n  \nResNet-18\nResNet-3418-layer34-layer\n18-layer\n34-layer\nFigure 4. Training on ImageNet . Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain\nnetworks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to\ntheir plain counterparts.\nplain ResNet\n18 layers 27.94 27.88\n34 layers 28.54 25.03\nTable 2. Top-1 error (%, 10-crop testing) on ImageNet validation.\nHere the ResNets have no extra parameter compared to their plain\ncounterparts. Fig. 4 shows the training procedures.\n34-layer plain net has higher training error throughout the\nwhole training procedure, even though the solution space\nof the 18-layer plain network is a subspace of that of the\n34-layer one.\nWe argue that this optimization difﬁculty is unlikely to\nbe caused by vanishing gradients. These plain networks are\ntrained with BN [16], which ensures forward propagated\nsignals to have non-zero variances. We also verify that the\nbackward propagated gradients exhibit healthy norms with\nBN. So neither forward nor backward signals vanish. In\nfact, the 34-layer plain net is still able to achieve compet-\nitive accuracy (Table 3), suggesting that the solver works\nto some extent. We conjecture that the deep plain nets may\nhave exponentially low convergence rates, which impact thereducing of the training error3. The reason for such opti-\nmization difﬁculties will be studied in the future.\nResidual Networks. Next we evaluate 18-layer and 34-\nlayer residual nets ( ResNets ). The baseline architectures\nare the same as the above plain nets, expect that a shortcut\nconnection is added to each pair of 3 \u00023 ﬁlters as in Fig. 3\n(right). In the ﬁrst comparison (Table 2 and Fig. 4 right),\nwe use identity mapping for all shortcuts and zero-padding\nfor increasing dimensions (option A). So they have no extra\nparameter compared to the plain counterparts.\nWe have three major observations from Table 2 and\nFig. 4. First, the situation is reversed with residual learn-\ning – the 34-layer ResNet is better than the 18-layer ResNet\n(by 2.8%). More importantly, the 34-layer ResNet exhibits\nconsiderably lower training error and is generalizable to the\nvalidation data. This indicates that the degradation problem\nis well addressed in this setting and we manage to obtain\naccuracy gains from increased depth.\nSecond, compared to its plain counterpart, the 34-layer\n3We have experimented with more training iterations (3 \u0002) and still ob-\nserved the degradation problem, suggesting that this problem cannot be\nfeasibly addressed by simply using more iterations.\n5model top-1 err. top-5 err.\nVGG-16 [41] 28.07 9.33\nGoogLeNet [44] - 9.15\nPReLU-net [13] 24.27 7.38\nplain-34 28.54 10.02\nResNet-34 A 25.03 7.76\nResNet-34 B 24.52 7.46\nResNet-34 C 24.19 7.40\nResNet-50 22.85 6.71\nResNet-101 21.75 6.05\nResNet-152 21.43 5.71\nTable 3. Error rates (%, 10-crop testing) on ImageNet validation.\nVGG-16 is based on our test. ResNet-50/101/152 are of option B\nthat only uses projections for increasing dimensions.\nmethod top-1 err. top-5 err.\nVGG [41] (ILSVRC’14) - 8.43y\nGoogLeNet [44] (ILSVRC’14) - 7.89\nVGG [41] (v5) 24.4 7.1\nPReLU-net [13] 21.59 5.71\nBN-inception [16] 21.99 5.81\nResNet-34 B 21.84 5.71\nResNet-34 C 21.53 5.60\nResNet-50 20.74 5.25\nResNet-101 19.87 4.60\nResNet-152 19.38 4.49\nTable 4. Error rates (%) of single-model results on the ImageNet\nvalidation set (exceptyreported on the test set).\nmethod top-5 err. ( test)\nVGG [41] (ILSVRC’14) 7.32\nGoogLeNet [44] (ILSVRC’14) 6.66\nVGG [41] (v5) 6.8\nPReLU-net [13] 4.94\nBN-inception [16] 4.82\nResNet (ILSVRC’15) 3.57\nTable 5. Error rates (%) of ensembles . The top-5 error is on the\ntest set of ImageNet and reported by the test server.\nResNet reduces the top-1 error by 3.5% (Table 2), resulting\nfrom the successfully reduced training error (Fig. 4 right vs.\nleft). This comparison veriﬁes the effectiveness of residual\nlearning on extremely deep systems.\nLast, we also note that the 18-layer plain/residual nets\nare comparably accurate (Table 2), but the 18-layer ResNet\nconverges faster (Fig. 4 right vs. left). When the net is “not\noverly deep” (18 layers here), the current SGD solver is still\nable to ﬁnd good solutions to the plain net. In this case, the\nResNet eases the optimization by providing faster conver-\ngence at the early stage.\nIdentity vs. Projection Shortcuts. We have shown that\n3x3, 641x1, 64\nrelu\n1x1, 256relu\nrelu3x3, 64\n3x3, 64\nrelurelu64-d 256-dFigure 5. A deeper residual function Ffor ImageNet. Left: a\nbuilding block (on 56 \u000256 feature maps) as in Fig. 3 for ResNet-\n34. Right: a “bottleneck” building block for ResNet-50/101/152.\nparameter-free, identity shortcuts help with training. Next\nwe investigate projection shortcuts (Eqn.(2)). In Table 3 we\ncompare three options: (A) zero-padding shortcuts are used\nfor increasing dimensions, and all shortcuts are parameter-\nfree (the same as Table 2 and Fig. 4 right); (B) projec-\ntion shortcuts are used for increasing dimensions, and other\nshortcuts are identity; and (C) all shortcuts are projections.\nTable 3 shows that all three options are considerably bet-\nter than the plain counterpart. B is slightly better than A. We\nargue that this is because the zero-padded dimensions in A\nindeed have no residual learning. C is marginally better than\nB, and we attribute this to the extra parameters introduced\nby many (thirteen) projection shortcuts. But the small dif-\nferences among A/B/C indicate that projection shortcuts are\nnot essential for addressing the degradation problem. So we\ndo not use option C in the rest of this paper, to reduce mem-\nory/time complexity and model sizes. Identity shortcuts are\nparticularly important for not increasing the complexity of\nthe bottleneck architectures that are introduced below.\nDeeper Bottleneck Architectures. Next we describe our\ndeeper nets for ImageNet. Because of concerns on the train-\ning time that we can afford, we modify the building block\nas a bottleneck design4. For each residual function F, we\nuse a stack of 3 layers instead of 2 (Fig. 5). The three layers\nare 1\u00021, 3\u00023, and 1\u00021 convolutions, where the 1 \u00021 layers\nare responsible for reducing and then increasing (restoring)\ndimensions, leaving the 3 \u00023 layer a bottleneck with smaller\ninput/output dimensions. Fig. 5 shows an example, where\nboth designs have similar time complexity.\nThe parameter-free identity shortcuts are particularly im-\nportant for the bottleneck architectures. If the identity short-\ncut in Fig. 5 (right) is replaced with projection, one can\nshow that the time complexity and model size are doubled,\nas the shortcut is connected to the two high-dimensional\nends. So identity shortcuts lead to more efﬁcient models\nfor the bottleneck designs.\n50-layer ResNet: We replace each 2-layer block in the\n4Deeper non-bottleneck ResNets ( e.g., Fig. 5 left) also gain accuracy\nfrom increased depth (as shown on CIFAR-10), but are not as economical\nas the bottleneck ResNets. So the usage of bottleneck designs is mainly due\nto practical considerations. We further note that the degradation problem\nof plain nets is also witnessed for the bottleneck designs.\n634-layer net with this 3-layer bottleneck block, resulting in\na 50-layer ResNet (Table 1). We use option B for increasing\ndimensions. This model has 3.8 billion FLOPs.\n101-layer and 152-layer ResNets: We construct 101-\nlayer and 152-layer ResNets by using more 3-layer blocks\n(Table 1). Remarkably, although the depth is signiﬁcantly\nincreased, the 152-layer ResNet (11.3 billion FLOPs) still\nhaslower complexity than VGG-16/19 nets (15.3/19.6 bil-\nlion FLOPs).\nThe 50/101/152-layer ResNets are more accurate than\nthe 34-layer ones by considerable margins (Table 3 and 4).\nWe do not observe the degradation problem and thus en-\njoy signiﬁcant accuracy gains from considerably increased\ndepth. The beneﬁts of depth are witnessed for all evaluation\nmetrics (Table 3 and 4).\nComparisons with State-of-the-art Methods. In Table 4\nwe compare with the previous best single-model results.\nOur baseline 34-layer ResNets have achieved very compet-\nitive accuracy. Our 152-layer ResNet has a single-model\ntop-5 validation error of 4.49%. This single-model result\noutperforms all previous ensemble results (Table 5). We\ncombine six models of different depth to form an ensemble\n(only with two 152-layer ones at the time of submitting).\nThis leads to 3.57% top-5 error on the test set (Table 5).\nThis entry won the 1st place in ILSVRC 2015.\n4.2. CIFAR-10 and Analysis\nWe conducted more studies on the CIFAR-10 dataset\n[20], which consists of 50k training images and 10k test-\ning images in 10 classes. We present experiments trained\non the training set and evaluated on the test set. Our focus\nis on the behaviors of extremely deep networks, but not on\npushing the state-of-the-art results, so we intentionally use\nsimple architectures as follows.\nThe plain/residual architectures follow the form in Fig. 3\n(middle/right). The network inputs are 32 \u000232 images, with\nthe per-pixel mean subtracted. The ﬁrst layer is 3 \u00023 convo-\nlutions. Then we use a stack of 6nlayers with 3\u00023 convo-\nlutions on the feature maps of sizes f32;16;8grespectively,\nwith 2nlayers for each feature map size. The numbers of\nﬁlters aref16;32;64grespectively. The subsampling is per-\nformed by convolutions with a stride of 2. The network ends\nwith a global average pooling, a 10-way fully-connected\nlayer, and softmax. There are totally 6 n+2 stacked weighted\nlayers. The following table summarizes the architecture:\noutput map size 32\u000232 16\u000216 8\u00028\n# layers 1+2n 2n 2n\n# ﬁlters 16 32 64\nWhen shortcut connections are used, they are connected\nto the pairs of 3\u00023 layers (totally 3nshortcuts). On this\ndataset we use identity shortcuts in all cases ( i.e., option A),method error (%)\nMaxout [10] 9.38\nNIN [25] 8.81\nDSN [24] 8.22\n# layers # params\nFitNet [35] 19 2.5M 8.39\nHighway [42, 43] 19 2.3M 7.54 (7.72\u00060.16)\nHighway [42, 43] 32 1.25M 8.80\nResNet 20 0.27M 8.75\nResNet 32 0.46M 7.51\nResNet 44 0.66M 7.17\nResNet 56 0.85M 6.97\nResNet 110 1.7M 6.43 (6.61\u00060.16)\nResNet 1202 19.4M 7.93\nTable 6. Classiﬁcation error on the CIFAR-10 test set. All meth-\nods are with data augmentation. For ResNet-110, we run it 5 times\nand show “best (mean \u0006std)” as in [43].\nso our residual models have exactly the same depth, width,\nand number of parameters as the plain counterparts.\nWe use a weight decay of 0.0001 and momentum of 0.9,\nand adopt the weight initialization in [13] and BN [16] but\nwith no dropout. These models are trained with a mini-\nbatch size of 128 on two GPUs. We start with a learning\nrate of 0.1, divide it by 10 at 32k and 48k iterations, and\nterminate training at 64k iterations, which is determined on\na 45k/5k train/val split. We follow the simple data augmen-\ntation in [24] for training: 4 pixels are padded on each side,\nand a 32\u000232 crop is randomly sampled from the padded\nimage or its horizontal ﬂip. For testing, we only evaluate\nthe single view of the original 32 \u000232 image.\nWe compare n=f3;5;7;9g, leading to 20, 32, 44, and\n56-layer networks. Fig. 6 (left) shows the behaviors of the\nplain nets. The deep plain nets suffer from increased depth,\nand exhibit higher training error when going deeper. This\nphenomenon is similar to that on ImageNet (Fig. 4, left) and\non MNIST (see [42]), suggesting that such an optimization\ndifﬁculty is a fundamental problem.\nFig. 6 (middle) shows the behaviors of ResNets. Also\nsimilar to the ImageNet cases (Fig. 4, right), our ResNets\nmanage to overcome the optimization difﬁculty and demon-\nstrate accuracy gains when the depth increases.\nWe further explore n= 18 that leads to a 110-layer\nResNet. In this case, we ﬁnd that the initial learning rate\nof 0.1 is slightly too large to start converging5. So we use\n0.01 to warm up the training until the training error is below\n80% (about 400 iterations), and then go back to 0.1 and con-\ntinue training. The rest of the learning schedule is as done\npreviously. This 110-layer network converges well (Fig. 6,\nmiddle). It has fewer parameters than other deep and thin\n5With an initial learning rate of 0.1, it starts converging ( <90% error)\nafter several epochs, but still reaches similar accuracy.\n70 1 2 3 4 5 6051020\niter. (1e4)error (%)\n  \nplain-20\nplain-32\nplain-44\nplain-56\n0 1 2 3 4 5 6051020\niter. (1e4)error (%)\n  \nResNet-20\nResNet-32\nResNet-44\nResNet-56\nResNet-110 56-layer\n20-layer\n110-layer20-layer\n4 5 60151020\niter. (1e4)error (%)\n  \nresidual-110\nresidual-1202Figure 6. Training on CIFAR-10 . Dashed lines denote training error, and bold lines denote testing error. Left: plain networks. The error\nof plain-110 is higher than 60% and not displayed. Middle : ResNets. Right : ResNets with 110 and 1202 layers.\n0 20 40 60 80 100123\nlayer index (sorted by magnitude)std\n  \nplain-20\nplain-56\nResNet-20\nResNet-56\nResNet-1100 20 40 60 80 100123\nlayer index (original)std\n  \nplain-20\nplain-56\nResNet-20\nResNet-56\nResNet-110\nFigure 7. Standard deviations (std) of layer responses on CIFAR-\n10. The responses are the outputs of each 3 \u00023 layer, after BN and\nbefore nonlinearity. Top: the layers are shown in their original\norder. Bottom : the responses are ranked in descending order.\nnetworks such as FitNet [35] and Highway [42] (Table 6),\nyet is among the state-of-the-art results (6.43%, Table 6).\nAnalysis of Layer Responses. Fig. 7 shows the standard\ndeviations (std) of the layer responses. The responses are\nthe outputs of each 3 \u00023 layer, after BN and before other\nnonlinearity (ReLU/addition). For ResNets, this analy-\nsis reveals the response strength of the residual functions.\nFig. 7 shows that ResNets have generally smaller responses\nthan their plain counterparts. These results support our ba-\nsic motivation (Sec.3.1) that the residual functions might\nbe generally closer to zero than the non-residual functions.\nWe also notice that the deeper ResNet has smaller magni-\ntudes of responses, as evidenced by the comparisons among\nResNet-20, 56, and 110 in Fig. 7. When there are more\nlayers, an individual layer of ResNets tends to modify the\nsignal less.\nExploring Over 1000 layers. We explore an aggressively\ndeep model of over 1000 layers. We set n= 200 that\nleads to a 1202-layer network, which is trained as described\nabove. Our method shows no optimization difﬁculty , and\nthis103-layer network is able to achieve training error\n<0.1% (Fig. 6, right). Its test error is still fairly good\n(7.93%, Table 6).\nBut there are still open problems on such aggressively\ndeep models. The testing result of this 1202-layer network\nis worse than that of our 110-layer network, although bothtraining data 07+12 07++12\ntest data VOC 07 test VOC 12 test\nVGG-16 73.2 70.4\nResNet-101 76.4 73.8\nTable 7. Object detection mAP (%) on the PASCAL VOC\n2007/2012 test sets using baseline Faster R-CNN. See also Ta-\nble 10 and 11 for better results.\nmetric mAP@.5 mAP@[.5, .95]\nVGG-16 41.5 21.2\nResNet-101 48.4 27.2\nTable 8. Object detection mAP (%) on the COCO validation set\nusing baseline Faster R-CNN. See also Table 9 for better results.\nhave similar training error. We argue that this is because of\noverﬁtting. The 1202-layer network may be unnecessarily\nlarge (19.4M) for this small dataset. Strong regularization\nsuch as maxout [10] or dropout [14] is applied to obtain the\nbest results ([10, 25, 24, 35]) on this dataset. In this paper,\nwe use no maxout/dropout and just simply impose regular-\nization via deep and thin architectures by design, without\ndistracting from the focus on the difﬁculties of optimiza-\ntion. But combining with stronger regularization may im-\nprove results, which we will study in the future.\n4.3. Object Detection on PASCAL and MS COCO\nOur method has good generalization performance on\nother recognition tasks. Table 7 and 8 show the object de-\ntection baseline results on PASCAL VOC 2007 and 2012\n[5] and COCO [26]. We adopt Faster R-CNN [32] as the de-\ntection method. Here we are interested in the improvements\nof replacing VGG-16 [41] with ResNet-101. The detection\nimplementation (see appendix) of using both models is the\nsame, so the gains can only be attributed to better networks.\nMost remarkably, on the challenging COCO dataset we ob-\ntain a 6.0% increase in COCO’s standard metric (mAP@[.5,\n.95]), which is a 28% relative improvement. This gain is\nsolely due to the learned representations.\nBased on deep residual nets, we won the 1st places in\nseveral tracks in ILSVRC & COCO 2015 competitions: Im-\nageNet detection, ImageNet localization, COCO detection,\nand COCO segmentation. The details are in the appendix.\n8References\n[1] Y . Bengio, P. Simard, and P. Frasconi. Learning long-term dependen-\ncies with gradient descent is difﬁcult. IEEE Transactions on Neural\nNetworks , 5(2):157–166, 1994.\n[2] C. M. Bishop. Neural networks for pattern recognition . Oxford\nuniversity press, 1995.\n[3] W. L. Briggs, S. F. McCormick, et al. A Multigrid Tutorial . Siam,\n2000.\n[4] K. Chatﬁeld, V . Lempitsky, A. Vedaldi, and A. Zisserman. The devil\nis in the details: an evaluation of recent feature encoding methods.\nInBMVC , 2011.\n[5] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\nserman. The Pascal Visual Object Classes (VOC) Challenge. IJCV ,\npages 303–338, 2010.\n[6] S. Gidaris and N. Komodakis. Object detection via a multi-region &\nsemantic segmentation-aware cnn model. In ICCV , 2015.\n[7] R. Girshick. Fast R-CNN. In ICCV , 2015.\n[8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier-\narchies for accurate object detection and semantic segmentation. In\nCVPR , 2014.\n[9] X. Glorot and Y . Bengio. Understanding the difﬁculty of training\ndeep feedforward neural networks. In AISTATS , 2010.\n[10] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and\nY . Bengio. Maxout networks. arXiv:1302.4389 , 2013.\n[11] K. He and J. Sun. Convolutional neural networks at constrained time\ncost. In CVPR , 2015.\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep\nconvolutional networks for visual recognition. In ECCV , 2014.\n[13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiﬁers:\nSurpassing human-level performance on imagenet classiﬁcation. In\nICCV , 2015.\n[14] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov. Improving neural networks by preventing co-\nadaptation of feature detectors. arXiv:1207.0580 , 2012.\n[15] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural\ncomputation , 9(8):1735–1780, 1997.\n[16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift. In ICML , 2015.\n[17] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest\nneighbor search. TPAMI , 33, 2011.\n[18] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\nC. Schmid. Aggregating local image descriptors into compact codes.\nTPAMI , 2012.\n[19] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for\nfast feature embedding. arXiv:1408.5093 , 2014.\n[20] A. Krizhevsky. Learning multiple layers of features from tiny im-\nages. Tech Report , 2009.\n[21] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation\nwith deep convolutional neural networks. In NIPS , 2012.\n[22] Y . LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,\nW. Hubbard, and L. D. Jackel. Backpropagation applied to hand-\nwritten zip code recognition. Neural computation , 1989.\n[23] Y . LeCun, L. Bottou, G. B. Orr, and K.-R. M ¨uller. Efﬁcient backprop.\nInNeural Networks: Tricks of the Trade , pages 9–50. Springer, 1998.\n[24] C.-Y . Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-\nsupervised nets. arXiv:1409.5185 , 2014.\n[25] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv:1312.4400 ,\n2013.\n[26] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll ´ar, and C. L. Zitnick. Microsoft COCO: Common objects in\ncontext. In ECCV . 2014.\n[27] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks\nfor semantic segmentation. In CVPR , 2015.[28] G. Mont ´ufar, R. Pascanu, K. Cho, and Y . Bengio. On the number of\nlinear regions of deep neural networks. In NIPS , 2014.\n[29] V . Nair and G. E. Hinton. Rectiﬁed linear units improve restricted\nboltzmann machines. In ICML , 2010.\n[30] F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for\nimage categorization. In CVPR , 2007.\n[31] T. Raiko, H. Valpola, and Y . LeCun. Deep learning made easier by\nlinear transformations in perceptrons. In AISTATS , 2012.\n[32] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards\nreal-time object detection with region proposal networks. In NIPS ,\n2015.\n[33] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Object detection\nnetworks on convolutional feature maps. arXiv:1504.06066 , 2015.\n[34] B. D. Ripley. Pattern recognition and neural networks . Cambridge\nuniversity press, 1996.\n[35] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\nY . Bengio. Fitnets: Hints for thin deep nets. In ICLR , 2015.\n[36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet\nlarge scale visual recognition challenge. arXiv:1409.0575 , 2014.\n[37] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to\nthe nonlinear dynamics of learning in deep linear neural networks.\narXiv:1312.6120 , 2013.\n[38] N. N. Schraudolph. Accelerated gradient descent by factor-centering\ndecomposition. Technical report, 1998.\n[39] N. N. Schraudolph. Centering neural network gradient factors. In\nNeural Networks: Tricks of the Trade , pages 207–226. Springer,\n1998.\n[40] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y . Le-\nCun. Overfeat: Integrated recognition, localization and detection\nusing convolutional networks. In ICLR , 2014.\n[41] K. Simonyan and A. Zisserman. Very deep convolutional networks\nfor large-scale image recognition. In ICLR , 2015.\n[42] R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks.\narXiv:1505.00387 , 2015.\n[43] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep\nnetworks. 1507.06228 , 2015.\n[44] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er-\nhan, V . Vanhoucke, and A. Rabinovich. Going deeper with convolu-\ntions. In CVPR , 2015.\n[45] R. Szeliski. Fast surface interpolation using hierarchical basis func-\ntions. TPAMI , 1990.\n[46] R. Szeliski. Locally adapted hierarchical basis preconditioning. In\nSIGGRAPH , 2006.\n[47] T. Vatanen, T. Raiko, H. Valpola, and Y . LeCun. Pushing stochas-\ntic gradient towards second-order methods–backpropagation learn-\ning with transformations in nonlinearities. In Neural Information\nProcessing , 2013.\n[48] A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library\nof computer vision algorithms, 2008.\n[49] W. Venables and B. Ripley. Modern applied statistics with s-plus.\n1999.\n[50] M. D. Zeiler and R. Fergus. Visualizing and understanding convolu-\ntional neural networks. In ECCV , 2014.\n9A. Object Detection Baselines\nIn this section we introduce our detection method based\non the baseline Faster R-CNN [32] system. The models are\ninitialized by the ImageNet classiﬁcation models, and then\nﬁne-tuned on the object detection data. We have experi-\nmented with ResNet-50/101 at the time of the ILSVRC &\nCOCO 2015 detection competitions.\nUnlike VGG-16 used in [32], our ResNet has no hidden\nfc layers. We adopt the idea of “Networks on Conv fea-\nture maps” (NoC) [33] to address this issue. We compute\nthe full-image shared conv feature maps using those lay-\ners whose strides on the image are no greater than 16 pixels\n(i.e., conv1, conv2 x, conv3 x, and conv4 x, totally 91 conv\nlayers in ResNet-101; Table 1). We consider these layers as\nanalogous to the 13 conv layers in VGG-16, and by doing\nso, both ResNet and VGG-16 have conv feature maps of the\nsame total stride (16 pixels). These layers are shared by a\nregion proposal network (RPN, generating 300 proposals)\n[32] and a Fast R-CNN detection network [7]. RoI pool-\ning [7] is performed before conv5 1. On this RoI-pooled\nfeature, all layers of conv5 x and up are adopted for each\nregion, playing the roles of VGG-16’s fc layers. The ﬁnal\nclassiﬁcation layer is replaced by two sibling layers (classi-\nﬁcation and box regression [7]).\nFor the usage of BN layers, after pre-training, we com-\npute the BN statistics (means and variances) for each layer\non the ImageNet training set. Then the BN layers are ﬁxed\nduring ﬁne-tuning for object detection. As such, the BN\nlayers become linear activations with constant offsets and\nscales, and BN statistics are not updated by ﬁne-tuning. We\nﬁx the BN layers mainly for reducing memory consumption\nin Faster R-CNN training.\nPASCAL VOC\nFollowing [7, 32], for the PASCAL VOC 2007 testset,\nwe use the 5k trainval images in VOC 2007 and 16k train-\nvalimages in VOC 2012 for training (“07+12”). For the\nPASCAL VOC 2012 testset, we use the 10k trainval +test\nimages in VOC 2007 and 16k trainval images in VOC 2012\nfor training (“07++12”). The hyper-parameters for train-\ning Faster R-CNN are the same as in [32]. Table 7 shows\nthe results. ResNet-101 improves the mAP by >3% over\nVGG-16. This gain is solely because of the improved fea-\ntures learned by ResNet.\nMS COCO\nThe MS COCO dataset [26] involves 80 object cate-\ngories. We evaluate the PASCAL VOC metric (mAP @\nIoU = 0.5) and the standard COCO metric (mAP @ IoU =\n.5:.05:.95). We use the 80k images on the train set for train-\ning and the 40k images on the val set for evaluation. Our\ndetection system for COCO is similar to that for PASCAL\nVOC. We train the COCO models with an 8-GPU imple-\nmentation, and thus the RPN step has a mini-batch size of8 images ( i.e., 1 per GPU) and the Fast R-CNN step has a\nmini-batch size of 16 images. The RPN step and Fast R-\nCNN step are both trained for 240k iterations with a learn-\ning rate of 0.001 and then for 80k iterations with 0.0001.\nTable 8 shows the results on the MS COCO validation\nset. ResNet-101 has a 6% increase of mAP@[.5, .95] over\nVGG-16, which is a 28% relative improvement, solely con-\ntributed by the features learned by the better network. Re-\nmarkably, the mAP@[.5, .95]’s absolute increase (6.0%) is\nnearly as big as mAP@.5’s (6.9%). This suggests that a\ndeeper network can improve both recognition and localiza-\ntion.\nB. Object Detection Improvements\nFor completeness, we report the improvements made for\nthe competitions. These improvements are based on deep\nfeatures and thus should beneﬁt from residual learning.\nMS COCO\nBox reﬁnement. Our box reﬁnement partially follows the it-\nerative localization in [6]. In Faster R-CNN, the ﬁnal output\nis a regressed box that is different from its proposal box. So\nfor inference, we pool a new feature from the regressed box\nand obtain a new classiﬁcation score and a new regressed\nbox. We combine these 300 new predictions with the orig-\ninal 300 predictions. Non-maximum suppression (NMS) is\napplied on the union set of predicted boxes using an IoU\nthreshold of 0.3 [8], followed by box voting [6]. Box re-\nﬁnement improves mAP by about 2 points (Table 9).\nGlobal context. We combine global context in the Fast\nR-CNN step. Given the full-image conv feature map, we\npool a feature by global Spatial Pyramid Pooling [12] (with\na “single-level” pyramid) which can be implemented as\n“RoI” pooling using the entire image’s bounding box as the\nRoI. This pooled feature is fed into the post-RoI layers to\nobtain a global context feature. This global feature is con-\ncatenated with the original per-region feature, followed by\nthe sibling classiﬁcation and box regression layers. This\nnew structure is trained end-to-end. Global context im-\nproves mAP@.5 by about 1 point (Table 9).\nMulti-scale testing. In the above, all results are obtained by\nsingle-scale training/testing as in [32], where the image’s\nshorter side is s= 600 pixels. Multi-scale training/testing\nhas been developed in [12, 7] by selecting a scale from a\nfeature pyramid, and in [33] by using maxout layers. In\nour current implementation, we have performed multi-scale\ntesting following [33]; we have not performed multi-scale\ntraining because of limited time. In addition, we have per-\nformed multi-scale testing only for the Fast R-CNN step\n(but not yet for the RPN step). With a trained model, we\ncompute conv feature maps on an image pyramid, where the\nimage’s shorter sides are s2f200;400;600;800;1000g.\n10training data COCO train COCO trainval\ntest data COCO val COCO test-dev\nmAP @.5 @[.5, .95] @.5 @[.5, .95]\nbaseline Faster R-CNN (VGG-16) 41.5 21.2\nbaseline Faster R-CNN (ResNet-101) 48.4 27.2\n+box reﬁnement 49.9 29.9\n+context 51.1 30.0 53.3 32.2\n+multi-scale testing 53.8 32.5 55.7 34.9\nensemble 59.0 37.4\nTable 9. Object detection improvements on MS COCO using Faster R-CNN and ResNet-101.\nsystem net data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv\nbaseline VGG-16 07+12 73.2 76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6\nbaseline ResNet-101 07+12 76.4 79.8 80.7 76.2 68.3 55.9 85.1 85.3 89.8 56.7 87.8 69.4 88.3 88.9 80.9 78.4 41.7 78.6 79.8 85.3 72.0\nbaseline+++ ResNet-101 COCO+07+12 85.6 90.0 89.6 87.8 80.8 76.1 89.9 89.9 89.6 75.5 90.0 80.7 89.6 90.3 89.1 88.7 65.4 88.1 85.6 89.0 86.8\nTable 10. Detection results on the PASCAL VOC 2007 test set. The baseline is the Faster R-CNN system. The system “baseline+++”\ninclude box reﬁnement, context, and multi-scale testing in Table 9.\nsystem net data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv\nbaseline VGG-16 07++12 70.4 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5\nbaseline ResNet-101 07++12 73.8 86.5 81.6 77.2 58.0 51.0 78.6 76.6 93.2 48.6 80.4 59.0 92.1 85.3 84.8 80.7 48.1 77.3 66.5 84.7 65.6\nbaseline+++ ResNet-101 COCO+07++12 83.8 92.1 88.4 84.8 75.9 71.4 86.3 87.8 94.2 66.8 89.4 69.2 93.9 91.9 90.9 89.6 67.9 88.2 76.8 90.3 80.0\nTable 11. Detection results on the PASCAL VOC 2012 test set ( http://host.robots.ox.ac.uk:8080/leaderboard/\ndisplaylb.php?challengeid=11&compid=4 ). The baseline is the Faster R-CNN system. The system “baseline+++” include\nbox reﬁnement, context, and multi-scale testing in Table 9.\nWe select two adjacent scales from the pyramid following\n[33]. RoI pooling and subsequent layers are performed on\nthe feature maps of these two scales [33], which are merged\nby maxout as in [33]. Multi-scale testing improves the mAP\nby over 2 points (Table 9).\nUsing validation data. Next we use the 80k+40k trainval set\nfor training and the 20k test-dev set for evaluation. The test-\ndev set has no publicly available ground truth and the result\nis reported by the evaluation server. Under this setting, the\nresults are an mAP@.5 of 55.7% and an mAP@[.5, .95] of\n34.9% (Table 9). This is our single-model result.\nEnsemble. In Faster R-CNN, the system is designed to learn\nregion proposals and also object classiﬁers, so an ensemble\ncan be used to boost both tasks. We use an ensemble for\nproposing regions, and the union set of proposals are pro-\ncessed by an ensemble of per-region classiﬁers. Table 9\nshows our result based on an ensemble of 3 networks. The\nmAP is 59.0% and 37.4% on the test-dev set. This result\nwon the 1st place in the detection task in COCO 2015.\nPASCAL VOC\nWe revisit the PASCAL VOC dataset based on the above\nmodel. With the single model on the COCO dataset (55.7%\nmAP@.5 in Table 9), we ﬁne-tune this model on the PAS-\nCAL VOC sets. The improvements of box reﬁnement, con-\ntext, and multi-scale testing are also adopted. By doing soval2 test\nGoogLeNet [44] (ILSVRC’14) - 43.9\nour single model (ILSVRC’15) 60.5 58.8\nour ensemble (ILSVRC’15) 63.6 62.1\nTable 12. Our results (mAP, %) on the ImageNet detection dataset.\nOur detection system is Faster R-CNN [32] with the improvements\nin Table 9, using ResNet-101.\nwe achieve 85.6% mAP on PASCAL VOC 2007 (Table 10)\nand 83.8% on PASCAL VOC 2012 (Table 11)6. The result\non PASCAL VOC 2012 is 10 points higher than the previ-\nous state-of-the-art result [6].\nImageNet Detection\nThe ImageNet Detection (DET) task involves 200 object\ncategories. The accuracy is evaluated by mAP@.5. Our\nobject detection algorithm for ImageNet DET is the same\nas that for MS COCO in Table 9. The networks are pre-\ntrained on the 1000-class ImageNet classiﬁcation set, and\nare ﬁne-tuned on the DET data. We split the validation set\ninto two parts (val1/val2) following [8]. We ﬁne-tune the\ndetection models using the DET training set and the val1\nset. The val2 set is used for validation. We do not use other\nILSVRC 2015 data. Our single model with ResNet-101 has\n6http://host.robots.ox.ac.uk:8080/anonymous/3OJ4OJ.html ,\nsubmitted on 2015-11-26.\n11LOC\nmethodLOC\nnetworktestingLOC error\non GT CLSclassiﬁcation\nnetworktop-5 LOC error\non predicted CLS\nVGG’s [41] VGG-16 1-crop 33.1 [41]\nRPN ResNet-101 1-crop 13.3\nRPN ResNet-101 dense 11.7\nRPN ResNet-101 dense ResNet-101 14.4\nRPN+RCNN ResNet-101 dense ResNet-101 10.6\nRPN+RCNN ensemble dense ensemble 8.9\nTable 13. Localization error (%) on the ImageNet validation. In\nthe column of “LOC error on GT class” ([41]), the ground truth\nclass is used. In the “testing” column, “1-crop” denotes testing\non a center crop of 224 \u0002224 pixels, “dense” denotes dense (fully\nconvolutional) and multi-scale testing.\n58.8% mAP and our ensemble of 3 models has 62.1% mAP\non the DET test set (Table 12). This result won the 1st place\nin the ImageNet detection task in ILSVRC 2015 , surpassing\nthe second place by 8.5 points (absolute).\nC. ImageNet Localization\nThe ImageNet Localization (LOC) task [36] requires to\nclassify and localize the objects. Following [40, 41], we\nassume that the image-level classiﬁers are ﬁrst adopted for\npredicting the class labels of an image, and the localiza-\ntion algorithm only accounts for predicting bounding boxes\nbased on the predicted classes. We adopt the “per-class re-\ngression” (PCR) strategy [40, 41], learning a bounding box\nregressor for each class. We pre-train the networks for Im-\nageNet classiﬁcation and then ﬁne-tune them for localiza-\ntion. We train networks on the provided 1000-class Ima-\ngeNet training set.\nOur localization algorithm is based on the RPN frame-\nwork of [32] with a few modiﬁcations. Unlike the way in\n[32] that is category-agnostic, our RPN for localization is\ndesigned in a per-class form. This RPN ends with two sib-\nling 1\u00021 convolutional layers for binary classiﬁcation ( cls)\nand box regression ( reg), as in [32]. The clsandreglayers\nare both in a per-class from, in contrast to [32]. Speciﬁ-\ncally, the clslayer has a 1000-d output, and each dimension\nisbinary logistic regression for predicting being or not be-\ning an object class; the reglayer has a 1000\u00024-d output\nconsisting of box regressors for 1000 classes. As in [32],\nour bounding box regression is with reference to multiple\ntranslation-invariant “anchor” boxes at each position.\nAs in our ImageNet classiﬁcation training (Sec. 3.4), we\nrandomly sample 224 \u0002224 crops for data augmentation.\nWe use a mini-batch size of 256 images for ﬁne-tuning. To\navoid negative samples being dominate, 8 anchors are ran-\ndomly sampled for each image, where the sampled positive\nand negative anchors have a ratio of 1:1 [32]. For testing,\nthe network is applied on the image fully-convolutionally.\nTable 13 compares the localization results. Following\n[41], we ﬁrst perform “oracle” testing using the ground truth\nclass as the classiﬁcation prediction. VGG’s paper [41] re-methodtop-5 localization err\nval test\nOverFeat [40] (ILSVRC’13) 30.0 29.9\nGoogLeNet [44] (ILSVRC’14) - 26.7\nVGG [41] (ILSVRC’14) 26.9 25.3\nours (ILSVRC’15) 8.9 9.0\nTable 14. Comparisons of localization error (%) on the ImageNet\ndataset with state-of-the-art methods.\nports a center-crop error of 33.1% (Table 13) using ground\ntruth classes. Under the same setting, our RPN method us-\ning ResNet-101 net signiﬁcantly reduces the center-crop er-\nror to 13.3%. This comparison demonstrates the excellent\nperformance of our framework. With dense (fully convolu-\ntional) and multi-scale testing, our ResNet-101 has an error\nof 11.7% using ground truth classes. Using ResNet-101 for\npredicting classes (4.6% top-5 classiﬁcation error, Table 4),\nthe top-5 localization error is 14.4%.\nThe above results are only based on the proposal network\n(RPN) in Faster R-CNN [32]. One may use the detection\nnetwork (Fast R-CNN [7]) in Faster R-CNN to improve the\nresults. But we notice that on this dataset, one image usually\ncontains a single dominate object, and the proposal regions\nhighly overlap with each other and thus have very similar\nRoI-pooled features. As a result, the image-centric training\nof Fast R-CNN [7] generates samples of small variations,\nwhich may not be desired for stochastic training. Motivated\nby this, in our current experiment we use the original R-\nCNN [8] that is RoI-centric, in place of Fast R-CNN.\nOur R-CNN implementation is as follows. We apply the\nper-class RPN trained as above on the training images to\npredict bounding boxes for the ground truth class. These\npredicted boxes play a role of class-dependent proposals.\nFor each training image, the highest scored 200 proposals\nare extracted as training samples to train an R-CNN classi-\nﬁer. The image region is cropped from a proposal, warped\nto 224\u0002224 pixels, and fed into the classiﬁcation network\nas in R-CNN [8]. The outputs of this network consist of two\nsibling fc layers for clsandreg, also in a per-class form.\nThis R-CNN network is ﬁne-tuned on the training set us-\ning a mini-batch size of 256 in the RoI-centric fashion. For\ntesting, the RPN generates the highest scored 200 proposals\nfor each predicted class, and the R-CNN network is used to\nupdate these proposals’ scores and box positions.\nThis method reduces the top-5 localization error to\n10.6% (Table 13). This is our single-model result on the\nvalidation set. Using an ensemble of networks for both clas-\nsiﬁcation and localization, we achieve a top-5 localization\nerror of 9.0% on the test set. This number signiﬁcantly out-\nperforms the ILSVRC 14 results (Table 14), showing a 64%\nrelative reduction of error. This result won the 1st place in\nthe ImageNet localization task in ILSVRC 2015.\n12\n\n\n\nPublished as a conference paper at ICLR 2016\nMULTI -SCALE CONTEXT AGGREGATION BY\nDILATED CONVOLUTIONS\nFisher Yu\nPrinceton University\nVladlen Koltun\nIntel Labs\nABSTRACT\nState-of-the-art models for semantic segmentation are based on adaptations of\nconvolutional networks that had originally been designed for image classiﬁca-\ntion. However, dense prediction problems such as semantic segmentation are\nstructurally different from image classiﬁcation. In this work, we develop a new\nconvolutional network module that is speciﬁcally designed for dense prediction.\nThe presented module uses dilated convolutions to systematically aggregate multi-\nscale contextual information without losing resolution. The architecture is based\non the fact that dilated convolutions support exponential expansion of the receptive\nﬁeld without loss of resolution or coverage. We show that the presented context\nmodule increases the accuracy of state-of-the-art semantic segmentation systems.\nIn addition, we examine the adaptation of image classiﬁcation networks to dense\nprediction and show that simplifying the adapted network can increase accuracy.\n1 I NTRODUCTION\nMany natural problems in computer vision are instances of dense prediction. The goal is to com-\npute a discrete or continuous label for each pixel in the image. A prominent example is semantic\nsegmentation, which calls for classifying each pixel into one of a given set of categories (He et al.,\n2004; Shotton et al., 2009; Kohli et al., 2009; Kr ¨ahenb ¨uhl & Koltun, 2011). Semantic segmenta-\ntion is challenging because it requires combining pixel-level accuracy with multi-scale contextual\nreasoning (He et al., 2004; Galleguillos & Belongie, 2010).\nSigniﬁcant accuracy gains in semantic segmentation have recently been obtained through the use of\nconvolutional networks (LeCun et al., 1989) trained by backpropagation (Rumelhart et al., 1986).\nSpeciﬁcally, Long et al. (2015) showed that convolutional network architectures that had originally\nbeen developed for image classiﬁcation can be successfully repurposed for dense prediction. These\nreporposed networks substantially outperform the prior state of the art on challenging semantic seg-\nmentation benchmarks. This prompts new questions motivated by the structural differences between\nimage classiﬁcation and dense prediction. Which aspects of the repurposed networks are truly nec-\nessary and which reduce accuracy when operated densely? Can dedicated modules designed specif-\nically for dense prediction improve accuracy further?\nModern image classiﬁcation networks integrate multi-scale contextual information via succes-\nsive pooling and subsampling layers that reduce resolution until a global prediction is obtained\n(Krizhevsky et al., 2012; Simonyan & Zisserman, 2015). In contrast, dense prediction calls for multi-\nscale contextual reasoning in combination with full-resolution output. Recent work has studied two\napproaches to dealing with the conﬂicting demands of multi-scale reasoning and full-resolution\ndense prediction. One approach involves repeated up-convolutions that aim to recover lost resolu-\ntion while carrying over the global perspective from downsampled layers (Noh et al., 2015; Fischer\net al., 2015). This leaves open the question of whether severe intermediate downsampling was truly\nnecessary. Another approach involves providing multiple rescaled versions of the image as input to\nthe network and combining the predictions obtained for these multiple inputs (Farabet et al., 2013;\nLin et al., 2015; Chen et al., 2015b). Again, it is not clear whether separate analysis of rescaled input\nimages is truly necessary.\n1arXiv:1511.07122v3  [cs.CV]  30 Apr 2016Published as a conference paper at ICLR 2016\nIn this work, we develop a convolutional network module that aggregates multi-scale contextual\ninformation without losing resolution or analyzing rescaled images. The module can be plugged\ninto existing architectures at any resolution. Unlike pyramid-shaped architectures carried over from\nimage classiﬁcation, the presented context module is designed speciﬁcally for dense prediction. It is\na rectangular prism of convolutional layers, with no pooling or subsampling. The module is based\non dilated convolutions, which support exponential expansion of the receptive ﬁeld without loss of\nresolution or coverage.\nAs part of this work, we also re-examine the performance of repurposed image classiﬁcation net-\nworks on semantic segmentation. The performance of the core prediction modules can be uninten-\ntionally obscured by increasingly elaborate systems that involve structured prediction, multi-column\narchitectures, multiple training datasets, and other augmentations. We therefore examine the leading\nadaptations of deep image classiﬁcation networks in a controlled setting and remove vestigial com-\nponents that hinder dense prediction performance. The result is an initial prediction module that is\nboth simpler and more accurate than prior adaptations.\nUsing the simpliﬁed prediction module, we evaluate the presented context network through con-\ntrolled experiments on the Pascal VOC 2012 dataset (Everingham et al., 2010). The experiments\ndemonstrate that plugging the context module into existing semantic segmentation architectures re-\nliably increases their accuracy.\n2 D ILATED CONVOLUTIONS\nLetF:Z2!Rbe a discrete function. Let \nr= [\u0000r;r]2\\Z2and letk: \nr!Rbe a discrete\nﬁlter of size (2r+ 1)2. The discrete convolution operator \u0003can be deﬁned as\n(F\u0003k)(p) =X\ns+t=pF(s)k(t): (1)\nWe now generalize this operator. Let lbe a dilation factor and let \u0003lbe deﬁned as\n(F\u0003lk)(p) =X\ns+lt=pF(s)k(t): (2)\nWe will refer to\u0003las a dilated convolution or an l-dilated convolution. The familiar discrete convo-\nlution\u0003is simply the 1-dilated convolution.\nThe dilated convolution operator has been referred to in the past as “convolution with a dilated ﬁlter”.\nIt plays a key role in the algorithme `a trous , an algorithm for wavelet decomposition (Holschneider\net al., 1987; Shensa, 1992).1We use the term “dilated convolution” instead of “convolution with a\ndilated ﬁlter” to clarify that no “dilated ﬁlter” is constructed or represented. The convolution opera-\ntor itself is modiﬁed to use the ﬁlter parameters in a different way. The dilated convolution operator\ncan apply the same ﬁlter at different ranges using different dilation factors. Our deﬁnition reﬂects\nthe proper implementation of the dilated convolution operator, which does not involve construction\nof dilated ﬁlters.\nIn recent work on convolutional networks for semantic segmentation, Long et al. (2015) analyzed\nﬁlter dilation but chose not to use it. Chen et al. (2015a) used dilation to simplify the architec-\nture of Long et al. (2015). In contrast, we develop a new convolutional network architecture that\nsystematically uses dilated convolutions for multi-scale context aggregation.\nOur architecture is motivated by the fact that dilated convolutions support exponentially expanding\nreceptive ﬁelds without losing resolution or coverage. Let F0;F1;:::;F n\u00001:Z2!Rbe discrete\nfunctions and let k0;k1;:::;k n\u00002: \n1!Rbe discrete 3\u00023ﬁlters. Consider applying the ﬁlters\nwith exponentially increasing dilation:\nFi+1=Fi\u00032ikifori= 0;1;:::;n\u00002: (3)\nDeﬁne the receptive ﬁeld of an element pinFi+1as the set of elements in F0that modify the value\nofFi+1(p). Let the size of the receptive ﬁeld of pinFi+1be the number of these elements. It is\n1Some recent work mistakenly referred to the dilated convolution operator itself as the algorithme `a trous .\nThis is incorrect. The algorithme `a trous applies a ﬁlter at multiple scales to produce a signal decomposition.\nThe algorithm uses dilated convolutions, but is not equivalent to the dilated convolution operator itself.\n2Published as a conference paper at ICLR 2016\n(a)\n (b)\n (c)\nFigure 1: Systematic dilation supports exponential expansion of the receptive ﬁeld without loss of\nresolution or coverage. (a) F1is produced from F0by a 1-dilated convolution; each element in F1\nhas a receptive ﬁeld of 3\u00023. (b)F2is produced from F1by a 2-dilated convolution; each element\ninF2has a receptive ﬁeld of 7\u00027. (c)F3is produced from F2by a 4-dilated convolution; each\nelement inF3has a receptive ﬁeld of 15\u000215. The number of parameters associated with each layer\nis identical. The receptive ﬁeld grows exponentially while the number of parameters grows linearly.\neasy to see that the size of the receptive ﬁeld of each element in Fi+1is(2i+2\u00001)\u0002(2i+2\u00001).\nThe receptive ﬁeld is a square of exponentially increasing size. This is illustrated in Figure 1.\n3 M ULTI -SCALE CONTEXT AGGREGATION\nThe context module is designed to increase the performance of dense prediction architectures by\naggregating multi-scale contextual information. The module takes Cfeature maps as input and\nproducesCfeature maps as output. The input and output have the same form, thus the module can\nbe plugged into existing dense prediction architectures.\nWe begin by describing a basic form of the context module. In this basic form, each layer has C\nchannels. The representation in each layer is the same and could be used to directly obtain a dense\nper-class prediction, although the feature maps are not normalized and no loss is deﬁned inside the\nmodule. Intuitively, the module can increase the accuracy of the feature maps by passing them\nthrough multiple layers that expose contextual information.\nThe basic context module has 7 layers that apply 3\u00023convolutions with different dilation factors.\nThe dilations are 1, 1, 2, 4, 8, 16, and 1. Each convolution operates on all layers: strictly speaking,\nthese are 3\u00023\u0002Cconvolutions with dilation in the ﬁrst two dimensions. Each of these convolutions\nis followed by a pointwise truncation max(\u0001;0). A ﬁnal layer performs 1\u00021\u0002Cconvolutions and\nproduces the output of the module. The architecture is summarized in Table 1. Note that the front-\nend module that provides the input to the context network in our experiments produces feature maps\nat64\u000264resolution. We therefore stop the exponential expansion of the receptive ﬁeld after layer 6.\nOur initial attempts to train the context module failed to yield an improvement in prediction accuracy.\nExperiments revealed that standard initialization procedures do not readily support the training of the\nmodule. Convolutional networks are commonly initialized using samples from random distributions\n(Glorot & Bengio, 2010; Krizhevsky et al., 2012; Simonyan & Zisserman, 2015). However, we\nfound that random initialization schemes were not effective for the context module. We found an\nalternative initialization with clear semantics to be much more effective:\nkb(t;a) = 1 [t=0]1[a=b]; (4)\nwhereais the index of the input feature map and bis the index of the output map. This is a form\nof identity initialization, which has recently been advocated for recurrent networks (Le et al., 2015).\nThis initialization sets all ﬁlters such that each layer simply passes the input directly to the next. A\nnatural concern is that this initialization could put the network in a mode where backpropagation\ncannot signiﬁcantly improve the default behavior of simply passing information through. However,\nexperiments indicate that this is not the case. Backpropagation reliably harvests the contextual\ninformation provided by the network to increase the accuracy of the processed maps.\n3Published as a conference paper at ICLR 2016\nLayer 1 2 3 4 5 6 7 8\nConvolution 3\u000233\u000233\u00023 3\u00023 3\u00023 3\u00023 3\u00023 1\u00021\nDilation 1 1 2 4 8 16 1 1\nTruncation Yes Yes Yes Yes Yes Yes Yes No\nReceptive ﬁeld 3\u000235\u000259\u0002917\u000217 33\u000233 65\u000265 67\u000267 67\u000267\nOutput channels\nBasic C C C C C C C C\nLarge 2C 2C 4C 8C 16C 32C 32C C\nTable 1: Context network architecture. The network processes Cfeature maps by aggregating\ncontextual information at progressively increasing scales without losing resolution.\nThis completes the presentation of the basic context network. Our experiments show that even this\nbasic module can increase dense prediction accuracy both quantitatively and qualitatively. This is\nparticularly notable given the small number of parameters in the network: \u001964C2parameters in\ntotal.\nWe have also trained a larger context network that uses a larger number of feature maps in the\ndeeper layers. The number of maps in the large network is summarized in Table 1. We generalize\nthe initialization scheme to account for the difference in the number of feature maps in different\nlayers. Letciandci+1be the number of feature maps in two consecutive layers. Assume that C\ndivides both ciandci+1. The initialization is\nkb(t;a) =8\n><\n>:C\nci+1t= 0 and\u0016aC\nci\u0017\n=\u0016bC\nci+1\u0017\n\" otherwise(5)\nHere\"\u0018N (0;\u001b2)and\u001b\u001cC=c i+1. The use of random noise breaks ties among feature maps\nwith a common predecessor.\n4 F RONT END\nWe implemented and trained a front-end prediction module that takes a color image as input and\nproducesC= 21 feature maps as output. The front-end module follows the work of Long et al.\n(2015) and Chen et al. (2015a), but was implemented separately. We adapted the VGG-16 network\n(Simonyan & Zisserman, 2015) for dense prediction and removed the last two pooling and striding\nlayers. Speciﬁcally, each of these pooling and striding layers was removed and convolutions in\nall subsequent layers were dilated by a factor of 2 for each pooling layer that was ablated. Thus\nconvolutions in the ﬁnal layers, which follow both ablated pooling layers, are dilated by a factor of\n4. This enables initialization with the parameters of the original classiﬁcation network, but produces\nhigher-resolution output. The front-end module takes padded images as input and produces feature\nmaps at resolution 64\u000264. We use reﬂection padding: the buffer zone is ﬁlled by reﬂecting the\nimage about each edge.\nOur front-end module is obtained by removing vestiges of the classiﬁcation network that are counter-\nproductive for dense prediction. Most signiﬁcantly, we remove the last two pooling and striding\nlayers entirely, whereas Long et al. kept them and Chen et al. replaced striding by dilation but\nkept the pooling layers. We found that simplifying the network by removing the pooling layers\nmade it more accurate. We also remove the padding of the intermediate feature maps. Intermediate\npadding was used in the original classiﬁcation network, but is neither necessary nor justiﬁed in dense\nprediction.\nThis simpliﬁed prediction module was trained on the Pascal VOC 2012 training set, augmented by\nthe annotations created by Hariharan et al. (2011). We did not use images from the VOC-2012\nvalidation set for training and therefore only used a subset of the annotations of Hariharan et al.\n(2011). Training was performed by stochastic gradient descent (SGD) with mini-batch size 14,\nlearning rate 10\u00003, and momentum 0:9. The network was trained for 60K iterations.\nWe now compare the accuracy of our front-end module to the FCN-8s design of Long et al. (2015)\nand the DeepLab network of Chen et al. (2015a). For FCN-8s and DeepLab, we evaluate the public\n4Published as a conference paper at ICLR 2016\n(a) Image\n (b) FCN-8s\n (c) DeepLab\n (d) Our front end\n (e) Ground truth\nFigure 2: Semantic segmentations produced by different adaptations of the VGG-16 classiﬁcation\nnetwork. From left to right: (a) input image, (b) prediction by FCN-8s (Long et al., 2015), (c)\nprediction by DeepLab (Chen et al., 2015a), (d) prediction by our simpliﬁed front-end module, (e)\nground truth.\naero\nbike\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse\nmbike\nperson\nplant\nsheep\nsofa\ntrain\ntv\nmean IoU\nFCN-8s 76.8 34.2 68.9 49.4 60.3 75.3 74.7 77.6 21.4 62.5 46.8 71.8 63.9 76.5 73.9 45.2 72.4 37.4 70.9 55.1 62.2\nDeepLab 72 3171.2 53.7 60.5 7771.9 73.1 25.2 62.6 49.1 68.7 63.3 73.9 73.6 50.8 72.3 42.1 67.9 52.6 62.1\nDeepLab-Msc 74.9 34.1 72.6 52.9 61.0 77.9 73.0 73.7 26.4 62.2 49.3 68.4 64.1 74.0 75.0 51.7 72.7 42.5 67.2 55.7 62.9\nOur front end 82.2 37.4 72.7 57.1 62.7 82.8 77.8 78.9 28 7051.6 73.1 72.8 81.5 79.1 56.6 77.1 49.9 75.3 60.9 67.6\nTable 2: Our front-end prediction module is simpler and more accurate than prior models. This table\nreports accuracy on the VOC-2012 test set.\nmodels trained by the original authors on VOC-2012. Segmentations produced by the different\nmodels on images from the VOC-2012 dataset are shown in Figure 2. The accuracy of the models\non the VOC-2012 test set is reported in Table 2.\nOur front-end prediction module is both simpler and more accurate than the prior models. Specif-\nically, our simpliﬁed model outperforms both FCN-8s and the DeepLab network by more than 5\npercentage points on the test set. Interestingly, our simpliﬁed front-end module outperforms the\nleaderboard accuracy of DeepLab+CRF on the test set by more than a percentage point ( 67:6%\nvs.66:4%) without using a CRF.\n5Published as a conference paper at ICLR 2016\n5 E XPERIMENTS\nOur implementation is based on the Caffe library (Jia et al., 2014). Our implementation of dilated\nconvolutions is now part of the stanfard Caffe distribution.\nFor fair comparison with recent high-performing systems, we trained a front-end module that has\nthe same structure as described in Section 4, but is trained on additional images from the Microsoft\nCOCO dataset (Lin et al., 2014). We used all images in Microsoft COCO with at least one object\nfrom the VOC-2012 categories. Annotated objects from other categories were treated as background.\nTraining was performed in two stages. In the ﬁrst stage, we trained on VOC-2012 images and\nMicrosoft COCO images together. Training was performed by SGD with mini-batch size 14 and\nmomentum 0.9. 100K iterations were performed with a learning rate of 10\u00003and 40K subsequent\niterations were performed with a learning rate of 10\u00004. In the second stage, we ﬁne-tuned the\nnetwork on VOC-2012 images only. Fine-tuning was performed for 50K iterations with a learning\nrate of 10\u00005. Images from the VOC-2012 validation set were not used for training.\nThe front-end module trained by this procedure achieves 69:8%mean IoU on the VOC-2012 vali-\ndation set and 71:3%mean IoU on the test set. Note that this level of accuracy is achieved by the\nfront-end alone, without the context module or structured prediction. We again attribute this high\naccuracy in part to the removal of vestigial components originally developed for image classiﬁcation\nrather than dense prediction.\nControlled evaluation of context aggregation. We now perform controlled experiments to eval-\nuate the utility of the context network presented in Section 3. We begin by plugging each of the two\ncontext modules (Basic and Large) into the front end. Since the receptive ﬁeld of the context net-\nwork is 67\u000267, we pad the input feature maps by a buffer of width 33. Zero padding and reﬂection\npadding yielded similar results in our experiments. The context module accepts feature maps from\nthe front end as input and is given this input during training. Joint training of the context module\nand the front-end module did not yield a signiﬁcant improvement in our experiments. The learning\nrate was set to 10\u00003. Training was initialized as described in Section 3.\nTable 3 shows the effect of adding the context module to three different architectures for semantic\nsegmentation. The ﬁrst architecture (top) is the front end described in Section 4. It performs seman-\ntic segmentation without structured prediction, akin to the original work of Long et al. (2015). The\nsecond architecture (Table 3, middle) uses the dense CRF to perform structured prediction, akin to\nthe system of Chen et al. (2015a). We use the implementation of Kr ¨ahenb ¨uhl & Koltun (2011) and\ntrain the CRF parameters by grid search on the validation set. The third architecture (Table 3, bot-\ntom) uses the CRF-RNN for structured prediction (Zheng et al., 2015). We use the implementation\nof Zheng et al. (2015) and train the CRF-RNN in each condition.\nThe experimental results demonstrate that the context module improves accuracy in each of the\nthree conﬁgurations. The basic context module increases accuracy in each conﬁguration. The large\ncontext module increases accuracy by a larger margin. The experiments indicate that the context\nmodule and structured prediction are synergisic: the context module increases accuracy with or\nwithout subsequent structured prediction. Qualitative results are shown in Figure 3.\nEvaluation on the test set. We now perform an evaluation on the test set by submitting our re-\nsults to the Pascal VOC 2012 evaluation server. The results are reported in Table 4. We use the\nlarge context module for these experiments. As the results demonstrate, the context module yields\na signiﬁcant boost in accuracy over the front end. The context module alone, without subsequent\nstructured prediction, outperforms DeepLab-CRF-COCO-LargeFOV (Chen et al., 2015a). The con-\ntext module with the dense CRF, using the original implementation of Kr ¨ahenb ¨uhl & Koltun (2011),\nperforms on par with the very recent CRF-RNN (Zheng et al., 2015). The context module in com-\nbination with the CRF-RNN further increases accuracy over the performance of the CRF-RNN.\n6 C ONCLUSION\nWe have examined convolutional network architectures for dense prediction. Since the model must\nproduce high-resolution output, we believe that high-resolution operation throughout the network\n6Published as a conference paper at ICLR 2016\n(a) Image\n (b) Front end\n (c) + Context\n (d) + CRF-RNN\n (e) Ground truth\nFigure 3: Semantic segmentations produced by different models. From left to right: (a) input image,\n(b) prediction by the front-end module, (c) prediction by the large context network plugged into the\nfront end, (d) prediction by the front end + context module + CRF-RNN, (e) ground truth.\naero\nbike\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse\nmbike\nperson\nplant\nsheep\nsofa\ntrain\ntv\nmean IoU\nFront end 86.3 38.2 76.8 66.8 63.2 87.3 78.7 8233.7 76.7 53.5 73.7 7676.6 8351.9 77.8 4479.9 66.3 69.8\nFront + Basic 86.4 37.6 78.5 66.3 64.1 89.9 79.9 84.9 36.1 79.4 55.8 77.6 81.6 7983.1 51.2 81.3 43.7 82.3 65.7 71.3\nFront + Large 87.3 39.2 80.3 65.6 66.4 90.2 82.6 85.8 34.8 81.9 51.7 7984.1 80.9 83.2 51.2 83.2 44.7 83.4 65.6 72.1\nFront end + CRF 89.2 38.8 8069.8 63.2 88.8 8085.2 33.8 80.6 55.5 77.1 80.8 77.3 84.3 53.1 80.4 4580.7 67.9 71.6\nFront + Basic + CRF 89.1 38.7 81.4 67.4 65 91 8186.7 37.5 81 5779.6 83.6 79.9 84.6 52.7 83.3 44.3 82.6 67.2 72.7\nFront + Large + CRF 89.6 39.9 82.7 66.7 67.5 91.1 83.3 87.4 3683.3 52.5 80.7 85.7 81.8 84.4 52.6 84.4 45.3 83.7 66.7 73.3\nFront end + RNN 88.8 38.1 80.8 69.1 65.6 89.9 79.6 85.7 36.3 83.6 57.3 77.9 83.2 7784.6 54.7 82.1 46.9 80.9 66.7 72.5\nFront + Basic + RNN 8938.4 82.3 67.9 65.2 91.5 80.4 87.2 38.4 82.1 57.7 79.9 8579.6 84.5 53.5 84 4582.8 66.2 73.1\nFront + Large + RNN 89.3 39.2 83.6 67.2 6992.1 83.1 8838.4 84.8 55.3 81.2 86.7 81.3 84.3 53.6 84.4 45.8 83.8 67 73.9\nTable 3: Controlled evaluation of the effect of the context module on the accuracy of three different\narchitectures for semantic segmentation. Experiments performed on the VOC-2012 validation set.\nValidation images were not used for training. Top: adding the context module to a semantic segmen-\ntation front end with no structured prediction (Long et al., 2015). The basic context module increases\naccuracy, the large module increases it by a larger margin. Middle: the context module increases\naccuracy when plugged into a front-end + dense CRF conﬁguration (Chen et al., 2015a). Bottom:\nthe context module increases accuracy when plugged into a front-end + CRF-RNN conﬁguration\n(Zheng et al., 2015).\nis both feasible and desirable. Our work shows that the dilated convolution operator is particularly\nsuited to dense prediction due to its ability to expand the receptive ﬁeld without losing resolution\nor coverage. We have utilized dilated convolutions to design a new network structure that reliably\nincreases accuracy when plugged into existing semantic segmentation systems. As part of this work,\nwe have also shown that the accuracy of existing convolutional networks for semantic segmentation\ncan be increased by removing vestigial components that had been developed for image classiﬁcation.\n7Published as a conference paper at ICLR 2016\naero\nbike\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse\nmbike\nperson\nplant\nsheep\nsofa\ntrain\ntv\nmean IoU\nDeepLab++ 89.1 38.3 88.1 63.3 69.7 87.1 83.1 8529.3 76.5 56.5 79.8 77.9 85.8 82.4 57.4 84.3 54.9 80.5 64.1 72.7\nDeepLab-MSc++ 89.2 46.7 88.5 63.5 68.4 87.0 81.2 86.3 32.6 80.7 62.4 81.0 81.3 84.3 82.1 56.2 84.6 58.3 76.2 67.2 73.9\nCRF-RNN 90.4 55.3 88.7 68.4 69.8 88.3 82.4 85.1 32.6 78.5 64.4 79.6 81.9 86.4 81.8 58.6 82.4 53.5 77.4 70.1 74.7\nFront end 86.6 37.3 84.9 62.4 67.3 86.2 81.2 82.1 32.6 77.4 58.3 75.9 8183.6 82.3 54.2 81.5 50.1 77.5 63 71.3\nContext 89.1 39.1 86.8 62.6 68.9 88.2 82.6 87.7 33.8 81.2 59.2 81.8 87.2 83.3 83.6 53.6 84.9 53.7 80.5 62.9 73.5\nContext + CRF 91.3 39.9 88.9 64.3 69.8 88.9 82.6 89.7 34.7 82.7 59.5 8388.4 84.2 8555.3 86.7 54.4 81.9 63.6 74.7\nContext + CRF-RNN 91.7 39.6 87.8 63.1 71.8 89.7 82.9 89.8 37.2 84 6383.3 8983.8 85.1 56.8 87.6 5680.2 64.7 75.3\nTable 4: Evaluation on the VOC-2012 test set. ‘DeepLab++’ stands for DeepLab-CRF-COCO-\nLargeFOV and ‘DeepLab-MSc++’ stands for DeepLab-MSc-CRF-LargeFOV-COCO-CrossJoint\n(Chen et al., 2015a). ‘CRF-RNN’ is the system of Zheng et al. (2015). ‘Context’ refers to the\nlarge context module plugged into our front end. The context network yields very high accuracy,\nourperforming the DeepLab++ architecture without performing structured prediction. Combining\nthe context network with the CRF-RNN structured prediction module increases the accuracy of the\nCRF-RNN system.\nWe believe that the presented work is a step towards dedicated architectures for dense prediction that\nare not constrained by image classiﬁcation precursors. As new sources of data become available,\nfuture architectures may be trained densely end-to-end, removing the need for pre-training on image\nclassiﬁcation datasets. This may enable architectural simpliﬁcation and uniﬁcation. Speciﬁcally,\nend-to-end dense training may enable a fully dense architecture akin to the presented context net-\nwork to operate at full resolution throughout, accepting the raw image as input and producing dense\nlabel assignments at full resolution as output.\nState-of-the-art systems for semantic segmentation leave signiﬁcant room for future advances. Fail-\nure cases of our most accurate conﬁguration are shown in Figure 4. We will release our code and\ntrained models to support progress in this area.\nACKNOWLEDGEMENTS\nWe thank Vibhav Vineet for proofreading, help with experiments, and related discussions. We are\nalso grateful to Jonathan Long and the Caffe team for their feedback and for rapidly pulling our\nimplementation into the Caffe library.\nREFERENCES\nBadrinarayanan, Vijay, Handa, Ankur, and Cipolla, Roberto. SegNet: A deep convolutional encoder-decoder\narchitecture for robust semantic pixel-wise labelling. arXiv:1505.07293 , 2015.\nBrostow, Gabriel J., Fauqueur, Julien, and Cipolla, Roberto. Semantic object classes in video: A high-deﬁnition\nground truth database. Pattern Recognition Letters , 30(2), 2009.\nChen, Liang-Chieh, Papandreou, George, Kokkinos, Iasonas, Murphy, Kevin, and Yuille, Alan L. Semantic\nimage segmentation with deep convolutional nets and fully connected CRFs. In ICLR , 2015a.\nSofa\nChair\nImage\nHorsePerson\nPerson Our result\n Ground truth\n Image\nCatDogHorse Our result\n Ground truth\nFigure 4: Failure cases from the VOC-2012 validation set. The most accurate architecture we trained\n(Context + CRF-RNN) performs poorly on these images.\n8Published as a conference paper at ICLR 2016\nChen, Liang-Chieh, Yang, Yi, Wang, Jiang, Xu, Wei, and Yuille, Alan L. Attention to scale: Scale-aware\nsemantic image segmentation. arXiv:1511.03339 , 2015b.\nCordts, Marius, Omran, Mohamed, Ramos, Sebastian, Rehfeld, Timo, Enzweiler, Markus, Benenson, Rodrigo,\nFranke, Uwe, Roth, Stefan, and Schiele, Bernt. The Cityscapes dataset for semantic urban scene understand-\ning. In CVPR , 2016.\nEveringham, Mark, Gool, Luc J. Van, Williams, Christopher K. I., Winn, John M., and Zisserman, Andrew.\nThe Pascal visual object classes (VOC) challenge. IJCV , 88(2), 2010.\nFarabet, Cl ´ement, Couprie, Camille, Najman, Laurent, and LeCun, Yann. Learning hierarchical features for\nscene labeling. PAMI , 35(8), 2013.\nFischer, Philipp, Dosovitskiy, Alexey, Ilg, Eddy, H ¨ausser, Philip, Hazrba, Caner, Golkov, Vladimir, van der\nSmagt, Patrick, Cremers, Daniel, and Brox, Thomas. Learning optical ﬂow with convolutional neural net-\nworks. In ICCV , 2015.\nGalleguillos, Carolina and Belongie, Serge J. Context based object categorization: A critical survey. Computer\nVision and Image Understanding , 114(6), 2010.\nGeiger, Andreas, Lenz, Philip, Stiller, Christoph, and Urtasun, Raquel. Vision meets robotics: The KITTI\ndataset. International Journal of Robotics Research , 32(11), 2013.\nGlorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty of training deep feedforward neural networks.\nInAISTATS , 2010.\nHariharan, Bharath, Arbelaez, Pablo, Bourdev, Lubomir D., Maji, Subhransu, and Malik, Jitendra. Semantic\ncontours from inverse detectors. In ICCV , 2011.\nHe, Xuming, Zemel, Richard S., and Carreira-Perpi ˜n´an, Miguel ´A. Multiscale conditional random ﬁelds for\nimage labeling. In CVPR , 2004.\nHolschneider, M., Kronland-Martinet, R., Morlet, J., and Tchamitchian, Ph. A real-time algorithm for signal\nanalysis with the help of the wavelet transform. In Wavelets: Time-Frequency Methods and Phase Space.\nProceedings of the International Conference , 1987.\nJia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross B., Guadar-\nrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature embedding. In Proc.\nACM Multimedia , 2014.\nKohli, Pushmeet, Ladicky, Lubor, and Torr, Philip H. S. Robust higher order potentials for enforcing label\nconsistency. IJCV , 82(3), 2009.\nKr¨ahenb ¨uhl, Philipp and Koltun, Vladlen. Efﬁcient inference in fully connected CRFs with Gaussian edge\npotentials. In NIPS , 2011.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. ImageNet classiﬁcation with deep convolutional\nneural networks. In NIPS , 2012.\nKundu, Abhijit, Vineet, Vibhav, and Koltun, Vladlen. Feature space optimization for semantic video segmen-\ntation. In CVPR , 2016.\nLadicky, Lubor, Russell, Christopher, Kohli, Pushmeet, and Torr, Philip H. S. Associative hierarchical CRFs\nfor object class image segmentation. In ICCV , 2009.\nLe, Quoc V ., Jaitly, Navdeep, and Hinton, Geoffrey E. A simple way to initialize recurrent networks of rectiﬁed\nlinear units. arXiv:1504.00941 , 2015.\nLeCun, Yann, Boser, Bernhard, Denker, John S., Henderson, Donnie, Howard, Richard E., Hubbard, Wayne,\nand Jackel, Lawrence D. Backpropagation applied to handwritten zip code recognition. Neural Computation ,\n1(4), 1989.\nLin, Guosheng, Shen, Chunhua, Reid, Ian, and van dan Hengel, Anton. Efﬁcient piecewise training of deep\nstructured models for semantic segmentation. arXiv:1504.01013 , 2015.\nLin, Tsung-Yi, Maire, Michael, Belongie, Serge, Hays, James, Perona, Pietro, Ramanan, Deva, Doll ´ar, Piotr,\nand Zitnick, C. Lawrence. Microsoft COCO: Common objects in context. In ECCV , 2014.\nLiu, Buyu and He, Xuming. Multiclass semantic video segmentation with object-level active inference. In\nCVPR , 2015.\nLong, Jonathan, Shelhamer, Evan, and Darrell, Trevor. Fully convolutional networks for semantic segmenta-\ntion. In CVPR , 2015.\n9Published as a conference paper at ICLR 2016\nNoh, Hyeonwoo, Hong, Seunghoon, and Han, Bohyung. Learning deconvolution network for semantic seg-\nmentation. In ICCV , 2015.\nRos, Germ ´an, Ramos, Sebastian, Granados, Manuel, Bakhtiary, Amir, V ´azquez, David, and L ´opez, Anto-\nnio Manuel. Vision-based ofﬂine-online perception paradigm for autonomous driving. In WACV , 2015.\nRumelhart, David E., Hinton, Geoffrey E., and Williams, Ronald J. Learning representations by back-\npropagating errors. Nature , 323, 1986.\nShensa, Mark J. The discrete wavelet transform: wedding the `a trous and Mallat algorithms. IEEE Transactions\non Signal Processing , 40(10), 1992.\nShotton, Jamie, Winn, John M., Rother, Carsten, and Criminisi, Antonio. TextonBoost for image understanding:\nMulti-class object recognition and segmentation by jointly modeling texture, layout, and context. IJCV , 81\n(1), 2009.\nSimonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image recognition.\nInICLR , 2015.\nSturgess, Paul, Alahari, Karteek, Ladicky, Lubor, and Torr, Philip H. S. Combining appearance and structure\nfrom motion features for road scene understanding. In BMVC , 2009.\nTighe, Joseph and Lazebnik, Svetlana. Superparsing – scalable nonparametric image parsing with superpixels.\nIJCV , 101(2), 2013.\nZheng, Shuai, Jayasumana, Sadeep, Romera-Paredes, Bernardino, Vineet, Vibhav, Su, Zhizhong, Du, Dalong,\nHuang, Chang, and Torr, Philip. Conditional random ﬁelds as recurrent neural networks. In ICCV , 2015.\n10Published as a conference paper at ICLR 2016\nAPPENDIX A U RBAN SCENE UNDERSTANDING\nIn this appendix, we report experiments on three datasets for urban scene understanding: the CamVid\ndataset (Brostow et al., 2009), the KITTI dataset (Geiger et al., 2013), and the new Cityscapes\ndataset (Cordts et al., 2016). As the accuracy measure we use the mean IoU (Everingham et al.,\n2010). We only train our model on the training set, even when a validation set is available. The\nresults reported in this section do not use conditional random ﬁelds or other forms of structured\nprediction. They were obtained with convolutional networks that combine a front-end module and a\ncontext module, akin to the “Front + Basic” network evaluated in Table 3. The trained models can\nbe found at https://github.com/fyu/dilation .\nWe now summarize the training procedure used for training the front-end module. This procedure\napplies to all datasets. Training is performed with stochastic gradient descent. Each mini-batch\ncontains 8 crops from randomly sampled images. Each crop is of size 628\u0002628and is randomly\nsampled from a padded image. Images are padded using reﬂection padding. No padding is used\nin the intermediate layers. The learning rate is 10\u00004and momentum is set to 0:99. The number of\niterations depends on the number of images in the dataset and is reported for each dataset below.\nThe context modules used for these datasets are all derived from the “Basic” network, using the\nterminology of Table 1. The number of channels in each layer is the number of predicted classes\nC. (For example, C= 19 for the Cityscapes dataset.) Each layer in the context module is padded\nsuch that the input and response maps have the same size. The number of layers in the context\nmodule depends on the resolution of the images in the dataset. Joint training of the complete model,\ncomposed of the front-end and the context module, is summarized below for each dataset.\nA.1 C AMVID\nWe use the split of Sturgess et al. (2009), which partitions the dataset into 367 training images, 100\nvalidation images, and 233 test images. 11 semantic classes are used. The images are downsampled\nto640\u0002480.\nThe context module has 8 layers, akin to the model used for the Pascal VOC dataset in the main body\nof the paper. The overall training procedure is as follows. First, the front-end module is trained for\n20K iterations. Then the complete model (front-end + context) is jointly trained by sampling crops\nof size 852\u0002852 with batch size 1. The learning rate for joint training is set to 10\u00005and the\nmomentum is set to 0:9.\nResults on the CamVid test set are reported in Table 5. We refer to our complete convolutional\nnetwork (front-end + context) as Dilation8, since the context module has 8 layers. Our model out-\nperforms the prior work. This model was used as the unary classiﬁer in the recent work of Kundu\net al. (2016).\nBuilding\nTree\nSky\nCar\nSign\nRoad\nPedestrian\nFence\nPole\nSidewalk\nBicyclist\nmean IoU\nALE 73.4 70.2 91.1 64.2 24.4 91.1 29.1 31.0 13.6 72.4 28.6 53.6\nSuperParsing 70.4 54.8 83.5 43.3 25.4 83.4 11.6 18.3 5.2 57.4 8.9 42.0\nLiu and He 66.8 66.6 90.1 62.9 21.4 85.8 28.0 17.8 8.3 63.5 8.5 47.2\nSegNet 68.7 52.0 87.0 58.5 13.4 86.2 25.3 17.9 16.0 60.5 24.8 46.4\nDeepLab-LFOV 81.5 74.6 89.0 82.2 42.3 92.2 48.4 27.2 14.3 75.4 50.1 61.6\nDilation8 82.6 76.2 89.9 84.0 46.9 92.2 56.3 35.8 23.4 75.3 55.5 65.3\nTable 5: Semantic segmentation results on the CamVid dataset. Our model (Dilation8) is com-\npared to ALE (Ladicky et al., 2009), SuperParsing (Tighe & Lazebnik, 2013), Liu and He (Liu &\nHe, 2015), SegNet (Badrinarayanan et al., 2015), and the DeepLab-LargeFOV model (Chen et al.,\n2015a). Our model outperforms the prior work.\n11Published as a conference paper at ICLR 2016\nA.2 KITTI\nWe use the training and validation split of Ros et al. (2015): 100 training images and 46 test images.\nThe images were all collected from the KITTI visual odometry/SLAM dataset. The image resolution\nis1226\u0002370. Since the vertical resolution is small compared to the other datasets, we remove Layer\n6 in Table 1. The resulting context module has 7 layers. The complete network (front-end + context)\nis referred to as Dilation7.\nThe front-end is trained for 10K iterations. Next, the front-end and the context module are trained\njointly. For joint training, the crop size is 900\u0002900and momentum is set to 0.99, while the other\nparameters are the same as the ones used for the CamVid dataset. Joint training is performed for\n20K iterations.\nThe results are shown in Table 6. As the table demonstrates, our model outperforms the prior work.\nBuilding\nTree\nSky\nCar\nSign\nRoad\nPedestrian\nFence\nPole\nSidewalk\nBicyclist\nmean IoU\nRos et al. 71.8 69.5 84.4 51.2 4.2 72.4 1.7 32.4 2.6 45.3 3.2 39.9\nDeepLab-LFOV 82.8 78.6 82.4 78.0 28.8 91.3 0.0 39.4 29.9 72.4 12.9 54.2\nDilation7 84.6 81.1 83 81.4 41.8 92.9 4.6 47.1 35.2 73.1 26.4 59.2\nTable 6: Semantic segmentation results on the KITTI dataset. We compare our results to Ros et al.\n(2015) and to the DeepLab-LargeFOV model (Chen et al., 2015a). Our network (Dilation7) yields\nhigher accuracy than the prior work.\nA.3 C ITYSCAPES\nThe Cityscapes dataset contains 2975 training images, 500 validation images, and 1525 test im-\nages (Cordts et al., 2016). Due to the high image resolution ( 2048\u00021024 ), we add two layers to the\ncontext network after Layer 6 in Table 1. These two layers have dilation 32 and 64, respectively. The\ntotal number of layers in the context module is 10 and we refer to the complete model (front-end +\ncontext) as Dilation10.\nThe Dilation10 network was trained in three stages. First, the front-end prediction module was\ntrained for 40K iterations. Second, the context module was trained for 24K iterations on whole\n(uncropped) images, with learning rate 10\u00004, momentum 0:99, and batch size 100. Third, the\ncomplete model (front-end + context) was jointly trained for 60K iterations on halves of images\n(input size 1396\u00021396 , including padding), with learning rate 10\u00005, momentum 0:99, and batch\nsize 1.\nFigure 5 visualizes the effect of the training stages on the performance of the model. Quantitative\nresults are given in Tables 7 and 8.\nThe performance of Dilation10 was compared to prior work on the Cityscapes dataset by Cordts\net al. (2016). In their evaluation, Dilation10 outperformed all prior models (Cordts et al., 2016).\nDilation10 was also used as the unary classiﬁer in the recent work of Kundu et al. (2016), which\nused structured prediction to increase accuracy further.\n12Published as a conference paper at ICLR 2016\n(a) Image (b) Ground truth\n(c) Front end (d) +Context (e) +Joint (f) Ground truth\nFigure 5: Results produced by the Dilation10 model after different training stages. (a) Input image.\n(b) Ground truth segmentation. (c) Segmentation produced by the model after the ﬁrst stage of\ntraining (front-end only). (d) Segmentation produced after the second stage, which trains the context\nmodule. (e) Segmentation produced after the third stage, in which both modules are trained jointly.\nRoad\nSidewalk\nBuilding\nWall\nFence\nPole\nLight\nSign\nVegetation\nTerrain\nSky\nPerson\nRider\nCar\nTruck\nBus\nTrain\nMotorcycle\nBicycle\nmean IoU\nValidation set\n97.2 79.5 90.4 44.9 52.4 55.1 56.7 69 9158.7 92.6 75.7 5092.2 56.2 72.6 54.3 46.2 70.1 68.7\nTest set\n97.6 79.2 89.9 37.3 47.6 53.2 58.6 65.2 91.8 69.4 93.7 78.9 5593.3 45.5 53.4 47.7 52.2 66 67.1\nTable 7: Per-class and mean class-level IoU achieved by our model (Dilation10) on the Cityscapes\ndataset.\nFlat Nature Object Sky Construction Human Vehicle mean IoU\nValidation set\n98.2 91.4 62.3 92.6 90.7 77.6 91 86.3\nTest set\n98.3 91.4 60.5 93.7 90.2 79.8 91.8 86.5\nTable 8: Per-category and mean category-level IoU on the Cityscapes dataset.\n13\n\n\n\nNeural Message Passing for Quantum Chemistry\nJustin Gilmer1Samuel S. Schoenholz1Patrick F. Riley2Oriol Vinyals3George E. Dahl1\nAbstract\nSupervised learning on molecules has incredi-\nble potential to be useful in chemistry, drug dis-\ncovery, and materials science. Luckily, sev-\neral promising and closely related neural network\nmodels invariant to molecular symmetries have\nalready been described in the literature. These\nmodels learn a message passing algorithm and\naggregation procedure to compute a function of\ntheir entire input graph. At this point, the next\nstep is to ﬁnd a particularly effective variant of\nthis general approach and apply it to chemical\nprediction benchmarks until we either solve them\nor reach the limits of the approach. In this pa-\nper, we reformulate existing models into a sin-\ngle common framework we call Message Pass-\ning Neural Networks (MPNNs) and explore ad-\nditional novel variations within this framework.\nUsing MPNNs we demonstrate state of the art re-\nsults on an important molecular property predic-\ntion benchmark; these results are strong enough\nthat we believe future work should focus on\ndatasets with larger molecules or more accurate\nground truth labels.\n1. Introduction\nThe past decade has seen remarkable success in the use\nof deep neural networks to understand and translate nat-\nural language (Wu et al., 2016), generate and decode com-\nplex audio signals (Hinton et al., 2012), and infer fea-\ntures from real-world images and videos (Krizhevsky et al.,\n2012). Although chemists have applied machine learn-\ning to many problems over the years, predicting the prop-\nerties of molecules and materials using machine learning\n(and especially deep learning) is still in its infancy. To\ndate, most research applying machine learning to chemistry\ntasks (Hansen et al., 2015; Huang & von Lilienfeld, 2016;\n1Google Brain2Google3Google DeepMind. Correspon-\ndence to: Justin Gilmer <gilmer@google.com >, George E. Dahl\n<gdahl@google.com >.\nProceedings of the 34thInternational Conference on Machine\nLearning , Sydney, Australia, PMLR 70, 2017. Copyright 2017\nby the author(s).\nDFT\n⇠103seconds\nMessage Passing Neural Net\n⇠10\u00002secondsE,!0,. . .Targets\n1Figure 1. A Message Passing Neural Network predicts quantum\nproperties of an organic molecule by modeling a computationally\nexpensive DFT calculation.\nRupp et al., 2012; Rogers & Hahn, 2010; Montavon et al.,\n2012; Behler & Parrinello, 2007; Schoenholz et al., 2016)\nhas revolved around feature engineering. While neural net-\nworks have been applied in a variety of situations (Merk-\nwirth & Lengauer, 2005; Micheli, 2009; Lusci et al., 2013;\nDuvenaud et al., 2015), they have yet to become widely\nadopted. This situation is reminiscent of the state of image\nmodels before the broad adoption of convolutional neural\nnetworks and is due, in part, to a dearth of empirical evi-\ndence that neural architectures with the appropriate induc-\ntive bias can be successful in this domain.\nRecently, large scale quantum chemistry calculation and\nmolecular dynamics simulations coupled with advances in\nhigh throughput experiments have begun to generate data\nat an unprecedented rate. Most classical techniques do\nnot make effective use of the larger amounts of data that\nare now available. The time is ripe to apply more power-\nful and ﬂexible machine learning methods to these prob-\nlems, assuming we can ﬁnd models with suitable inductive\nbiases. The symmetries of atomic systems suggest neu-\nral networks that operate on graph structured data and are\ninvariant to graph isomorphism might also be appropriate\nfor molecules. Sufﬁciently successful models could some-\nday help automate challenging chemical search problems\nin drug discovery or materials science.\nIn this paper, our goal is to demonstrate effective ma-\nchine learning models for chemical prediction problemsarXiv:1704.01212v2  [cs.LG]  12 Jun 2017Neural Message Passing for Quantum Chemistry\nthat are capable of learning their own features from molec-\nular graphs directly and are invariant to graph isomorphism.\nTo that end, we describe a general framework for super-\nvised learning on graphs called Message Passing Neural\nNetworks (MPNNs) that simply abstracts the commonali-\nties between several of the most promising existing neural\nmodels for graph structured data, in order to make it easier\nto understand the relationships between them and come up\nwith novel variations. Given how many researchers have\npublished models that ﬁt into the MPNN framework, we\nbelieve that the community should push this general ap-\nproach as far as possible on practically important graph\nproblems and only suggest new variations that are well\nmotivated by applications, such as the application we con-\nsider here: predicting the quantum mechanical properties\nof small organic molecules (see task schematic in ﬁgure 1).\nIn general, the search for practically effective machine\nlearning (ML) models in a given domain proceeds through\na sequence of increasingly realistic and interesting bench-\nmarks. Here we focus on the QM9 dataset as such a bench-\nmark (Ramakrishnan et al., 2014). QM9 consists of 130k\nmolecules with 13 properties for each molecule which are\napproximated by an expensive1quantum mechanical simu-\nlation method (DFT), to yield 13 corresponding regression\ntasks. These tasks are plausibly representative of many im-\nportant chemical prediction problems and are (currently)\ndifﬁcult for many existing methods. Additionally, QM9\nalso includes complete spatial information for the single\nlow energy conformation of the atoms in the molecule that\nwas used in calculating the chemical properties. QM9\ntherefore lets us consider both the setting where the com-\nplete molecular geometry is known (atomic distances, bond\nangles, etc.) and the setting where we need to compute\nproperties that might still be deﬁned in terms of the spa-\ntial positions of atoms, but where only the atom and bond\ninformation (i.e. graph) is available as input. In the lat-\nter case, the model must implicitly ﬁt something about the\ncomputation used to determine a low energy 3D conforma-\ntion and hopefully would still work on problems where it is\nnot clear how to compute a reasonable 3D conformation.\nWhen measuring the performance of our models on QM9,\nthere are two important benchmark error levels. The ﬁrst\nis the estimated average error of the DFT approximation\nto nature, which we refer to as “DFT error.” The sec-\nond, known as “chemical accuracy,” is a target error that\nhas been established by the chemistry community. Esti-\nmates of DFT error and chemical accuracy are provided\nfor each of the 13 targets in Faber et al. (2017). One im-\nportant goal of this line of research is to produce a model\nwhich can achieve chemical accuracy with respect to the\n1By comparison, the inference time of the neural networks dis-\ncussed in this work is 300k times faster.true targets as measured by an extremely precise experi-\nment. The dataset containing the true targets on all 134k\nmolecules does not currently exist. However, the ability\nto ﬁt the DFT approximation to within chemical accuracy\nwould be an encouraging step in this direction. For all 13\ntargets, achieving chemical accuracy is at least as hard as\nachieving DFT error. In the rest of this paper when we talk\nabout chemical accuracy we generally mean with respect to\nour available ground truth labels.\nIn this paper, by exploring novel variations of models in the\nMPNN family, we are able to both achieve a new state of\nthe art on the QM9 dataset and to predict the DFT calcula-\ntion to within chemical accuracy on all but two targets. In\nparticular, we provide the following key contributions:\n\u000fWe develop an MPNN which achieves state of the art\nresults on all 13 targets and predicts DFT to within\nchemical accuracy on 11 out of 13 targets.\n\u000fWe develop several different MPNNs which predict\nDFT to within chemical accuracy on 5 out of 13 tar-\ngets while operating on the topology of the molecule\nalone (with no spatial information as input).\n\u000fWe develop a general method to train MPNNs with\nlarger node representations without a corresponding\nincrease in computation time or memory, yielding a\nsubstantial savings over previous MPNNs for high di-\nmensional node representations.\nWe believe our work is an important step towards making\nwell-designed MPNNs the default for supervised learning\non modestly sized molecules. In order for this to hap-\npen, researchers need to perform careful empirical stud-\nies to ﬁnd the proper way to use these types of models\nand to make any necessary improvements to them, it is\nnot sufﬁcient for these models to have been described in\nthe literature if there is only limited accompanying empir-\nical work in the chemical domain. Indeed convolutional\nneural networks existed for decades before careful empiri-\ncal work applying them to image classiﬁcation (Krizhevsky\net al., 2012) helped them displace SVMs on top of hand-\nengineered features for a host of computer vision problems.\n2. Message Passing Neural Networks\nThere are at least eight notable examples of models from\nthe literature that we can describe using our Message Pass-\ning Neural Networks (MPNN) framework. For simplicity\nwe describe MPNNs which operate on undirected graphs\nGwith node features xvand edge features evw. It is triv-\nial to extend the formalism to directed multigraphs. The\nforward pass has two phases, a message passing phase and\na readout phase. The message passing phase runs for TNeural Message Passing for Quantum Chemistry\ntime steps and is deﬁned in terms of message functions Mt\nand vertex update functions Ut. During the message pass-\ning phase, hidden states ht\nvat each node in the graph are\nupdated based on messages mt+1\nvaccording to\nmt+1\nv=X\nw2N(v)Mt(ht\nv;ht\nw;evw) (1)\nht+1\nv=Ut(ht\nv;mt+1\nv) (2)\nwhere in the sum, N(v)denotes the neighbors of vin graph\nG. The readout phase computes a feature vector for the\nwhole graph using some readout function Raccording to\n^y=R(fhT\nvjv2Gg): (3)\nThe message functions Mt, vertex update functions Ut, and\nreadout function Rare all learned differentiable functions.\nRoperates on the set of node states and must be invariant to\npermutations of the node states in order for the MPNN to be\ninvariant to graph isomorphism. In what follows, we deﬁne\nprevious models in the literature by specifying the message\nfunctionMt, vertex update function Ut, and readout func-\ntionRused. Note one could also learn edge features in\nan MPNN by introducing hidden states for all edges in the\ngraphht\nevwand updating them analogously to equations 1\nand 2. Of the existing MPNNs, only Kearnes et al. (2016)\nhas used this idea.\nConvolutional Networks for Learning Molecular Fin-\ngerprints, Duvenaud et al. (2015)\nThe message function used is M(hv;hw;evw) =\n(hw;evw)where (:;:)denotes concatenation. The vertex\nupdate function used is Ut(ht\nv;mt+1\nv) =\u001b(Hdeg(v)\ntmt+1\nv),\nwhere\u001bis the sigmoid function, deg (v)is the degree of\nvertexvandHN\ntis a learned matrix for each time step t\nand vertex degree N.Rhas skip connections to all previous\nhidden states ht\nvand is equal to f \nP\nv;tsoftmax (Wtht\nv)!\n,\nwherefis a neural network and Wtare learned readout\nmatrices, one for each time step t. This message pass-\ning scheme may be problematic since the resulting mes-\nsage vector is mt+1\nv= (Pht\nw;Pevw);which separately\nsums over connected nodes and connected edges. It fol-\nlows that the message passing implemented in Duvenaud\net al. (2015) is unable to identify correlations between edge\nstates and node states.\nGated Graph Neural Networks (GG-NN), Li et al.\n(2016)\nThe message function used is Mt(ht\nv;ht\nw;evw) =Aevwht\nw,\nwhereAevwis a learned matrix, one for each edge label e\n(the model assumes discrete edge types). The update func-\ntion isUt=GRU(ht\nv;mt+1\nv), where GRU is the GatedRecurrent Unit introduced in Cho et al. (2014). This work\nused weight tying, so the same update function is used at\neach time step t. Finally,\nR=X\nv2V\u001b\u0010\ni(h(T)\nv;h0\nv)\u0011\n\f\u0010\nj(h(T)\nv)\u0011\n(4)\nwhereiandjare neural networks, and \fdenotes element-\nwise multiplication.\nInteraction Networks, Battaglia et al. (2016)\nThis work considered both the case where there is a tar-\nget at each node in the graph, and where there is a graph\nlevel target. It also considered the case where there are\nnode level effects applied at each time step, in such a\ncase the update function takes as input the concatenation\n(hv;xv;mv)wherexvis an external vector representing\nsome outside inﬂuence on the vertex v. The message func-\ntionM(hv;hw;evw)is a neural network which takes the\nconcatenation (hv;hw;evw). The vertex update function\nU(hv;xv;mv)is a neural network which takes as input\nthe concatenation (hv;xv;mv). Finally, in the case where\nthere is a graph level output, R=f(P\nv2GhT\nv)wherefis\na neural network which takes the sum of the ﬁnal hidden\nstateshT\nv. Note the original work only deﬁned the model\nforT= 1.\nMolecular Graph Convolutions, Kearnes et al. (2016)\nThis work deviates slightly from other MPNNs in\nthat it introduces edge representations et\nvw which\nare updated during the message passing phase.\nThe message function used for node messages is\nM(ht\nv;ht\nw;et\nvw) =et\nvw. The vertex update function\nisUt(ht\nv;mt+1\nv) =\u000b(W1(\u000b(W0ht\nv);mt+1\nv))where\n(:;:)denotes concatenation, \u000bis the ReLU activation\nandW1;W 0are learned weight matrices. The edge\nstate update is deﬁned by et+1\nvw=U0\nt(et\nvw;ht\nv;ht\nw) =\n\u000b(W4(\u000b(W2;et\nvw);\u000b(W3(ht\nv;ht\nw)))) where theWiare\nalso learned weight matrices.\nDeep Tensor Neural Networks, Sch ¨utt et al. (2017)\nThe message from wtovis computed by\nMt=tanh\u0000\nWfc((Wcfht\nw+b1)\f(Wd fevw+b2))\u0001\nwhereWfc,Wcf,Wd fare matrices and b1,b2are bias\nvectors. The update function used is Ut(ht\nv;mt+1\nv) =\nht\nv+mt+1\nv. The readout function passes each node inde-\npendently through a single hidden layer neural network and\nsums the outputs, in particular\nR=X\nvNN(hT\nv):\nLaplacian Based Methods, Bruna et al. (2013); Deffer-\nrard et al. (2016); Kipf & Welling (2016)Neural Message Passing for Quantum Chemistry\nThese methods generalize the notion of the convolution op-\neration typically applied to image datasets to an operation\nthat operates on an arbitrary graph Gwith a real valued ad-\njacency matrix A. The operations deﬁned in Bruna et al.\n(2013); Defferrard et al. (2016) result in message functions\nof the form Mt(ht\nv;ht\nw) =Ct\nvwht\nw, where the matrices\nCt\nvware parameterized by the eigenvectors of the graph\nlaplacianL, and the learned parameters of the model. The\nvertex update function used is Ut(ht\nv;mt+1\nv) =\u001b(mt+1\nv)\nwhere\u001bis some pointwise non-linearity (such as ReLU).\nThe Kipf & Welling (2016) model results in a mes-\nsage function Mt(ht\nv;ht\nw) =cvwht\nwwherecvw=\n(deg(v)deg(w))\u00001=2Avw. The vertex update function is\nUt\nv(ht\nv;mt+1\nv) = ReLU (Wtmt+1\nv). For the exact expres-\nsions for the Ct\nvwand the derivation of the reformulation of\nthese models as MPNNs, see the supplementary material.\n2.1. Moving Forward\nGiven how many instances of MPNNs have appeared in the\nliterature, we should focus on pushing this general fam-\nily as far as possible in a speciﬁc application of substan-\ntial practical importance. This way we can determine the\nmost crucial implementation details and potentially reach\nthe limits of these models to guide us towards future mod-\neling improvements.\nOne downside of all of these approaches is computation\ntime. Recent work has adapted the GG-NN architecture to\nlarger graphs by passing messages on only subsets of the\ngraph at each time step (Marino et al., 2016). In this work\nwe also present a MPNN modiﬁcation that can improve the\ncomputational costs.\n3. Related Work\nAlthough in principle quantum mechanics lets us compute\nthe properties of molecules, the laws of physics lead to\nequations that are far too difﬁcult to solve exactly. There-\nfore scientists have developed a hierarchy of approxima-\ntions to quantum mechanics with varying tradeoffs of speed\nand accuracy, such as Density Functional Theory (DFT)\nwith a variety of functionals (Becke, 1993; Hohenberg &\nKohn, 1964), the GW approximation (Hedin, 1965), and\nQuantum Monte-Carlo (Ceperley & Alder, 1986). Despite\nbeing widely used, DFT is simultaneously still too slow to\nbe applied to large systems (scaling as O(N3\ne)whereNeis\nthe number of electrons) and exhibits systematic as well as\nrandom errors relative to exact solutions to Schr ¨odinger’s\nequation. For example, to run the DFT calculation on a sin-\ngle 9 heavy atom molecule in QM9 takes around an hour\non a single core of a Xeon E5-2660 (2.2 GHz) using a ver-\nsion of Gaussian G09 (ES64L-G09RevD.01) (Bing et al.,\n2017). For a 17 heavy atom molecule, computation time isup to 8 hours. Empirical potentials have been developed,\nsuch as the Stillinger-Weber potential (Stillinger & Weber,\n1985), that are fast and accurate but must be created from\nscratch, from ﬁrst principles, for every new composition of\natoms.\nHu et al. (2003) used neural networks to approximate a par-\nticularly troublesome term in DFT called the exchange cor-\nrelation potential to improve the accuracy of DFT. How-\never, their method fails to improve upon the efﬁciency of\nDFT and relies on a large set of ad hoc atomic descriptors.\nTwo more recent approaches by Behler & Parrinello (2007)\nand Rupp et al. (2012) attempt to approximate solutions\nto quantum mechanics directly without appealing to DFT.\nIn the ﬁrst case single-hidden-layer neural networks were\nused to approximate the energy and forces for conﬁgura-\ntions of a Silicon melt with the goal of speeding up molec-\nular dynamics simulations. The second paper used Ker-\nnel Ridge Regression (KRR) to infer atomization energies\nover a wide range of molecules. In both cases hand en-\ngineered features were used (symmetry functions and the\nCoulomb matrix, respectively) that built physical symme-\ntries into the input representation. Subsequent papers have\nreplaced KRR by a neural network.\nBoth of these lines of research used hand engineered fea-\ntures that have intrinsic limitations. The work of Behler &\nParrinello (2007) used a representation that was manifestly\ninvariant to graph isomorphism, but has difﬁculty when ap-\nplied to systems with more than three species of atoms and\nfails to generalize to novel compositions. The represen-\ntation used in Rupp et al. (2012) is not invariant to graph\nisomorphism. Instead, this invariance must be learned by\nthe downstream model through dataset augmentation.\nIn addition to the eight MPNNs discussed in Section 2 there\nhave been a number of other approaches to machine learn-\ning on graphical data which take advantage of the symme-\ntries in a number of ways. One such family of approaches\ndeﬁne a preprocessing step which constructs a canonical\ngraph representation which can then be fed into into a stan-\ndard classiﬁer. Examples in this family include Niepert\net al. (2016) and Rupp et al. (2012). Finally Scarselli et al.\n(2009) deﬁne a message passing process on graphs which\nis run until convergence, instead of for a ﬁnite number of\ntime steps as in MPNNs.\n4. QM9 Dataset\nTo investigate the success of MPNNs on predicting chem-\nical properties, we use the publicly available QM9 dataset\n(Ramakrishnan et al., 2014). Molecules in the dataset con-\nsist of Hydrogen (H), Carbon (C), Oxygen (O), Nitrogen\n(N), and Flourine (F) atoms and contain up to 9 heavy (non\nHydrogen) atoms. In all, this results in about 134k drug-Neural Message Passing for Quantum Chemistry\nlike organic molecules that span a wide range of chemistry.\nFor each molecule DFT is used to ﬁnd a reasonable low\nenergy structure and hence atom “positions” are available.\nAdditionally a wide range of interesting and fundamental\nchemical properties are computed. Given how fundamen-\ntal some of the QM9 properties are, it is hard to believe\nsuccess on more challenging chemical tasks will be possi-\nble if we can’t make accurate statistical predictions for the\nproperties computed in QM9.\nWe can group the different properties we try to predict into\nfour broad categories. First, we have four properties re-\nlated to how tightly bound together the atoms in a molecule\nare. These measure the energy required to break up the\nmolecule at different temperatures and pressures. These\ninclude the atomization energy at 0K,U0(eV), atomiza-\ntion energy at room temperature, U(eV), enthalpy of at-\nomization at room temperature, H(eV), and free energy of\natomization, G(eV).\nNext there are properties related to fundamental vibrations\nof the molecule, including the highest fundamental vibra-\ntional frequency !1(cm\u00001) and the zero point vibrational\nenergy (ZPVE) (eV).\nAdditionally, there are a number of properties that concern\nthe states of the electrons in the molecule. They include\nthe energy of the electron in the highest occupied molecu-\nlar orbital (HOMO) \"HOMO (eV), the energy of the lowest\nunoccupied molecular orbital (LUMO) \"LUMO (eV), and the\nelectron energy gap ( \u0001\"(eV)). The electron energy gap is\nsimply the difference \"HOMO\u0000\"LUMO .\nFinally, there are several measures of the spatial distribu-\ntion of electrons in the molecule. These include the elec-\ntronic spatial extent hR2i(Bohr2), the norm of the dipole\nmoment\u0016(Debye), and the norm of static polarizability \u000b\n(Bohr3). For a more detailed description of these proper-\nties, see the supplementary material.\n5. MPNN Variants\nWe began our exploration of MPNNs around the GG-NN\nmodel which we believe to be a strong baseline. We fo-\ncused on trying different message functions, output func-\ntions, ﬁnding the appropriate input representation, and\nproperly tuning hyperparameters.\nFor the rest of the paper we use dto denote the dimension\nof the internal hidden representation of each node in the\ngraph, andnto denote the number of nodes in the graph.\nOur implementation of MPNNs in general operates on di-\nrected graphs with a separate message channel for incom-\ning and outgoing edges, in which case the incoming mes-\nsagemvis the concatenation of min\nvandmout\nv, this was also\nused in Li et al. (2016). When we apply this to undirectedchemical graphs we treat the graph as directed, where each\noriginal edge becomes both an incoming and outgoing edge\nwith the same label. Note there is nothing special about the\ndirection of the edge, it is only relevant for parameter tying.\nTreating undirected graphs as directed means that the size\nof the message channel is 2dinstead ofd.\nThe input to our MPNN model is a set of feature vectors\nfor the nodes of the graph, xv, and an adjacency matrix A\nwith vector valued entries to indicate different bonds in the\nmolecule as well as pairwise spatial distance between two\natoms. We experimented as well with the message func-\ntion used in the GG-NN family, which assumes discrete\nedge labels, in which case the matrix Ahas entries in a dis-\ncrete alphabet of size k. The initial hidden states h0\nvare set\nto be the atom input feature vectors xvand are padded up\nto some larger dimension d. All of our experiments used\nweight tying at each time step t, and a GRU (Cho et al.,\n2014) for the update function as in the GG-NN family.\n5.1. Message Functions\nMatrix Multiplication: We started with the message func-\ntion used in GG-NN which is deﬁned by the equation\nM(hv;hw;evw) =Aevwhw.\nEdge Network: To allow vector valued edge features\nwe propose the message function M(hv;hw;evw) =\nA(evw)hwwhereA(evw)is a neural network which maps\nthe edge vector evwto ad\u0002dmatrix.\nPair Message: One property that the matrix multiplication\nrule has is that the message from node wto nodevis a\nfunction only of the hidden state hwand the edge evw. In\nparticular, it does not depend on the hidden state ht\nv. In\ntheory, a network may be able to use the message channel\nmore efﬁciently if the node messages are allowed to de-\npend on both the source and destination node. Thus we\nalso tried using a variant on the message function as de-\nscribed in (Battaglia et al., 2016). Here the message from\nwtovalong edgeeismwv=f(ht\nw;ht\nv;evw)wherefis\na neural network.\nWhen we apply the above message functions to directed\ngraphs, there are two separate functions used, Minand an\nMout. Which function is applied to a particular edge evw\ndepends on the direction of that edge.\n5.2. Virtual Graph Elements\nWe explored two different ways to change how the mes-\nsages are passed throughout the model. The simplest mod-\niﬁcation involves adding a separate “virtual” edge type for\npairs of nodes that are not connected. This can be imple-\nmented as a data preprocessing step and allows information\nto travel long distances during the propagation phase.Neural Message Passing for Quantum Chemistry\nWe also experimented with using a latent “master” node,\nwhich is connected to every input node in the graph with\na special edge type. The master node serves as a global\nscratch space that each node both reads from and writes to\nin every step of message passing. We allow the master node\nto have a separate node dimension dmaster , as well as sep-\nara"
  }
}